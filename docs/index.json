[
{
	"uri": "/network-policies/calico/install_calico/",
	"title": "Install Calico",
	"tags": [],
	"description": "",
	"content": "Apply the Calico manifest from the aws/amazon-vpc-cni-k8s GitHub project. This creates the daemon sets in the kube-system namespace.\nkubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.2/calico.yaml  Let\u0026rsquo;s go over few key features of the Calico manifest:\n1) We see an annotation throughout; annotations are a way to attach non-identifying metadata to objects. This metadata is not used internally by Kubernetes, so they cannot be used to identify within k8s. Instead, they are used by external tools and libraries. Examples of annotations include build/release timestamps, client library information for debugging, or fields managed by a network policy like Calico in this case.\nkind: DaemonSet apiVersion: extensions/v1beta1 metadata: name: calico-node namespace: kube-system labels: k8s-app: calico-node spec: selector: matchLabels: k8s-app: calico-node updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: k8s-app: calico-node annotations: # This, along with the CriticalAddonsOnly toleration below, # marks the pod as a critical add-on, ensuring it gets # priority scheduling and that its resources are reserved # if it ever gets evicted. *scheduler**.alpha.kubernetes.io/critical-pod: ''* ...  In contrast, Labels in Kubernetes are intended to be used to specify identifying attributes for objects. They are used by selector queries or with label selectors. Since they are used internally by Kubernetes the structure of keys and values is constrained, to optimize queries.\n2) We see that the manifest has a tolerations attribute. Taints and tolerations work together to ensure pods are not scheduled onto inappropriate nodes. Taints are applied to nodes, and the only pods that can tolerate the taint are allowed to run on those nodes.\nA taint consists of a key, a value for it and an effect, which can be:\n PreferNoSchedule: Prefer not to schedule intolerant pods to the tainted node NoSchedule: Do not schedule intolerant pods to the tainted node NoExecute: In addition to not scheduling, also evict intolerant pods that are already running on the node.   Like taints, tolerations also have a key value pair and an effect, with the addition of operator. Here in the Calico manifest, we see tolerations has just one attribute: Operator = exists. This means the key value pair is omitted and the toleration will match any taint, ensuring it runs on all nodes.\n tolerations: - operator: Exists  Watch the kube-system daemon sets and wait for the calico-node daemon set to have the DESIRED number of pods in the READY state.\nkubectl get daemonset calico-node --namespace=kube-system  Expected Output:\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE calico-node 3 3 3 3 3 \u0026lt;none\u0026gt; 38s  "
},
{
	"uri": "/network-policies/calico/stars_policy_demo/create_resources/",
	"title": "Create Resources",
	"tags": [],
	"description": "",
	"content": " Before creating network polices, let\u0026rsquo;s create the required resources.\nCreate a new folder for the configuration files.\nmkdir ~/environment/calico_resources cd ~/environment/calico_resources  Stars Namespace Copy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/namespace.yaml  Let\u0026rsquo;s examine our file by running cat namespace.yaml.\nkind: Namespace apiVersion: v1 metadata: name: stars  Create a namespace called stars:\nkubectl apply -f namespace.yaml  We will create frontend and backend replication controllers and services in this namespace in later steps.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/management-ui.yaml wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/backend.yaml wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/frontend.yaml wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/client.yaml  cat management-ui.yaml:\napiVersion: v1 kind: Namespace metadata: name: management-ui labels: role: management-ui --- apiVersion: v1 kind: Service metadata: name: management-ui namespace: management-ui spec: type: LoadBalancer ports: - port: 80 targetPort: 9001 selector: role: management-ui --- apiVersion: v1 kind: ReplicationController metadata: name: management-ui namespace: management-ui spec: replicas: 1 template: metadata: labels: role: management-ui spec: containers: - name: management-ui image: calico/star-collect:v0.1.0 imagePullPolicy: Always ports: - containerPort: 9001  Create a management-ui namespace, with a management-ui service and replication controller within that namespace:\nkubectl apply -f management-ui.yaml  cat backend.yaml to see how the backend service is built:\napiVersion: v1 kind: Service metadata: name: backend namespace: stars spec: ports: - port: 6379 targetPort: 6379 selector: role: backend --- apiVersion: v1 kind: ReplicationController metadata: name: backend namespace: stars spec: replicas: 1 template: metadata: labels: role: backend spec: containers: - name: backend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=6379 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 6379  Let\u0026rsquo;s examine the frontend service with cat frontend.yaml:\napiVersion: v1 kind: Service metadata: name: frontend namespace: stars spec: ports: - port: 80 targetPort: 80 selector: role: frontend --- apiVersion: v1 kind: ReplicationController metadata: name: frontend namespace: stars spec: replicas: 1 template: metadata: labels: role: frontend spec: containers: - name: frontend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=80 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 80  Create frontend and backend replication controllers and services within the stars namespace:\nkubectl apply -f backend.yaml kubectl apply -f frontend.yaml  Lastly, let\u0026rsquo;s examine how the client namespace, and a client service for a replication controller. are built. cat client.yaml:\nkind: Namespace apiVersion: v1 metadata: name: client labels: role: client --- apiVersion: v1 kind: ReplicationController metadata: name: client namespace: client spec: replicas: 1 template: metadata: labels: role: client spec: containers: - name: client image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status ports: - containerPort: 9000 --- apiVersion: v1 kind: Service metadata: name: client namespace: client spec: ports: - port: 9000 targetPort: 9000 selector: role: client  Apply the client configuraiton.\nkubectl apply -f client.yaml  Check their status, and wait for all the pods to reach the Running status:\n$ kubectl get pods --all-namespaces  Your output should look like this:\nNAMESPACE NAME READY STATUS RESTARTS AGE client client-nkcfg 1/1 Running 0 24m kube-system aws-node-6kqmw 1/1 Running 0 50m kube-system aws-node-grstb 1/1 Running 1 50m kube-system aws-node-m7jg8 1/1 Running 1 50m kube-system calico-node-b5b7j 1/1 Running 0 28m kube-system calico-node-dw694 1/1 Running 0 28m kube-system calico-node-vtz9k 1/1 Running 0 28m kube-system calico-typha-75667d89cb-4q4zx 1/1 Running 0 28m kube-system calico-typha-horizontal-autoscaler-78f747b679-kzzwq 1/1 Running 0 28m kube-system kube-dns-7cc87d595-bd9hq 3/3 Running 0 1h kube-system kube-proxy-lp4vw 1/1 Running 0 50m kube-system kube-proxy-rfljb 1/1 Running 0 50m kube-system kube-proxy-wzlqg 1/1 Running 0 50m management-ui management-ui-wzvz4 1/1 Running 0 24m stars backend-tkjrx 1/1 Running 0 24m stars frontend-q4r84 1/1 Running 0 24m  It may take several minutes to download all the required Docker images.\n To summarize the different resources we created:\n A namespace called stars frontend and backend replication controllers and services within stars namespace A namespace called management-ui Replication controller and service management-ui for the user interface seen on the browser, in the management-ui namespace A namespace called client client replication controller and service in client namespace  "
},
{
	"uri": "/",
	"title": "Amazon EKS Workshop",
	"tags": [],
	"description": "",
	"content": "Amazon EKS Workshop In this workshop, we will explore multiple ways to configure VPC, ALB, and EC2 Kubernetes workers, and Amazon Elastic Kubernetes Service.\n"
},
{
	"uri": "/conclusion/conclusion/",
	"title": "What Have We Accomplished",
	"tags": [],
	"description": "",
	"content": "We have:\n Deployed an application consisting of microservices Deployed the Kubernetes Dashboard Deployed packages using Helm Deployed a centralized logging infrastructure Configured Automatic scaling of our pods and worker nodes  "
},
{
	"uri": "/network-policies/calico/stars_policy_demo/default_policy/",
	"title": "Default Pod-to-Pod Communication",
	"tags": [],
	"description": "",
	"content": "In Kubernetes, the pods by default can communicate with other pods, regardless of which host they land on. Every pod gets its own IP address so you do not need to explicitly create links between pods. This is demonstrated by the management-ui.\nkind: Service metadata: name: management-ui namespace: management-ui spec: type: LoadBalancer ports: - port: 80 targetPort: 9001  To open the Management UI, retrieve the DNS name of the Management UI using:\nkubectl get svc -o wide -n management-ui  Copy the EXTERNAL-IP from the output, and paste into a browser. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com” - the full value is the DNS address.\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR management-ui LoadBalancer 10.100.239.7 a8b8c5f77eda911e8b1a60ea5d5305a4-720629306.us-east-1.elb.amazonaws.com 80:31919/TCP 9s role=management-ui  The UI here shows the default behavior, of all services being able to reach each other.\n"
},
{
	"uri": "/network-policies/calico/stars_policy_demo/apply_network_policies/",
	"title": "Apply Network Policies",
	"tags": [],
	"description": "",
	"content": " In a production level cluster, it is not secure to have open pod to pod communication. Let\u0026rsquo;s see how we can isolate the services from each other.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/apply_network_policies.files/default-deny.yaml  Let\u0026rsquo;s examine our file by running cat default-deny.yaml.\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny spec: podSelector: matchLabels: {}  Let\u0026rsquo;s go over the network policy. Here we see the podSelector does not have any matchLabels, essentially blocking all the pods from accessing it.\nApply the network policy in the stars namespace (frontend and backend services) and the client namespace (client service):\nkubectl apply -n stars -f default-deny.yaml kubectl apply -n client -f default-deny.yaml  Upon refreshing your browser, you see that the management UI cannot reach any of the nodes, so nothing shows up in the UI.\nNetwork policies in Kubernetes use labels to select pods, and define rules on what traffic is allowed to reach those pods. They may specify ingress or egress or both. Each rule allows traffic which matches both the from and ports sections.\nCreate two new network policies.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/apply_network_policies.files/allow-ui.yaml wget https://eksworkshop.com/calico/stars_policy_demo/apply_network_policies.files/allow-ui-client.yaml  Again, we can examine our file contents by running: cat allow-ui.yaml\nkind: NetworkPolicy apiVersion: extensions/v1beta1 metadata: namespace: stars name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui  cat allow-ui-client.yaml\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: client name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui  Challenge: How do we apply our network policies to allow the traffic we want?\n  Expand here to see the solution   kubectl apply -f allow-ui.yaml kubectl apply -f allow-ui-client.yaml    Upon refreshing your browser, you can see that the management UI can reach all the services, but they cannot communicate with each other.\n"
},
{
	"uri": "/network-policies/calico/stars_policy_demo/directional_traffic/",
	"title": "Allow Directional Traffic",
	"tags": [],
	"description": "",
	"content": " Let\u0026rsquo;s see how we can allow directional traffic from client to frontend and backend.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/directional_traffic.files/backend-policy.yaml wget https://eksworkshop.com/calico/stars_policy_demo/directional_traffic.files/frontend-policy.yaml  Let\u0026rsquo;s examine this backend policy with cat backend-policy.yaml:\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: backend-policy spec: podSelector: matchLabels: role: backend ingress: - from: - \u0026lt;EDIT: UPDATE WITH THE CONFIGURATION NEEDED TO WHITELIST FRONTEND USING PODSELECTOR\u0026gt; ports: - protocol: TCP port: 6379  Challenge: After reviewing the manifest, you\u0026rsquo;ll see we have intentionally left few of the configuration fields for you to EDIT. Please edit the configuration as suggested. You can find helpful info in this Kubernetes documentation\n  Expand here to see the solution   kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: backend-policy spec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379    Let\u0026rsquo;s examine the frontend policy with cat frontend-policy.yaml:\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: frontend-policy spec: podSelector: matchLabels: role: frontend ingress: - from: - \u0026lt;EDIT: UPDATE WITH THE CONFIGURATION NEEDED TO WHITELIST CLIENT USING NAMESPACESELECTOR\u0026gt; ports: - protocol: TCP port: 80  Challenge: Please edit the configuration as suggested. You can find helpful info in this Kubernetes documentation\n   Expand here to see the solution   kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: frontend-policy spec: podSelector: matchLabels: role: frontend ingress: - from: - namespaceSelector: matchLabels: role: client ports: - protocol: TCP port: 80    To allow traffic from frontend service to the backend service apply the following manifest:\nkubectl apply -f backend-policy.yaml  And allow traffic from the client namespace to the frontend service:\nkubectl apply -f frontend-policy.yaml  Upon refreshing your browser, you should be able to see the network policies in action:\nLet\u0026rsquo;s have a look at the backend-policy. Its spec has a podSelector that selects all pods with the label role:backend, and allows ingress from all pods that have the label role:frontend and on TCP port 6379, but not the other way round. Traffic is allowed in one direction on a specific port number.\nspec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379  The frontend-policy is similar, except it allows ingress from namespaces that have the label role: client on TCP port 80.\nspec: podSelector: matchLabels: role: frontend ingress: - from: - namespaceSelector: matchLabels: role: client ports: - protocol: TCP port: 80  "
},
{
	"uri": "/monitoring/prereqs/",
	"title": "Prereqs",
	"tags": [],
	"description": "",
	"content": " Is helm installed? We will use helm to install Prometheus \u0026amp; Grafana monitoring tools for this chapter. Please review installing helm chapter for instructions if you don\u0026rsquo;t have it installed.\nhelm ls  "
},
{
	"uri": "/weave_flux/prereqs/",
	"title": "Prereqs",
	"tags": [],
	"description": "",
	"content": " Is helm installed? We will use helm to install Weave Flux and a sample Helm chart. Check to see if helm is installed:\nhelm version  If helm is not found, see installing helm for instructions.\n\nDoes S3 artifact bucket exist and are IAM service roles created? AWS CodePipeline and AWS CodeBuild both need AWS Identity and Access Management (IAM) service roles to create a Docker image build pipeline.\nIn this step, we are going to create an IAM role and add an inline policy that we will use in the CodeBuild stage to interact with the EKS cluster via kubectl.\nCreate the bucket and roles:\n# Use your account number below ACCOUNT_ID=$(aws sts get-caller-identity | jq -r '.Account') aws s3 mb s3://eksworkshop-${ACCOUNT_ID}-codepipeline-artifacts cd ~/environment wget https://eksworkshop.com/weave_flux/iam.files/cpAssumeRolePolicyDocument.json aws iam create-role --role-name eksworkshop-CodePipelineServiceRole --assume-role-policy-document file://cpAssumeRolePolicyDocument.json wget https://eksworkshop.com/weave_flux/iam.files/cpPolicyDocument.json aws iam put-role-policy --role-name eksworkshop-CodePipelineServiceRole --policy-name codepipeline-access --policy-document file://cpPolicyDocument.json wget https://eksworkshop.com/weave_flux/iam.files/cbAssumeRolePolicyDocument.json aws iam create-role --role-name eksworkshop-CodeBuildServiceRole --assume-role-policy-document file://cbAssumeRolePolicyDocument.json wget https://eksworkshop.com/weave_flux/iam.files/cbPolicyDocument.json aws iam put-role-policy --role-name eksworkshop-CodeBuildServiceRole --policy-name codebuild-access --policy-document file://cbPolicyDocument.json  "
},
{
	"uri": "/deploy/applications/",
	"title": "Deploy our Sample Applications",
	"tags": [],
	"description": "",
	"content": "apiVersion: apps/v1 kind: Deployment metadata: name: ecsdemo-nodejs labels: app: ecsdemo-nodejs namespace: default spec: replicas: 1 selector: matchLabels: app: ecsdemo-nodejs strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app: ecsdemo-nodejs spec: containers: - image: brentley/ecsdemo-nodejs:latest imagePullPolicy: Always name: ecsdemo-nodejs ports: - containerPort: 3000 protocol: TCP  In the sample file above, we describe the service and how it should be deployed. We will write this description to the kubernetes api using kubectl, and kubernetes will ensure our preferences are met as the application is deployed.\nThe containers listen on port 3000, and native service discovery will be used to locate the running containers and communicate with them.\n"
},
{
	"uri": "/network-policies/calico/stars_policy_demo/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Clean up the demo by deleting the namespaces:\nkubectl delete ns client stars management-ui  "
},
{
	"uri": "/statefulset/storageclass/",
	"title": "Define Storageclass",
	"tags": [],
	"description": "",
	"content": " Introduction Dynamic Volume Provisioning allows storage volumes to be created on-demand. StorageClass should be pre-created to define which provisoner should be used and what parameters should be passed when dynamic provisioning is invoked. (See parameters for AWS EBS)\nDefine Storage Class Copy/Paste the following commands into your Cloud9 Terminal.\nmkdir ~/environment/templates cd ~/environment/templates wget https://eksworkshop.com/statefulset/storageclass.files/mysql-storageclass.yml  Check the configuration of mysql-storageclass.yml file by following command.\ncat ~/environment/templates/mysql-storageclass.yml  You can see provisioner is kubernetes.io/aws-ebs and type is gp2 specified as a parameter.\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: mysql-gp2 provisioner: kubernetes.io/aws-ebs parameters: type: gp2 reclaimPolicy: Delete mountOptions: - debug  Create storageclass \u0026ldquo;mysql-gp2\u0026rdquo; by following command.\nkubectl create -f ~/environment/templates/mysql-storageclass.yml  We will specify \u0026ldquo;mysql-gp2\u0026rdquo; as the storageClassName in volumeClaimTemplates at \u0026ldquo;Create StatefulSet\u0026rdquo; section later.\nvolumeClaimTemplates: - metadata: name: data spec: accessModes: [\u0026quot;ReadWriteOnce\u0026quot;] storageClassName: mysql-gp2 resources: requests: storage: 10Gi    Related files   mysql-storageclass.yml  (0 ko)    "
},
{
	"uri": "/helm_root/helm_intro/install/",
	"title": "Install Helm CLI",
	"tags": [],
	"description": "",
	"content": " Before we can get started configuring helm we\u0026rsquo;ll need to first install the command line tools that you will interact with. To do this run the following.\ncd ~/environment curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get \u0026gt; get_helm.sh chmod +x get_helm.sh ./get_helm.sh  Once you install helm, the command will prompt you to run \u0026lsquo;helm init\u0026rsquo;. Do not run \u0026lsquo;helm init\u0026rsquo;. Follow the instructions to configure helm using Kubernetes RBAC and then install tiller as specified below If you accidentally run \u0026lsquo;helm init\u0026rsquo;, you can safely uninstall tiller by running \u0026lsquo;helm reset \u0026ndash;force\u0026rsquo;\n Configure Helm access with RBAC Helm relies on a service called tiller that requires special permission on the kubernetes cluster, so we need to build a Service Account for tiller to use. We\u0026rsquo;ll then apply this to the cluster.\nTo create a new service account manifest:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system EoF  Next apply the config:\nkubectl apply -f ~/environment/rbac.yaml  Then we can install tiller using the helm tooling\nhelm init --service-account tiller  This will install tiller into the cluster which gives it access to manage resources in your cluster.\n"
},
{
	"uri": "/healthchecks/livenessprobe/",
	"title": "Configure Liveness Probe",
	"tags": [],
	"description": "",
	"content": " Configure the Probe Use the command below to create a directory\nmkdir -p ~/environment/healthchecks  Save the manifest as ~/environment/healthchecks/liveness-app.yaml using your favorite editor. You can review the manifest that is described below. In the configuration file, the livenessProbe field determines how kubelet should check the container in order to consider whether it is healthy or not. kubelet uses the periodSeconds field to do frequent check on the Container. In this case, kubelet checks the liveness probe every 5 seconds. The initialDelaySeconds field is used to tell kubelet that it should wait for 5 seconds before doing the first probe. To perform a probe, kubelet sends a HTTP GET request to the server hosting this pod and if the handler for the servers /health returns a success code, then the container is considered healthy. If the handler returns a failure code, the kubelet kills the container and restarts it.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/healthchecks/liveness-app.yaml apiVersion: v1 kind: Pod metadata: name: liveness-app spec: containers: - name: liveness image: brentley/ecsdemo-nodejs livenessProbe: httpGet: path: /health port: 3000 initialDelaySeconds: 5 periodSeconds: 5 EoF  Let\u0026rsquo;s create the pod using the manifest:\nkubectl apply -f ~/environment/healthchecks/liveness-app.yaml  The above command creates a pod with liveness probe.\nkubectl get pod liveness-app  The output looks like below. Notice the RESTARTS\nNAME READY STATUS RESTARTS AGE liveness-app 1/1 Running 0 11s  The kubectl describe command will show an event history which will show any probe failures or restarts.\nkubectl describe pod liveness-app  Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 38s default-scheduler Successfully assigned liveness-app to ip-192-168-18-63.ec2.internal Normal SuccessfulMountVolume 38s kubelet, ip-192-168-18-63.ec2.internal MountVolume.SetUp succeeded for volume \u0026quot;default-token-8bmt2\u0026quot; Normal Pulling 37s kubelet, ip-192-168-18-63.ec2.internal pulling image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Pulled 37s kubelet, ip-192-168-18-63.ec2.internal Successfully pulled image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Created 37s kubelet, ip-192-168-18-63.ec2.internal Created container Normal Started 37s kubelet, ip-192-168-18-63.ec2.internal Started container  Introduce a Failure We will run the next command to send a SIGUSR1 signal to the nodejs application. By issuing this command we will send a kill signal to the application process in the docker runtime.\nkubectl exec -it liveness-app -- /bin/kill -s SIGUSR1 1  Describe the pod after waiting for 15-20 seconds and you will notice the kubelet actions of killing the container and restarting it.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned liveness-app to ip-192-168-18-63.ec2.internal Normal SuccessfulMountVolume 1m kubelet, ip-192-168-18-63.ec2.internal MountVolume.SetUp succeeded for volume \u0026quot;default-token-8bmt2\u0026quot; Warning Unhealthy 30s (x3 over 40s) kubelet, ip-192-168-18-63.ec2.internal Liveness probe failed: Get http://192.168.13.176:3000/health: net/http: request canceled (Client.Timeout exceeded while awaiting headers) Normal Pulling 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal pulling image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Pulled 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal Successfully pulled image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Created 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal Created container Normal Started 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal Started container Normal Killing 0s kubelet, ip-192-168-18-63.ec2.internal Killing container with id docker://liveness:Container failed liveness probe.. Container will be killed and recreated.  When the nodejs application entered a debug mode with SIGUSR1 signal, it did not respond to the health check pings and kubelet killed the container. The container was subject to the default restart policy.\nkubectl get pod liveness-app  The output looks like below:\nNAME READY STATUS RESTARTS AGE liveness-app 1/1 Running 1 12m  Challenge: How can we check the status of the container health checks?\n  Expand here to see the solution   kubectl logs liveness-app  You can also use kubectl logs to retrieve logs from a previous instantiation of a container with --previous flag, in case the container has crashed\nkubectl logs liveness-app --previous  \u0026lt;Output omitted\u0026gt; Example app listening on port 3000! ::ffff:192.168.43.7 - - [20/Nov/2018:22:53:01 +0000] \u0026quot;GET /health HTTP/1.1\u0026quot; 200 16 \u0026quot;-\u0026quot; \u0026quot;kube-probe/1.10\u0026quot; ::ffff:192.168.43.7 - - [20/Nov/2018:22:53:06 +0000] \u0026quot;GET /health HTTP/1.1\u0026quot; 200 17 \u0026quot;-\u0026quot; \u0026quot;kube-probe/1.10\u0026quot; ::ffff:192.168.43.7 - - [20/Nov/2018:22:53:11 +0000] \u0026quot;GET /health HTTP/1.1\u0026quot; 200 17 \u0026quot;-\u0026quot; \u0026quot;kube-probe/1.10\u0026quot; Starting debugger agent. Debugger listening on [::]:5858    "
},
{
	"uri": "/kubeflow/install/",
	"title": "Install",
	"tags": [],
	"description": "",
	"content": " In this chapter, we will install Kubeflow on Amazon EKS cluster. The cluster creation steps are outlined in Launch EKS chapter.\nInstall Kubeflow on Amazon EKS Download 0.6.1+ release of kfctl. This binary will allow you to install Kubeflow on Amazon EKS:\ncurl --silent --location \u0026quot;https://github.com/kubeflow/kubeflow/releases/download/v0.6.1/kfctl_v0.6.1_$(uname -s).tar.gz\u0026quot; | tar xz -C /tmp sudo mv -v /tmp/kfctl /usr/local/bin  Download Kubeflow configuration file:\nCONFIG=~/environment/kfctl_aws.yaml curl -Lo ${CONFIG} https://raw.githubusercontent.com/kubeflow/kubeflow/v0.6.1/bootstrap/config/kfctl_aws.yaml  Customize this configuration file for AWS region and IAM role for your worker nodes:\nsed -i \u0026quot;s@eksctl-kubeflow-aws-nodegroup-ng-a2-NodeInstanceRole-xxxxxxx@$ROLE_NAME@\u0026quot; ${CONFIG} sed -i \u0026quot;s@us-west-2@$AWS_REGION@\u0026quot; ${CONFIG}  Until https://github.com/kubeflow/kubeflow/issues/3827 is fixed, install aws-iam-authenticator:\ncurl -o aws-iam-authenticator https://amazon-eks.s3-us-west-2.amazonaws.com/1.13.7/2019-06-11/bin/linux/amd64/aws-iam-authenticator chmod +x aws-iam-authenticator sudo mv aws-iam-authenticator /usr/local/bin  Set Kubeflow application name:\nexport AWS_CLUSTER_NAME=eksworkshop-eksctl-yourusername export KFAPP=${AWS_CLUSTER_NAME}  Initialize the cluster:\nkfctl init ${KFAPP} --config=${CONFIG} -V  Create and apply AWS and Kubernetes resources in the cluster:\ncd ${KFAPP} kfctl generate all -V kfctl apply all -V  Wait for all pods to be in Running state (this can take a few minutes):\nkubectl get pods -n kubeflow  Validate that GPUs are available:\nkubectl get nodes \u0026quot;-o=custom-columns=NAME:.metadata.name,MEMORY:.status.allocatable.memory,CPU:.status.allocatable.cpu,GPU:.status.allocatable.nvidia\\.com/gpu\u0026quot; NAME MEMORY CPU GPU ip-192-168-54-93.us-east-2.compute.internal 251641628Ki 32 4 ip-192-168-68-80.us-east-2.compute.internal 251641628Ki 32 4  "
},
{
	"uri": "/exposing_service/connecting/",
	"title": "Connecting Applications with Services",
	"tags": [],
	"description": "",
	"content": " Before discussing the Kubernetes approach to networking, it is worthwhile to contrast it with the “normal” way networking works with Docker.\nBy default, Docker uses host-private networking, so containers can talk to other containers only if they are on the same machine. In order for Docker containers to communicate across nodes, there must be allocated ports on the machine’s own IP address, which are then forwarded or proxied to the containers. This obviously means that containers must either coordinate which ports they use very carefully or ports must be allocated dynamically.\nCoordinating ports across multiple developers is very difficult to do at scale and exposes users to cluster-level issues outside of their control. Kubernetes assumes that pods can communicate with other pods, regardless of which host they land on. We give every pod its own cluster-private-IP address so you do not need to explicitly create links between pods or map container ports to host ports. This means that containers within a Pod can all reach each other’s ports on localhost, and all pods in a cluster can see each other without NAT.\nExposing pods to the cluster Create an nginx Pod, and note that it has a container port specification:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/run-my-nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: selector: matchLabels: run: my-nginx replicas: 2 template: metadata: labels: run: my-nginx spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80 EoF  This makes it accessible from any node in your cluster. Check the nodes the Pod is running on:\nkubectl apply -f ~/environment/run-my-nginx.yaml kubectl get pods -l run=my-nginx -o wide  The output being something like this:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE my-nginx-756f645cd7-gsl4g 1/1 Running 0 63s 192.168.59.188 ip-192-168-38-150.us-west-2.compute.internal \u0026lt;none\u0026gt; my-nginx-756f645cd7-t8b6w 1/1 Running 0 63s 192.168.79.210 ip-192-168-92-222.us-west-2.compute.internal \u0026lt;none\u0026gt;  Check your pods’ IPs:\nkubectl get pods -l run=my-nginx -o yaml | grep podIP  Output being like:\n podIP: 192.168.59.188 podIP: 192.168.79.210  Creating a Service So we have pods running nginx in a flat, cluster wide, address space. In theory, you could talk to these pods directly, but what happens when a node dies? The pods die with it, and the Deployment will create new ones, with different IPs. This is the problem a Service solves.\nA Kubernetes Service is an abstraction which defines a logical set of Pods running somewhere in your cluster, that all provide the same functionality. When created, each Service is assigned a unique IP address (also called clusterIP). This address is tied to the lifespan of the Service, and will not change while the Service is alive. Pods can be configured to talk to the Service, and know that communication to the Service will be automatically load-balanced out to some pod that is a member of the Service.\nYou can create a Service for your 2 nginx replicas with kubectl expose:\nkubectl expose deployment/my-nginx  Output:\nservice/my-nginx exposed  This specification will create a Service which targets TCP port 80 on any Pod with the run: my-nginx label, and expose it on an abstracted Service port (targetPort: is the port the container accepts traffic on, port: is the abstracted Service port, which can be any port other pods use to access the Service). View Service API object to see the list of supported fields in service definition. Check your Service:\nkubectl get svc my-nginx  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx ClusterIP 10.100.225.196 \u0026lt;none\u0026gt; 80/TCP 25s  As mentioned previously, a Service is backed by a group of Pods. These Pods are exposed through endpoints. The Service’s selector will be evaluated continuously and the results will be POSTed to an Endpoints object also named my-nginx. When a Pod dies, it is automatically removed from the endpoints, and new Pods matching the Service’s selector will automatically get added to the endpoints. Check the endpoints, and note that the IPs are the same as the Pods created in the first step:\nkubectl describe svc my-nginx  Name: my-nginx Namespace: default Labels: run=my-nginx Annotations: \u0026lt;none\u0026gt; Selector: run=my-nginx Type: ClusterIP IP: 10.100.225.196 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP Endpoints: 192.168.59.188:80,192.168.79.210:80 Session Affinity: None Events: \u0026lt;none\u0026gt;  You should now be able to curl the nginx Service on : from any node in your cluster. Note that the Service IP is completely virtual, it never hits the wire. Let\u0026rsquo;s try that\nkubectl run -i --tty load-generator --image=busybox /bin/sh  To go into the cluster and then try with the ClusterIP:\nwget -q -O - 10.100.225.196  The output will be\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026quot;http://nginx.org/\u0026quot;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026quot;http://nginx.com/\u0026quot;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  To exit, just make a ctrl+D\n"
},
{
	"uri": "/custom_resource_definition/creating_crd/",
	"title": "Creating a CRD",
	"tags": [],
	"description": "",
	"content": "When you create a new CustomResourceDefinition (CRD), the Kubernetes API Server creates a new RESTful resource path for each version you specify. The CRD can be either namespaced or cluster-scoped, as specified in the CRD’s scope field. As with existing built-in objects, deleting a namespace deletes all custom objects in that namespace. CustomResourceDefinitions themselves are non-namespaced and are available to all namespaces.\nFor example, if you save the following CustomResourceDefinition to resourcedefinition.yaml:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/resourcedefinition.yaml apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: # name must match the spec fields below, and be in the form: \u0026lt;plural\u0026gt;.\u0026lt;group\u0026gt; name: crontabs.stable.example.com spec: # group name to use for REST API: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt; group: stable.example.com # list of versions supported by this CustomResourceDefinition versions: - name: v1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt; plural: crontabs # singular name to be used as an alias on the CLI and for display singular: crontab # kind is normally the CamelCased singular type. Your resource manifests use this. kind: CronTab # shortNames allow shorter string to match your resource on the CLI shortNames: - ct EoF  And create it:\nkubectl apply -f ~/environment/resourcedefinition.yaml  It might take a few seconds for the endpoint to be created. You can also watch the Established condition of your CustomResourceDefinition to be true or watch the discovery information of the API server for your resource to show up.\nNow, let\u0026rsquo;s check the recently created CRD.\nkubectl get crd crontabs.stable.example.com  The result will be something like this:\nNAME CREATED AT crontabs.stable.example.com 2019-05-09T16:50:55Z  Now, let\u0026rsquo;s see the Custom Resource in detail:\nkubectl describe crd crontabs.stable.example.com  The output:\nName: crontabs.stable.example.com Namespace: Labels: \u0026lt;none\u0026gt; Annotations: kubectl.kubernetes.io/last-applied-configuration={\u0026quot;apiVersion\u0026quot;:\u0026quot;apiextensions.k8s.io/v1beta1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;CustomResourceDefinition\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;crontabs.stable.example.com\u0026quot;,\u0026quot;names... API Version: apiextensions.k8s.io/v1beta1 Kind: CustomResourceDefinition Metadata: Creation Timestamp: 2019-05-09T16:50:55Z Generation: 1 Resource Version: 3193124 Self Link: /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/crontabs.stable.example.com UID: 9cad2caf-727a-11e9-9fb0-0e8a8b871ace Spec: Additional Printer Columns: JSON Path: .metadata.creationTimestamp Description: CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC. Populated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata Name: Age Type: date Group: stable.example.com Names: Kind: CronTab List Kind: CronTabList Plural: crontabs Short Names: ct Singular: crontab Scope: Namespaced Version: v1 Versions: Name: v1 Served: true Storage: true Status: Accepted Names: Kind: CronTab List Kind: CronTabList Plural: crontabs Short Names: ct Singular: crontab Conditions: Last Transition Time: 2019-05-09T16:50:55Z Message: no conflicts found Reason: NoConflicts Status: True Type: NamesAccepted Last Transition Time: \u0026lt;nil\u0026gt; Message: the initial names have been accepted Reason: InitialNamesAccepted Status: True Type: Established Stored Versions: v1 Events: \u0026lt;none\u0026gt;  Or we can check the resource directly from the Kubernetes API. First, we start the proxy in one tab of the Cloud9 environment:\nkubectl proxy --port=8080 --address='0.0.0.0' --disable-filter=true  And in another tab we check the existance of the Custom Resource\ncurl -i 127.0.0.1:8080/apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/crontabs.stable.example.com  The response being something like this:\nHTTP/1.1 200 OK Audit-Id: ec046098-8373-4c74-8ce7-a6a43951df6e Content-Length: 2582 Content-Type: application/json Date: Thu, 09 May 2019 18:07:05 GMT { \u0026quot;kind\u0026quot;: \u0026quot;CustomResourceDefinition\u0026quot;, \u0026quot;apiVersion\u0026quot;: \u0026quot;apiextensions.k8s.io/v1beta1\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;crontabs.stable.example.com\u0026quot;, \u0026quot;selfLink\u0026quot;: \u0026quot;/apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/crontabs.stable.example.com\u0026quot;, \u0026quot;uid\u0026quot;: \u0026quot;24babfb5-7285-11e9-a54d-0615623ca50e\u0026quot;, \u0026quot;resourceVersion\u0026quot;: \u0026quot;3271016\u0026quot;, \u0026quot;generation\u0026quot;: 1, \u0026quot;creationTimestamp\u0026quot;: \u0026quot;2019-05-09T18:06:18Z\u0026quot;, \u0026quot;annotations\u0026quot;: { \u0026quot;kubectl.kubernetes.io/last-applied-configuration\u0026quot;: \u0026quot;{\\\u0026quot;apiVersion\\\u0026quot;:\\\u0026quot;apiextensions.k8s.io/v1beta1\\\u0026quot;,\\\u0026quot;kind\\\u0026quot;:\\\u0026quot;CustomResourceDefinition\\\u0026quot;,\\\u0026quot;metadata\\\u0026quot;:{\\\u0026quot;annotations\\\u0026quot;:{},\\\u0026quot;name\\\u0026quot;:\\\u0026quot;crontabs.stable.example.com\\\u0026quot;,\\\u0026quot;namespace\\\u0026quot;:\\\u0026quot;\\\u0026quot;},\\\u0026quot;spec\\\u0026quot;:{\\\u0026quot;group\\\u0026quot;:\\\u0026quot;stable.example.com\\\u0026quot;,\\\u0026quot;names\\\u0026quot;:{\\\u0026quot;kind\\\u0026quot;:\\\u0026quot;CronTab\\\u0026quot;,\\\u0026quot;plural\\\u0026quot;:\\\u0026quot;crontabs\\\u0026quot;,\\\u0026quot;shortNames\\\u0026quot;:[\\\u0026quot;ct\\\u0026quot;],\\\u0026quot;singular\\\u0026quot;:\\\u0026quot;crontab\\\u0026quot;},\\\u0026quot;scope\\\u0026quot;:\\\u0026quot;Namespaced\\\u0026quot;,\\\u0026quot;versions\\\u0026quot;:[{\\\u0026quot;name\\\u0026quot;:\\\u0026quot;v1\\\u0026quot;,\\\u0026quot;served\\\u0026quot;:true,\\\u0026quot;storage\\\u0026quot;:true}]}}\\n\u0026quot; } }, \u0026quot;spec\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;stable.example.com\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;names\u0026quot;: { \u0026quot;plural\u0026quot;: \u0026quot;crontabs\u0026quot;, \u0026quot;singular\u0026quot;: \u0026quot;crontab\u0026quot;, \u0026quot;shortNames\u0026quot;: [ \u0026quot;ct\u0026quot; ], \u0026quot;kind\u0026quot;: \u0026quot;CronTab\u0026quot;, \u0026quot;listKind\u0026quot;: \u0026quot;CronTabList\u0026quot; }, \u0026quot;scope\u0026quot;: \u0026quot;Namespaced\u0026quot;, \u0026quot;versions\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;served\u0026quot;: true, \u0026quot;storage\u0026quot;: true } ], \u0026quot;additionalPrinterColumns\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;Age\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;date\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC.\\n\\nPopulated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata\u0026quot;, \u0026quot;JSONPath\u0026quot;: \u0026quot;.metadata.creationTimestamp\u0026quot; } ] }, \u0026quot;status\u0026quot;: { \u0026quot;conditions\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;NamesAccepted\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2019-05-09T18:06:18Z\u0026quot;, \u0026quot;reason\u0026quot;: \u0026quot;NoConflicts\u0026quot;, \u0026quot;message\u0026quot;: \u0026quot;no conflicts found\u0026quot; }, { \u0026quot;type\u0026quot;: \u0026quot;Established\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;lastTransitionTime\u0026quot;: null, \u0026quot;reason\u0026quot;: \u0026quot;InitialNamesAccepted\u0026quot;, \u0026quot;message\u0026quot;: \u0026quot;the initial names have been accepted\u0026quot; } ], \u0026quot;acceptedNames\u0026quot;: { \u0026quot;plural\u0026quot;: \u0026quot;crontabs\u0026quot;, \u0026quot;singular\u0026quot;: \u0026quot;crontab\u0026quot;, \u0026quot;shortNames\u0026quot;: [ \u0026quot;ct\u0026quot; ], \u0026quot;kind\u0026quot;: \u0026quot;CronTab\u0026quot;, \u0026quot;listKind\u0026quot;: \u0026quot;CronTabList\u0026quot; }, \u0026quot;storedVersions\u0026quot;: [ \u0026quot;v1\u0026quot; ] } }  "
},
{
	"uri": "/assigning_pods/node_selector/",
	"title": "nodeSelector",
	"tags": [],
	"description": "",
	"content": " nodeSelector is the simplest recommended form of node selection constraint. nodeSelector is a field of PodSpec. It specifies a map of key-value pairs. For the pod to be eligible to run on a node, the node must have each of the indicated key-value pairs as labels (it can have additional labels as well). The most common usage is one key-value pair.\nAttach a label to the node Run kubectl get nodes to get the names of your cluster’s nodes.\nkubectl get nodes  Output will be like\nNAME STATUS ROLES AGE VERSION ip-192-168-15-64.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 8d v1.12.7 ip-192-168-38-150.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 8d v1.12.7 ip-192-168-86-147.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 7d23h v1.12.7 ip-192-168-92-222.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 8d v1.12.7  Pick out the one that you want to add a label to, and then run\nkubectl label nodes \u0026lt;node-name\u0026gt; \u0026lt;label-key\u0026gt;=\u0026lt;label-value\u0026gt;  to add a label to the node you’ve chosen.\nFor example, if my node name is ‘ip-192-168-15-64.us-west-2.compute.internal’ and my desired label is ‘disktype=ssd’, then I can run\nkubectl label nodes ip-192-168-15-64.us-west-2.compute.internal disktype=ssd  You can verify that it worked by re-running kubectl get nodes \u0026ndash;show-labels and checking that the node now has a label. You can also use kubectl describe node \u0026ldquo;nodename\u0026rdquo; to see the full list of labels of the given node.\nkubectl get nodes --show-labels  Output will be like\nNAME STATUS ROLES AGE VERSION LABELS ip-192-168-15-64.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 8d v1.12.7 alpha.eksctl.io/cluster-name=eksworkshop-eksctl-yourusername,alpha.eksctl.io/instance-id=i-064fdae0afd3cbe8b,alpha.eksctl.io/nodegroup-name=ng-cd62916d,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.large,beta.kubernetes.io/os=linux,disktype=ssd,failure-domain.beta.kubernetes.io/region=us-west-2,failure-domain.beta.kubernetes.io/zone=us-west-2d,kubernetes.io/hostname=ip-192-168-15-64.us-west-2.compute.internal ip-192-168-38-150.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 8d v1.12.7 alpha.eksctl.io/cluster-name=eksworkshop-eksctl-yourusername,alpha.eksctl.io/instance-id=i-0420598c17da0a4b4,alpha.eksctl.io/nodegroup-name=ng-cd62916d,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.large,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-west-2,failure-domain.beta.kubernetes.io/zone=us-west-2c,kubernetes.io/hostname=ip-192-168-38-150.us-west-2.compute.internal ip-192-168-86-147.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 7d23h v1.12.7 alpha.eksctl.io/cluster-name=eksworkshop-eksctl-yourusername,alpha.eksctl.io/instance-id=i-02e33f4429c64e628,alpha.eksctl.io/nodegroup-name=ng-cd62916d,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.large,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-west-2,failure-domain.beta.kubernetes.io/zone=us-west-2b,kubernetes.io/hostname=ip-192-168-86-147.us-west-2.compute.internal ip-192-168-92-222.us-west-2.compute.internal Ready \u0026lt;none\u0026gt; 8d v1.12.7 alpha.eksctl.io/cluster-name=eksworkshop-eksctl-yourusername,alpha.eksctl.io/instance-id=i-02eadff5d2af1ce12,alpha.eksctl.io/nodegroup-name=ng-cd62916d,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.large,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-west-2,failure-domain.beta.kubernetes.io/zone=us-west-2b,kubernetes.io/hostname=ip-192-168-92-222.us-west-2.compute.internal  Add a nodeSelector field to your pod configuration Take whatever pod config file you want to run, and add a nodeSelector section to it, like this. For example, if this is my pod config:\napiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx  Then add a nodeSelector like so:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/pod-nginx.yaml apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd EoF  Then you run\nkubectl apply -f ~/environment/pod-nginx.yaml  And the Pod will get scheduled on the node that you attached the label to. You can verify that it worked by running\nkubectl get pods -o wide  And looking at the “NODE” that the Pod was assigned to\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE nginx 1/1 Running 0 12s 192.168.10.13 ip-192-168-15-64.us-west-2.compute.internal \u0026lt;none\u0026gt;  "
},
{
	"uri": "/batch/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Introduction Batch processing refers to performing units of work, referred to as a job in a repetitive and unattended fashion. Jobs are typically grouped together and processed in batches (hence the name).\nKubernetes includes native support for running Jobs. Jobs can run multiple pods in parallel until receiving a set number of completions. Each pod can contain multiple containers as a single unit of work.\nArgo enhances the batch processing experience by introducing a number of features:\n Steps based declaration of workflows Artifact support Step level inputs \u0026amp; outputs Loops Conditionals Visualization (using Argo Dashboard) \u0026hellip;and more  In this module, we will build a simple Kubernetes Job, recreate that job in Argo, and add common features and workflows for more advanced batch processing.\n"
},
{
	"uri": "/servicemesh_with_istio/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Istio Istio is a completely open source service mesh that layers transparently onto existing distributed applications. It\u0026rsquo;s also a platform, including APIs, that let it integrate into any logging platform, or telemetry or policy system.\nLet\u0026rsquo;s review in more detail what each of the components that make up this service mesh are.\n Envoy\n Processes the inbound/outbound traffic from inter-service and service-to-external-service transparently.  Pilot\n Pilot provides service discovery for the Envoy sidecars, traffic management capabilities for intelligent routing (e.g., A/B tests, canary deployments, etc.), and resiliency (timeouts, retries, circuit breakers, etc.)  Mixer\n Mixer enforces access control and usage policies across the service mesh, and collects telemetry data from the Envoy proxy and other services.  Citadel\n Citadel provides strong service-to-service and end-user authentication with built-in identity and credential management.   "
},
{
	"uri": "/irsa/preparation/",
	"title": "Preparation",
	"tags": [],
	"description": "",
	"content": " Enabling IAM Roles for Service Accounts on your Cluster  The IAM roles for service accounts feature is available on new Amazon EKS Kubernetes version 1.14 clusters, and clusters that were updated to versions 1.14 or 1.13 on or after September 3rd, 2019.  kubectl version --short     1.14 1.13     Client Version: v1.14.6-eks-5047ed Client Version: v1.13.7   Server Version: v1.14.6-eks-5047ed Server Version: v1.13.10-eks-5ac0f1    If your EKS cluster version is lower or not match with above, updating an Amazon EKS Cluster in the User Guide\n  You must use at least version 1.16.232 of the AWS CLI to receive the proper output from this command.  aws --version   aws-cli/1.16.238 Python/2.7.16 Linux/4.14.133-88.112.amzn1.x86_64 botocore/1.12.228\n If your aws cli version is lower than 1.16.232, use Installing the AWS CLI in the User Guide\n  Retrieve OpenID Connect issuer URL  aws eks describe-cluster --name eksworkshop-eksctl-yourusername --query cluster.identity.oidc.issuer --output text  https://oidc.eks.{AWS_REGION}.amazonaws.com/id/D48675832CA65BD10A532F59741CF90B \n"
},
{
	"uri": "/servicemesh_with_appmesh/create_the_k8s_app/",
	"title": "Create the k8s app",
	"tags": [],
	"description": "",
	"content": "To understand App Mesh, its best to also understand any applications that run on top of it. So in this chapter, we\u0026rsquo;ll first walk you through creating a simple EKS-based k8s application called \u0026ldquo;The DJ App\u0026rdquo;.\nArmed with the knowledge of how the DJ app works without a service mesh, you\u0026rsquo;ll better understand the service mesh functionality App Mesh brings to the equation.\n"
},
{
	"uri": "/monitoring/deploy-prometheus/",
	"title": "Deploy Prometheus",
	"tags": [],
	"description": "",
	"content": " Deploy Prometheus First we are going to install Prometheus. In this example, we are primarily going to use the standard configuration, but we do override the storage class. We will use gp2 EBS volumes for simplicity and demonstration purpose. When deploying in production, you would use io1 volumes with desired IOPS and increase the default storage size in the manifests to get better performance. Run the following command:\nkubectl create namespace prometheus helm install stable/prometheus \\ --name prometheus \\ --namespace prometheus \\ --set alertmanager.persistentVolume.storageClass=\u0026quot;gp2\u0026quot; \\ --set server.persistentVolume.storageClass=\u0026quot;gp2\u0026quot;  Make note of the prometheus endpoint in helm response (you will need this later). It should look similar to below:\nThe Prometheus server can be accessed via port 80 on the following DNS name from within your cluster: prometheus-server.prometheus.svc.cluster.local  Check if Prometheus components deployed as expected\nkubectl get all -n prometheus  You should see response similar to below. They should all be Ready and Available\nNAME READY STATUS RESTARTS AGE pod/prometheus-alertmanager-77cfdf85db-s9p48 2/2 Running 0 1m pod/prometheus-kube-state-metrics-74d5c694c7-vqtjd 1/1 Running 0 1m pod/prometheus-node-exporter-6dhpw 1/1 Running 0 1m pod/prometheus-node-exporter-nrfkn 1/1 Running 0 1m pod/prometheus-node-exporter-rtrm8 1/1 Running 0 1m pod/prometheus-pushgateway-d5fdc4f5b-dbmrg 1/1 Running 0 1m pod/prometheus-server-6d665b876-dsmh9 2/2 Running 0 1m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/prometheus-alertmanager ClusterIP 10.100.89.154 \u0026lt;none\u0026gt; 80/TCP 1m service/prometheus-kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 80/TCP 1m service/prometheus-node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 1m service/prometheus-pushgateway ClusterIP 10.100.136.143 \u0026lt;none\u0026gt; 9091/TCP 1m service/prometheus-server ClusterIP 10.100.151.245 \u0026lt;none\u0026gt; 80/TCP 1m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/prometheus-node-exporter 3 3 3 3 3 \u0026lt;none\u0026gt; 1m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/prometheus-alertmanager 1 1 1 1 1m deployment.apps/prometheus-kube-state-metrics 1 1 1 1 1m deployment.apps/prometheus-pushgateway 1 1 1 1 1m deployment.apps/prometheus-server 1 1 1 1 1m NAME DESIRED CURRENT READY AGE replicaset.apps/prometheus-alertmanager-77cfdf85db 1 1 1 1m replicaset.apps/prometheus-kube-state-metrics-74d5c694c7 1 1 1 1m replicaset.apps/prometheus-pushgateway-d5fdc4f5b 1 1 1 1m replicaset.apps/prometheus-server-6d665b876 1 1 1 1m  In order to access the Prometheus server URL, we are going to use the kubectl port-forward command to access the application. In Cloud9, run:\nkubectl port-forward -n prometheus deploy/prometheus-server 8080:9090  In your Cloud9 environment, click Tools / Preview / Preview Running Application. Scroll to the end of the URL and append:\n/targets  In the web UI, you can see all the targets and metrics being monitored by Prometheus:\n"
},
{
	"uri": "/intro_to_rbac/intro/",
	"title": "What is RBAC?",
	"tags": [],
	"description": "",
	"content": " According to the official kubernetes docs:\n Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within an enterprise.\n The core logical components of RBAC are:\nEntity\nA group, user, or service account (an identity representing an application that wants to execute certain operations (actions) and requires permissions to do so).\nResource\nA pod, service, or secret that the entity wants to access using the certain operations.\nRole\nUsed to define rules for the actions the entity can take on various resources.\nRole binding\nThis attaches (binds) a role to an entity, stating that the set of rules define the actions permitted by the attached entity on the specified resources.\nThere are two types of Roles (Role, ClusterRole) and the respective bindings (RoleBinding, ClusterRoleBinding). These differentiate between authorization in a namespace or cluster-wide.\nNamespace\nNamespaces are an excellent way of creating security boundaries, they also provide a unique scope for object names as the \u0026lsquo;namespace\u0026rsquo; name implies. They are intended to be used in multi-tenant environments to create virtual kubernetes clusters on the same physical cluster.\nObjectives for this module In this module, we\u0026rsquo;re going to explore k8s RBAC by creating an IAM user called rbac-user who is authenticated to access the EKS cluster but is only authorized (via RBAC) to list, get, and watch pods and deployments in the \u0026lsquo;rbac-test\u0026rsquo; namespace.\nTo achieve this, we\u0026rsquo;ll create an IAM user, map that user to a kubernetes role, then perform kubernetes actions under that user\u0026rsquo;s context.\n"
},
{
	"uri": "/deploy/deploynodejs/",
	"title": "Deploy NodeJS Backend API",
	"tags": [],
	"description": "",
	"content": "Let’s bring up the NodeJS Backend API!\nCopy/Paste the following commands into your Cloud9 workspace:\ncd ~/environment/ecsdemo-nodejs kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml  We can watch the progress by looking at the deployment status:\nkubectl get deployment ecsdemo-nodejs  "
},
{
	"uri": "/eksctl/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "For this module, we need to download the eksctl binary:\ncurl --silent --location \u0026quot;https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz\u0026quot; | tar xz -C /tmp sudo mv -v /tmp/eksctl /usr/local/bin  Confirm the eksctl command works:\neksctl version  "
},
{
	"uri": "/network-policies/tigera/register/",
	"title": "Register",
	"tags": [],
	"description": "",
	"content": "Tigera Secure Cloud Edition can be enabled through the [AWS Marketplace]. However, for this workshop, Tigera has enabled a 30 day free trial. To register for the free trial, please follow the steps below:\n Go to the Tigera Secure Cloud Edition trial registration website at https://ce.tigera.io and click the register button.\n On the next page fill in your e-mail and company name and use the code EKS30 for the access code.\n Wait for a few minutes for a confirmation e-mail to arrive in your inbox.\n Click on the link in the e-mail and you will be directed to the Tigera support web portal. Create a new password and log into the Tigera Support web portal (you will automatically be directed there). You can come back to the support page in the future by using your login and password.\n Once on the support site, you will have an option to download software. Continue to follow the links until you are prompted to Download Tigera Secure Cloud Edition v1.0.1. When you click on that link, you will be presented with a webpage with instructions to install Tigera Secure Cloud Edition on your cluster. Save that page, as you will need to refer to those instructions in the next step.\n  "
},
{
	"uri": "/spotworkers/workers/",
	"title": "Add EC2 Workers - On-Demand and Spot",
	"tags": [],
	"description": "",
	"content": " We have our EKS Cluster and worker nodes already, but we need some Spot Instances configured as workers. We also need a Node Labeling strategy to identify which instances are Spot and which are on-demand so that we can make more intelligent scheduling decisions. We will use AWS CloudFormation to launch new worker nodes that will connect to the EKS cluster.\nThis template will create a single ASG that leverages the latest feature to mix multiple instance types and purchase as a single K8s nodegroup. Check out this blog: New – EC2 Auto Scaling Groups With Multiple Instance Types \u0026amp; Purchase Options for details.\nRetrieve the Worker Node Instance Profile ARN First, we will need to ensure the ARN Name our workers use is set in our environment:\ntest -n \u0026quot;$INSTANCE_PROFILE_ARN\u0026quot; \u0026amp;\u0026amp; echo INSTANCE_PROFILE_ARN is \u0026quot;$INSTANCE_PROFILE_ARN\u0026quot; || echo INSTANCE_PROFILE_ARN is not set  Copy the Profile ARN for use as a Parameter in the next step. If you receive an error or empty response, expand the steps below to export.\n  Expand here if you need to export the Instance Profile ARN   If INSTANCE_PROFILE_ARN is not set, please review: /eksctl/test/\n  # Example Output INSTANCE_PROFILE_ARN is arn:aws:iam::123456789101:instance-profile/eksctl-eksworkshop-eksctl-yourusername-nodegroup-ng-abcd1234-NodeInstanceProfile-ABCDEF1234  Retrieve the Security Group Name We also need to collect the ID of the security group used with the existing worker nodes.\nSTACK_NAME=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-yourusername-nodegroup) SG_ID=$(aws cloudformation describe-stack-resources --stack-name $STACK_NAME --logical-resource-id SG | jq -r '.StackResources[].PhysicalResourceId') echo $SG_ID  # Example Output sg-0d9fb7e709dff5675  Launch the CloudFormation Stack We will launch the CloudFormation template as a new set of worker nodes, but it\u0026rsquo;s also possible to update the nodegroup CloudFormation stack created by the eksctl tool.\nClick the Launch button to create the CloudFormation stack in the AWS Management Console.\n   Launch template       EKS Workers - Spot and On Demand  Launch    Download       Confirm the region is correct based on where you\u0026rsquo;ve deployed your cluster.\n Once the console is open you will need to configure the missing parameters. Use the table below for guidance.\n   Parameter Value     Stack Name: eksworkshop-spot-workers   Cluster Name: eksworkshop-eksctl-yourusername (or whatever you named your cluster)   ClusterControlPlaneSecurityGroup: Select from the dropdown. It will contain your cluster name and the words \u0026lsquo;ControlPlaneSecurityGroup\u0026rsquo;   NodeInstanceProfile: Use the Instance Profile ARN that copied in the step above. (e.g.eks-workshop-nodegroup)   UseExistingNodeSecurityGroups: Leave as \u0026lsquo;Yes\u0026rsquo;   ExistingNodeSecurityGroups: Use the SG name that copied in the step above. (e.g. sg-0123456789abcdef)   NodeImageId: Visit this link and select the non-GPU image for your region - Check for empty spaces in copy/paste   KeyName: SSH Key Pair created earlier or any valid key will work   NodeGroupName: Leave as spotworkers   VpcId: Select your workshop VPC from the dropdown   Subnets: Select the 3 private subnets for your workshop VPC from the dropdown   BootstrapArgumentsForOnDemand: --kubelet-extra-args --node-labels=lifecycle=OnDemand   BootstrapArgumentsForSpotFleet: --kubelet-extra-args '--node-labels=lifecycle=Ec2Spot --register-with-taints=spotInstance=true:PreferNoSchedule'    What\u0026rsquo;s going on with Bootstrap Arguments? The EKS Bootstrap.sh script is packaged into the EKS Optimized AMI that we are using, and only requires a single input, the EKS Cluster name. The bootstrap script supports setting any kubelet-extra-args at runtime. We have configured node-labels so that kubernetes knows what type of nodes we have provisioned. We set the lifecycle for the nodes as OnDemand or Ec2Spot. We are also tainting with PreferNoSchedule to prefer pods not be scheduled on Spot Instances. This is a “preference” or “soft” version of NoSchedule – the system will try to avoid placing a pod that does not tolerate the taint on the node, but it is not required.\nYou can leave the rest of the default parameters as is and continue through the remaining CloudFormation screens. Check the box next to I acknowledge that AWS CloudFormation might create IAM resources and click Create\nThe creation of the workers will take about 3 minutes.\n Confirm the Nodes Confirm that the new nodes joined the cluster correctly. You should see 2-3 more nodes added to the cluster.\nkubectl get nodes  You can use the node-labels to identify the lifecycle of the nodes\nkubectl get nodes --show-labels --selector=lifecycle=Ec2Spot  The output of this command should return 2 nodes. At the end of the node output, you should see the node label lifecycle=Ec2Spot\nNow we will show all nodes with the lifecycle=OnDemand. The output of this command should return 1 node as configured in our CloudFormation template.\nkubectl get nodes --show-labels --selector=lifecycle=OnDemand  You can use the kubectl describe nodes with one of the spot nodes to see the taints applied to the EC2 Spot Instances.\n"
},
{
	"uri": "/scaling/deploy_hpa/",
	"title": "Configure Horizontal Pod AutoScaler (HPA)",
	"tags": [],
	"description": "",
	"content": " Deploy the Metrics Server Metrics Server is a cluster-wide aggregator of resource usage data. These metrics will drive the scaling behavior of the deployments. We will deploy the metrics server using Helm configured in a previous module\nhelm install stable/metrics-server \\ --name metrics-server \\ --version 2.0.4 \\ --namespace metrics  Confirm the Metrics API is available. Return to the terminal in the Cloud9 Environment\nkubectl get apiservice v1beta1.metrics.k8s.io -o yaml  If all is well, you should see a status message similar to the one below in the response\nstatus: conditions: - lastTransitionTime: 2018-10-15T15:13:13Z message: all checks passed reason: Passed status: \u0026quot;True\u0026quot; type: Available  We are now ready to scale a deployed application "
},
{
	"uri": "/logging/prereqs/",
	"title": "Configure IAM Policy for Worker Nodes",
	"tags": [],
	"description": "",
	"content": "We will be deploying Fluentd as a DaemonSet, or one pod per worker node. The fluentd log daemon will collect logs and forward to CloudWatch Logs. This will require the nodes to have permissions to send logs and create log groups and log streams. This can be accomplished with an IAM user, IAM role, or by using a tool like Kube2IAM.\nIn our example, we will create an IAM policy and attach it the the Worker node role.\nFirst, we will need to ensure the Role Name our workers use is set in our environment:\ntest -n \u0026quot;$ROLE_NAME\u0026quot; \u0026amp;\u0026amp; echo ROLE_NAME is \u0026quot;$ROLE_NAME\u0026quot; || echo ROLE_NAME is not set  If you receive an error or empty response, expand the steps below to export.\n  Expand here if you need to export the Role Name   If ROLE_NAME is not set, please review: /eksctl/test/\n  mkdir ~/environment/iam_policy cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/iam_policy/k8s-logs-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: [ \u0026quot;logs:DescribeLogGroups\u0026quot;, \u0026quot;logs:DescribeLogStreams\u0026quot;, \u0026quot;logs:CreateLogGroup\u0026quot;, \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;logs:PutLogEvents\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot; } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker --policy-document file://~/environment/iam_policy/k8s-logs-policy.json  Validate that the policy is attached to the role\naws iam get-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker  "
},
{
	"uri": "/statefulset/configmap/",
	"title": "Create ConfigMap",
	"tags": [],
	"description": "",
	"content": " Introduction ConfigMap allow you to decouple configuration artifacts and secrets from image content to keep containerized applications portable. Using ConfigMap, you can independently control MySQL configuration.\nCreate ConfigMap Copy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/templates wget https://eksworkshop.com/statefulset/configmap.files/mysql-configmap.yml  Check the configuration of mysql-configmap.yml file by following command.\ncat ~/environment/templates/mysql-configmap.yml  ConfigMap stores master.cnf, slave.cnf and pass them when initializing master and slave pods defined in statefulset. master.cnf is for the MySQL master pod which has binary log option (log-bin) to provides a record of the data changes to be sent to slave servers and slave.cnf is for slave pods which has super-read-only option.\napiVersion: v1 kind: ConfigMap metadata: name: mysql-config labels: app: mysql data: master.cnf: | # Apply this config only on the master. [mysqld] log-bin slave.cnf: | # Apply this config only on slaves. [mysqld] super-read-only  Create configmap \u0026ldquo;mysql-config\u0026rdquo; by following command.\nkubectl create -f ~/environment/templates/mysql-configmap.yml    Related files   mysql-configmap.yml  (0 ko)    "
},
{
	"uri": "/helm_root/helm_micro/create_chart/",
	"title": "Create a Chart",
	"tags": [],
	"description": "",
	"content": "Helm charts have a structure similar to:\n/eksdemo /Chart.yaml # a description of the chart /values.yaml # defaults, may be overridden during install or upgrade /charts/ # May contain subcharts /templates/ # the template files themselves ...  We\u0026rsquo;ll follow this template, and create a new chart called eksdemo with the following commands:\ncd ~/environment helm create eksdemo  "
},
{
	"uri": "/dashboard/dashboard/",
	"title": "Deploy the Official Kubernetes Dashboard",
	"tags": [],
	"description": "",
	"content": "The official Kubernetes dashboard is not deployed by default, but there are instructions in the official documentation\nWe can deploy the dashboard with the following command:\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml  Since this is deployed to our private cluster, we need to access it via a proxy. Kube-proxy is available to proxy our requests to the dashboard service. In your workspace, run the following command:\nkubectl proxy --port=8080 --address='0.0.0.0' --disable-filter=true \u0026amp;  This will start the proxy, listen on port 8080, listen on all interfaces, and will disable the filtering of non-localhost requests.\nThis command will continue to run in the background of the current terminal\u0026rsquo;s session.\nWe are disabling request filtering, a security feature that guards against XSRF attacks. This isn\u0026rsquo;t recommended for a production environment, but is useful for our dev environment.\n "
},
{
	"uri": "/servicemesh_with_appmesh/create_the_k8s_app/prereqs/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": " At an AWS Event If you are running this chapter at an AWS Event, the prerequisites have already been met, and you can now move forward to the next chapter.\nOn Your Own If you are running this chapter on your own, your environment must meet the following requirements:\nAWS CLI The minimal supported version of the AWS CLI supported is 1.16.133.\nThe jq utility The jq utility is required by some of this module\u0026rsquo;s scripts. Make sure that you have it installed on the machine from which you run the tutorial steps.\nKubernetes and kubectl The minimal Kubernetes and kubectl versions supported are 1.11. You need a Kubernetes cluster deployed on Amazon Elastic Compute Cloud (Amazon EC2) or on an Amazon EKS cluster. Although the steps in this tutorial demonstrate using App Mesh on Amazon EKS, the instructions also work on upstream k8s running on Amazon EC2.\n"
},
{
	"uri": "/healthchecks/readinessprobe/",
	"title": "Configure Readiness Probe",
	"tags": [],
	"description": "",
	"content": " Configure the Probe Save the text from following block as ~/environment/healthchecks/readiness-deployment.yaml. The readinessProbe definition explains how a linux command can be configured as healthcheck. We create an empty file /tmp/healthy to configure readiness probe and use the same to understand how kubelet helps to update a deployment with only healthy pods.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/healthchecks/readiness-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: readiness-deployment spec: replicas: 3 selector: matchLabels: app: readiness-deployment template: metadata: labels: app: readiness-deployment spec: containers: - name: readiness-deployment image: alpine command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;touch /tmp/healthy \u0026amp;\u0026amp; sleep 86400\u0026quot;] readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 3 EoF  We will now create a deployment to test readiness probe:\nkubectl apply -f ~/environment/healthchecks/readiness-deployment.yaml  The above command creates a deployment with 3 replicas and readiness probe as described in the beginning.\nkubectl get pods -l app=readiness-deployment  The output looks similar to below:\n NAME READY STATUS RESTARTS AGE readiness-deployment-7869b5d679-922mx 1/1 Running 0 31s readiness-deployment-7869b5d679-vd55d 1/1 Running 0 31s readiness-deployment-7869b5d679-vxb6g 1/1 Running 0 31s  Let us also confirm that all the replicas are available to serve traffic when a service is pointed to this deployment.\nkubectl describe deployment readiness-deployment | grep Replicas:  The output looks like below:\nReplicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable  Introduce a Failure Pick one of the pods from above 3 and issue a command as below to delete the /tmp/healthy file which makes the readiness probe fail.\nkubectl exec -it \u0026lt;YOUR-READINESS-POD-NAME\u0026gt; -- rm /tmp/healthy  readiness-deployment-7869b5d679-922mx was picked in our example cluster. The /tmp/healthy file was deleted. This file must be present for the readiness check to pass. Below is the status after issuing the command.\nkubectl get pods -l app=readiness-deployment  The output looks similar to below:\nNAME READY STATUS RESTARTS AGE readiness-deployment-7869b5d679-922mx 0/1 Running 0 4m readiness-deployment-7869b5d679-vd55d 1/1 Running 0 4m readiness-deployment-7869b5d679-vxb6g 1/1 Running 0 4m  Traffic will not be routed to the first pod in the above deployment. The ready column confirms that the readiness probe for this pod did not pass and hence was marked as not ready.\nWe will now check for the replicas that are available to serve traffic when a service is pointed to this deployment.\nkubectl describe deployment readiness-deployment | grep Replicas:  The output looks like below:\nReplicas: 3 desired | 3 updated | 3 total | 2 available | 1 unavailable  When the readiness probe for a pod fails, the endpoints controller removes the pod from list of endpoints of all services that match the pod.\nChallenge: How would you restore the pod to Ready status?   Expand here to see the solution   Run the below command with the name of the pod to recreate the /tmp/healthy file. Once the pod passes the probe, it gets marked as ready and will begin to receive traffic again.\nkubectl exec -it \u0026lt;YOUR-READINESS-POD-NAME\u0026gt; -- touch /tmp/healthy  kubectl get pods -l app=readiness-deployment   \n"
},
{
	"uri": "/codepipeline/role/",
	"title": "Create IAM Role",
	"tags": [],
	"description": "",
	"content": "In an AWS CodePipeline, we are going to use AWS CodeBuild to deploy a sample Kubernetes service. This requires an AWS Identity and Access Management (IAM) role capable of interacting with the EKS cluster.\nIn this step, we are going to create an IAM role and add an inline policy that we will use in the CodeBuild stage to interact with the EKS cluster via kubectl.\nCreate the role:\ncd ~/environment TRUST=\u0026quot;{ \\\u0026quot;Version\\\u0026quot;: \\\u0026quot;2012-10-17\\\u0026quot;, \\\u0026quot;Statement\\\u0026quot;: [ { \\\u0026quot;Effect\\\u0026quot;: \\\u0026quot;Allow\\\u0026quot;, \\\u0026quot;Principal\\\u0026quot;: { \\\u0026quot;AWS\\\u0026quot;: \\\u0026quot;arn:aws:iam::${ACCOUNT_ID}:root\\\u0026quot; }, \\\u0026quot;Action\\\u0026quot;: \\\u0026quot;sts:AssumeRole\\\u0026quot; } ] }\u0026quot; echo '{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;eks:Describe*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }' \u0026gt; /tmp/iam-role-policy aws iam create-role --role-name EksWorkshopCodeBuildKubectlRole --assume-role-policy-document \u0026quot;$TRUST\u0026quot; --output text --query 'Role.Arn' aws iam put-role-policy --role-name EksWorkshopCodeBuildKubectlRole --policy-name eks-describe --policy-document file:///tmp/iam-role-policy  "
},
{
	"uri": "/network-policies/calico/",
	"title": "Create Network Policies Using Calico",
	"tags": [],
	"description": "",
	"content": "In this Chapter, we will create some network policies using Calico and see the rules in action.\nNetwork policies allow you to define rules that determine what type of traffic is allowed to flow between different services. Using network policies you can also define rules to restrict traffic. They are a means to improve your cluster\u0026rsquo;s security.\nFor example, you can only allow traffic from frontend to backend in your application.\nNetwork policies also help in isolating traffic within namespaces. For instance, if you have separate namespaces for development and production, you can prevent traffic flow between them by restrict pod to pod communication within the same namespace.\n"
},
{
	"uri": "/weave_flux/githubsetup/",
	"title": "GitHub Setup",
	"tags": [],
	"description": "",
	"content": "We are going to create 2 GitHub repositories. One will be used for a sample application that will trigger a Docker image build. Another will be used to hold Kubernetes manifests that Weave Flux deploys into the cluster. Note this is a pull based method compared to other continuous deployment tools that push to Kubernetes.\nCreate the sample application repository by clicking here.\nFill in the form with repository name, description, and check initializing the repository with a README as shown below and click Create repository.\nRepeat this process to create the Kubernetes manifests repositories by clicking here. Fill in the form as shown below and click Create repository.\nThe next step is to create a personal access token that will allow CodePipeline to receive callbacks from GitHub.\nOnce created, an access token can be stored in a secure enclave and reused, so this step is only required during the first run or when you need to generate new keys.\nOpen up the New personal access page in GitHub.\nYou may be prompted to enter your GitHub password\n Enter a value for Token description, check the repo permission scope and scroll down and click the Generate token button\nCopy the personal access token and save it in a secure place for the next step\nWe will need to revisit GitHub one more time once we provision Weave Flux to enable Weave to control repositories. However, at this time you can move on.\n"
},
{
	"uri": "/x-ray/role/",
	"title": "Modify IAM Role",
	"tags": [],
	"description": "",
	"content": "In order for the X-Ray daemon to communicate with the service, we need to add a policy to the worker nodes\u0026rsquo; AWS Identity and Access Management (IAM) role.\nFirst, we will need to ensure the Role Name our workers use is set in our environment:\ntest -n \u0026quot;$ROLE_NAME\u0026quot; \u0026amp;\u0026amp; echo ROLE_NAME is \u0026quot;$ROLE_NAME\u0026quot; || echo ROLE_NAME is not set    Expand here if you need to export the Role Name   If ROLE_NAME is not set, please review: /eksctl/test/\n  # Example Output ROLE_NAME is eks-workshop-nodegroup  aws iam attach-role-policy --role-name $ROLE_NAME \\ --policy-arn arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess  "
},
{
	"uri": "/prerequisites/",
	"title": "Start the workshop...",
	"tags": [],
	"description": "",
	"content": " Getting Started Continue with Create a Workspace\n"
},
{
	"uri": "/advanced-networking/secondary_cidr/",
	"title": "Using Secondary CIDRs with EKS",
	"tags": [],
	"description": "",
	"content": " Using Secondary CIDRs with EKS You can expand your VPC network by adding additional CIDR ranges. This capability can be used if you are running out of IP ranges within your existing VPC or if you have consumed all available RFC 1918 CIDR ranges within your corporate network. EKS supports additional IPv4 CIDR blocks in the 100.64.0.0/10 and 198.19.0.0/16 ranges. You can review this announcement from our what\u0026rsquo;s new blog\nIn this tutorial, we will walk you through the configuration that is needed so that you can launch your Pod networking on top of secondary CIDRs\n"
},
{
	"uri": "/exposing_service/accessing/",
	"title": "Accessing the Service",
	"tags": [],
	"description": "",
	"content": " Accessing the Service Kubernetes supports 2 primary modes of finding a Service - environment variables and DNS. The former works out of the box while the latter requires the CoreDNS cluster addon.\nEnvironment Variables When a Pod runs on a Node, the kubelet adds a set of environment variables for each active Service. This introduces an ordering problem. To see why, inspect the environment of your running nginx Pods (your Pod name will be different): Let\u0026rsquo;s view the pods again:\nkubectl get pods -l run=my-nginx -o wide  Output:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE my-nginx-756f645cd7-gsl4g 1/1 Running 0 22m 192.168.59.188 ip-192-168-38-150.us-west-2.compute.internal \u0026lt;none\u0026gt; my-nginx-756f645cd7-t8b6w 1/1 Running 0 22m 192.168.79.210 ip-192-168-92-222.us-west-2.compute.internal \u0026lt;none\u0026gt;  Now let\u0026rsquo;s inspect the environment of your running nginx Pods (your Pod name will be different):\nkubectl exec my-nginx-3800858182-jr4a2 -- printenv | grep SERVICE  KUBERNETES_SERVICE_PORT_HTTPS=443 TEST_SERVICE_HOST=10.100.36.158 TEST_SERVICE_PORT=80 KUBERNETES_SERVICE_PORT=443 KUBERNETES_SERVICE_HOST=10.100.0.1  Note there’s no mention of your Service. This is because you created the replicas before the Service. Another disadvantage of doing this is that the scheduler might put both Pods on the same machine, which will take your entire Service down if it dies. We can do this the right way by killing the 2 Pods and waiting for the Deployment to recreate them. This time around the Service exists before the replicas. This will give you scheduler-level Service spreading of your Pods (provided all your nodes have equal capacity), as well as the right environment variables:\nkubectl scale deployment my-nginx --replicas=0; kubectl scale deployment my-nginx --replicas=2;  kubectl get pods -l run=my-nginx -o wide  Output just in the moment of change:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE my-nginx-756f645cd7-9tgkw 1/1 Running 0 6s 192.168.14.67 ip-192-168-15-64.us-west-2.compute.internal \u0026lt;none\u0026gt; my-nginx-756f645cd7-gsl4g 0/1 Terminating 0 25m 192.168.59.188 ip-192-168-38-150.us-west-2.compute.internal \u0026lt;none\u0026gt; my-nginx-756f645cd7-ljjgq 1/1 Running 0 6s 192.168.63.80 ip-192-168-38-150.us-west-2.compute.internal \u0026lt;none\u0026gt; my-nginx-756f645cd7-t8b6w 0/1 Terminating 0 25m 192.168.79.210 ip-192-168-92-222.us-west-2.compute.internal \u0026lt;none\u0026gt;  You may notice that the pods have different names, since they are killed and recreated.\nkubectl exec my-nginx-3800858182-e9ihh -- printenv | grep SERVICE  MY_NGINX_SERVICE_HOST=10.100.225.196 TEST_SERVICE_HOST=10.100.36.158 MY_NGINX_SERVICE_PORT=80 KUBERNETES_SERVICE_HOST=10.100.0.1 KUBERNETES_SERVICE_PORT=443 TEST_SERVICE_PORT=80 KUBERNETES_SERVICE_PORT_HTTPS=443  DNS Kubernetes offers a DNS cluster addon Service that automatically assigns dns names to other Services. You can check if it’s running on your cluster:\nkubectl get services kube-dns --namespace=kube-system  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.0.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 8m  If it isn’t running, you can enable it. The rest of this section will assume you have a Service with a long lived IP (my-nginx), and a DNS server that has assigned a name to that IP (the CoreDNS cluster addon), so you can talk to the Service from any pod in your cluster using standard methods (e.g. gethostbyname). Let’s run another curl application to test this:\nkubectl run curl --image=radial/busyboxplus:curl -i --tty  Output:\nWaiting for pod default/curl-131556218-9fnch to be running, status is Pending, pod ready: false Hit enter for command prompt  Then, hit enter and run\nnslookup my-nginx  Output:\nkubectl run curl --image=radial/busyboxplus:curl -i --tty If you don't see a command prompt, try pressing enter. [ root@curl-5cc7b478b6-d8hqn:/ ]$ nslookup my-nginx Server: 10.100.0.10 Server: 10.100.0.10 Address 1: 10.100.0.10 kube-dns.kube-system.svc.cluster.local Name: my-nginx Address 1: 10.100.225.196 my-nginx.default.svc.cluster.local  "
},
{
	"uri": "/assigning_pods/affinity/",
	"title": "Affinity and anti-affinity",
	"tags": [],
	"description": "",
	"content": " Affinity and anti-affinity nodeSelector provides a very simple way to constrain pods to nodes with particular labels. The affinity/anti-affinity feature, currently in beta, greatly extends the types of constraints you can express. The key enhancements are:\n The language is more expressive (not just “AND of exact match”) You can indicate that the rule is “soft”/“preference” rather than a hard requirement, so if the scheduler can’t satisfy it, the pod will still be scheduled You can constrain against labels on other pods running on the node (or other topological domain), rather than against labels on the node itself, which allows rules about which pods can and cannot be co-located  The affinity feature consists of two types of affinity, “node affinity” and “inter-pod affinity/anti-affinity”. Node affinity is like the existing nodeSelector (but with the first two benefits listed above), while inter-pod affinity/anti-affinity constrains against pod labels rather than node labels, as described in the third item listed above, in addition to having the first and second properties listed above.\nNode affinity (beta feature) Node affinity was introduced as alpha in Kubernetes 1.2. Node affinity is conceptually similar to nodeSelector – it allows you to constrain which nodes your pod is eligible to be scheduled on, based on labels on the node.\nThere are currently two types of node affinity, called requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution.\nYou can think of them as “hard” and “soft” respectively, in the sense that the former specifies rules that must be met for a pod to be scheduled onto a node (just like nodeSelector but using a more expressive syntax), while the latter specifies preferences that the scheduler will try to enforce but will not guarantee. The “IgnoredDuringExecution” part of the names means that, similar to how nodeSelector works, if labels on a node change at runtime such that the affinity rules on a pod are no longer met, the pod will still continue to run on the node.\nThus an example of requiredDuringSchedulingIgnoredDuringExecution would be “only run the pod on nodes with Intel CPUs” and an example preferredDuringSchedulingIgnoredDuringExecution would be “try to run this set of pods in availability zone XYZ, but if it’s not possible, then allow some to run elsewhere”.\nNode affinity is specified as field nodeAffinity of field affinity in the PodSpec.\nLet\u0026rsquo;s see an example of a pod that uses node affinity:\nWe are going to create another label in the same node that in the last example:\nkubectl label nodes ip-192-168-15-64.us-west-2.compute.internal azname=az1  And create an affinity:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/pod-with-node-affinity.yaml apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: azname operator: In values: - az1 - az2 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: another-node-label-key operator: In values: - another-node-label-value containers: - name: with-node-affinity image: k8s.gcr.io/pause:2.0 EoF  This node affinity rule says the pod can only be placed on a node with a label whose key is azname and whose value is either az1 or az2. In addition, among nodes that meet that criteria, nodes with a label whose key is another-node-label-key and whose value is another-node-label-value should be preferred.\nLet\u0026rsquo;s apply this\nkubectl apply -f ~/environment/pod-with-node-affinity.yaml  And check if it worked with kubectl get pods -o wide\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE nginx 1/1 Running 0 35m 192.168.10.13 ip-192-168-15-64.us-west-2.compute.internal \u0026lt;none\u0026gt; with-node-affinity 1/1 Running 0 29s 192.168.14.121 ip-192-168-15-64.us-west-2.compute.internal \u0026lt;none\u0026gt;  Now let\u0026rsquo;s try to put the affinity in another node We are going to put the label in a different node so first, let\u0026rsquo;s clean the label and delete the Pod.\nkubectl delete -f ~/environment/pod-with-node-affinity.yaml kubectl label nodes ip-192-168-15-64.us-west-2.compute.internal azname-  We are putting the label to the node ip-192-168-86-147.us-west-2.compute.internal now\nkubectl label nodes ip-192-168-86-147.us-west-2.compute.internal azname=az1 kubectl apply -f ~/environment/pod-with-node-affinity.yaml  And check if it works with kubectl get pods -o wide\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE nginx 1/1 Running 0 43m 192.168.10.13 ip-192-168-15-64.us-west-2.compute.internal \u0026lt;none\u0026gt; with-node-affinity 1/1 Running 0 42s 192.168.68.249 ip-192-168-86-147.us-west-2.compute.internal \u0026lt;none\u0026gt;  You can see the operator In being used in the example. The new node affinity syntax supports the following operators: In, NotIn, Exists, DoesNotExist, Gt, Lt. You can use NotIn and DoesNotExist to achieve node anti-affinity behavior.\n If you specify both nodeSelector and nodeAffinity, both must be satisfied for the pod to be scheduled onto a candidate node. If you specify multiple nodeSelectorTerms associated with nodeAffinity types, then the pod can be scheduled onto a node if one of the nodeSelectorTerms is satisfied. If you specify multiple matchExpressions associated with nodeSelectorTerms, then the pod can be scheduled onto a node only if all matchExpressions can be satisfied. If you remove or change the label of the node where the pod is scheduled, the pod won’t be removed. In other words, the affinity selection works only at the time of scheduling the pod.  The weight field in preferredDuringSchedulingIgnoredDuringExecution is in the range 1-100. For each node that meets all of the scheduling requirements (resource request, RequiredDuringScheduling affinity expressions, etc.), the scheduler will compute a sum by iterating through the elements of this field and adding “weight” to the sum if the node matches the corresponding MatchExpressions. This score is then combined with the scores of other priority functions for the node. The node(s) with the highest total score are the most preferred.\n"
},
{
	"uri": "/custom_resource_definition/creating_co/",
	"title": "Create Custom Objects",
	"tags": [],
	"description": "",
	"content": "After the CustomResourceDefinition object has been created, you can create custom objects. Custom objects can contain custom fields. These fields can contain arbitrary JSON. In the following example, the cronSpec and image custom fields are set in a custom object of kind CronTab. The kind CronTab comes from the spec of the CustomResourceDefinition object you created above.\nIf you save the following YAML to my-crontab.yaml:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/my-crontab.yaml apiVersion: \u0026quot;stable.example.com/v1\u0026quot; kind: CronTab metadata: name: my-new-cron-object spec: cronSpec: \u0026quot;* * * * */5\u0026quot; image: my-awesome-cron-image EoF  and create it:\nkubectl apply -f my-crontab.yaml  You can then manage your CronTab objects using kubectl. For example:\nkubectl get crontab  Should print a list like this:\nNAME AGE my-new-cron-object 6s  Resource names are not case-sensitive when using kubectl, and you can use either the singular or plural forms defined in the CRD, as well as any short names.\nYou can also view the raw YAML data:\nkubectl get ct -o yaml  You should see that it contains the custom cronSpec and image fields from the yaml you used to create it:\napiVersion: v1 items: - apiVersion: stable.example.com/v1 kind: CronTab metadata: creationTimestamp: 2017-05-31T12:56:35Z generation: 1 name: my-new-cron-object namespace: default resourceVersion: \u0026quot;285\u0026quot; selfLink: /apis/stable.example.com/v1/namespaces/default/crontabs/my-new-cron-object uid: 9423255b-4600-11e7-af6a-28d2447dc82b spec: cronSpec: '* * * * */5' image: my-awesome-cron-image kind: List metadata: resourceVersion: \u0026quot;\u0026quot; selfLink: \u0026quot;\u0026quot;  We can also describe the custom object with kubectl:\nkubectl describe crontab  The output being something like this:\nName: my-new-cron-object Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: kubectl.kubernetes.io/last-applied-configuration={\u0026quot;apiVersion\u0026quot;:\u0026quot;stable.example.com/v1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;CronTab\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;my-new-cron-object\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;cronSpe... API Version: stable.example.com/v1 Kind: CronTab Metadata: Creation Timestamp: 2019-05-09T18:10:35Z Generation: 1 Resource Version: 3274450 Self Link: /apis/stable.example.com/v1/namespaces/default/crontabs/my-new-cron-object UID: bdc71d84-7285-11e9-a54d-0615623ca50e Spec: Cron Spec: * * * * */5 Image: my-awesome-cron-image Events: \u0026lt;none\u0026gt;  Or we can check the resource directly from the Kubernetes API. First, we start the proxy in one tab of the Cloud9 environment:\nkubectl proxy --port=8080 --address='0.0.0.0' --disable-filter=true  And in another tab we check the existance of the Custom Resource\ncurl -i 127.0.0.1:8080/apis/stable.example.com/v1/namespaces/default/crontabs/my-new-cron-object  With the output:\nHTTP/1.1 200 OK Audit-Id: 04c5ce6e-5a45-4064-8139-6c2b848bc467 Content-Length: 707 Content-Type: application/json Date: Thu, 09 May 2019 18:18:21 GMT {\u0026quot;apiVersion\u0026quot;:\u0026quot;stable.example.com/v1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;CronTab\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{\u0026quot;kubectl.kubernetes.io/last-applied-configuration\u0026quot;:\u0026quot;{\\\u0026quot;apiVersion\\\u0026quot;:\\\u0026quot;stable.example.com/v1\\\u0026quot;,\\\u0026quot;kind\\\u0026quot;:\\\u0026quot;CronTab\\\u0026quot;,\\\u0026quot;metadata\\\u0026quot;:{\\\u0026quot;annotations\\\u0026quot;:{},\\\u0026quot;name\\\u0026quot;:\\\u0026quot;my-new-cron-object\\\u0026quot;,\\\u0026quot;namespace\\\u0026quot;:\\\u0026quot;default\\\u0026quot;},\\\u0026quot;spec\\\u0026quot;:{\\\u0026quot;cronSpec\\\u0026quot;:\\\u0026quot;* * * * */5\\\u0026quot;,\\\u0026quot;image\\\u0026quot;:\\\u0026quot;my-awesome-cron-image\\\u0026quot;}}\\n\u0026quot;},\u0026quot;creationTimestamp\u0026quot;:\u0026quot;2019-05-09T18:10:35Z\u0026quot;,\u0026quot;generation\u0026quot;:1,\u0026quot;name\u0026quot;:\u0026quot;my-new-cron-object\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;,\u0026quot;resourceVersion\u0026quot;:\u0026quot;3274450\u0026quot;,\u0026quot;selfLink\u0026quot;:\u0026quot;/apis/stable.example.com/v1/namespaces/default/crontabs/my-new-cron-object\u0026quot;,\u0026quot;uid\u0026quot;:\u0026quot;bdc71d84-7285-11e9-a54d-0615623ca50e\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;cronSpec\u0026quot;:\u0026quot;* * * * */5\u0026quot;,\u0026quot;image\u0026quot;:\u0026quot;my-awesome-cron-image\u0026quot;}}  "
},
{
	"uri": "/intro_to_rbac/install_test_pods/",
	"title": "Install Test Pods",
	"tags": [],
	"description": "",
	"content": "In this tutorial, we\u0026rsquo;re going to demonstrate how to provide limited access to pods running in the rbac-test namespace for a user named rbac-user.\nTo do that, let\u0026rsquo;s first create the rbac-test namespace, and then install nginx into it:\nkubectl create namespace rbac-test kubectl create deploy nginx --image=nginx -n rbac-test  To verify the test pods were properly installed, run:\nkubectl get all -n rbac-test  Output should be similar to:\nNAME READY STATUS RESTARTS AGE pod/nginx-5c7588df-8mvxx 1/1 Running 0 48s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx 1/1 1 1 48s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-5c7588df 1 1 1 48s  "
},
{
	"uri": "/x-ray/x-ray-daemon/",
	"title": "Deploy X-Ray DaemonSet",
	"tags": [],
	"description": "",
	"content": "Now that we have modified the IAM role for the worker nodes to permit write operations to the X-Ray service, we are going to deploy the X-Ray DaemonSet to the EKS cluster. The X-Ray daemon will be deployed to each worker node in the EKS cluster. For reference, see the example implementation used in this module.\nThe AWS X-Ray SDKs are used to instrument your microservices. When using the DaemonSet in the example implementation, you need to configure it to point to xray-service.default:2000.\nThe following showcases how to configure the X-Ray SDK for Go. This is merely an example and not a required step in the workshop.\nfunc init() { xray.Configure(xray.Config{ DaemonAddr: \u0026quot;xray-service.default:2000\u0026quot;, LogLevel: \u0026quot;info\u0026quot;, }) }  To deploy the X-Ray DaemonSet:\nkubectl create -f https://eksworkshop.com/x-ray/daemonset.files/xray-k8s-daemonset.yaml  To see the status of the X-Ray DaemonSet:\nkubectl describe daemonset xray-daemon  The folllowing is an example of the command output:\nTo view the logs for all of the X-Ray daemon pods run the following\n kubectl logs -l app=xray-daemon  "
},
{
	"uri": "/codepipeline/configmap/",
	"title": "Modify aws-auth ConfigMap",
	"tags": [],
	"description": "",
	"content": "Now that we have the IAM role created, we are going to add the role to the aws-auth ConfigMap for the EKS cluster.\nOnce the ConfigMap includes this new role, kubectl in the CodeBuild stage of the pipeline will be able to interact with the EKS cluster via the IAM role.\nROLE=\u0026quot; - rolearn: arn:aws:iam::$ACCOUNT_ID:role/EksWorkshopCodeBuildKubectlRole\\n username: build\\n groups:\\n - system:masters\u0026quot; kubectl get -n kube-system configmap/aws-auth -o yaml | awk \u0026quot;/mapRoles: \\|/{print;print \\\u0026quot;$ROLE\\\u0026quot;;next}1\u0026quot; \u0026gt; /tmp/aws-auth-patch.yml kubectl patch configmap/aws-auth -n kube-system --patch \u0026quot;$(cat /tmp/aws-auth-patch.yml)\u0026quot;  If you would like to edit the aws-auth ConfigMap manually, you can run: $ kubectl edit -n kube-system configmap/aws-auth\n "
},
{
	"uri": "/exposing_service/exposing/",
	"title": "Exposing the Service",
	"tags": [],
	"description": "",
	"content": " Exposing the Service For some parts of your applications you may want to expose a Service onto an external IP address. Kubernetes supports two ways of doing this: NodePorts and LoadBalancers.\nkubectl get svc my-nginx  Output\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx ClusterIP 10.100.225.196 \u0026lt;none\u0026gt; 80/TCP 33m  Currently the Service does not have an External IP, so let’s now recreate the Service to use a cloud load balancer, just change the Type of my-nginx Service from ClusterIP to LoadBalancer:\nkubectl edit svc my-nginx  Once edited, we can check for the changes:\nkubectl get svc my-nginx  Output\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx LoadBalancer 10.100.225.196 aca434079a4cb0a9961170c1-23367063.us-west-2.elb.amazonaws.com 80:30470/TCP 39m  Now, let\u0026rsquo;s try if it\u0026rsquo;s accesible. The ELB can take a couple of minutes in being available on the DNS.\ncurl http://\u0026lt;EXTERNAL-IP\u0026gt; -k  Output\n\u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt;  The IP address in the EXTERNAL-IP column is the one that is available on the public internet. The CLUSTER-IP is only available inside your cluster/private cloud network.\nIf the Load Balancer name is too long to fit in the standard kubectl get svc output, you’ll need to do kubectl describe service my-nginx to see it. You’ll see something like this:\nkubectl describe service my-nginx | grep Ingress  Output\nLoadBalancer Ingress: a320587ffd19711e5a37606cf4a74574-1142138393.us-east-1.elb.amazonaws.com  "
},
{
	"uri": "/assigning_pods/affinity_usecases/",
	"title": "More Practical use-cases",
	"tags": [],
	"description": "",
	"content": " More Practical Use-cases AntiAffinity can be even more useful when they are used with higher level collections such as ReplicaSets, StatefulSets, Deployments, etc. One can easily configure that a set of workloads should be co-located in the same defined topology, eg., the same node.\nAlways co-located in the same node In a three node cluster, a web application has in-memory cache such as redis. We want the web-servers to be co-located with the cache as much as possible.\nHere is the yaml snippet of a simple redis deployment with three replicas and selector label app=store. The deployment has PodAntiAffinity configured to ensure the scheduler does not co-locate replicas on a single node.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/redis-with-node-affinity.yaml apiVersion: apps/v1 kind: Deployment metadata: name: redis-cache spec: selector: matchLabels: app: store replicas: 3 template: metadata: labels: app: store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; containers: - name: redis-server image: redis:3.2-alpine EoF  The below yaml snippet of the webserver deployment has podAntiAffinity and podAffinity configured. This informs the scheduler that all its replicas are to be co-located with pods that have selector label app=store. This will also ensure that each web-server replica does not co-locate on a single node.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/web-with-node-affinity.yaml apiVersion: apps/v1 kind: Deployment metadata: name: web-server spec: selector: matchLabels: app: web-store replicas: 3 template: metadata: labels: app: web-store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web-store topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; containers: - name: web-app image: nginx:1.12-alpine EoF  Let\u0026rsquo;s apply this Deployments\nkubectl apply -f ~/environment/redis-with-node-affinity.yaml kubectl apply -f ~/environment/web-with-node-affinity.yaml  If we create the above two deployments, our three node cluster should look like below.\nnode-1 - webserver-1 - cache-1\nnode-2 - webserver-2 - cache-2\nnode-3 - webserver-3 - cache-3\nAs you can see, all the 3 replicas of the web-server are automatically co-located with the cache as expected.\nkubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE redis-cache-1450370735-6dzlj 1/1 Running 0 8m 10.192.4.2 kube-node-3 redis-cache-1450370735-j2j96 1/1 Running 0 8m 10.192.2.2 kube-node-1 redis-cache-1450370735-z73mh 1/1 Running 0 8m 10.192.3.1 kube-node-2 web-server-1287567482-5d4dz 1/1 Running 0 7m 10.192.2.3 kube-node-1 web-server-1287567482-6f7v5 1/1 Running 0 7m 10.192.4.3 kube-node-3 web-server-1287567482-s330j 1/1 Running 0 7m 10.192.3.2 kube-node-2  "
},
{
	"uri": "/x-ray/microservices/",
	"title": "Deploy Example Microservices",
	"tags": [],
	"description": "",
	"content": "We now have the foundation in place to deploy microservices, which are instrumented with X-Ray SDKs, to the EKS cluster.\nIn this step, we are going to deploy example front-end and back-end microservices to the cluster. The example services are already instrumented using the X-Ray SDK for Go. Currently, X-Ray has SDKs for Go, Python, Node.js, Ruby, .NET and Java.\nkubectl apply -f https://eksworkshop.com/x-ray/sample-front.files/x-ray-sample-front-k8s.yml kubectl apply -f https://eksworkshop.com/x-ray/sample-back.files/x-ray-sample-back-k8s.yml  To review the status of the deployments, you can run:\nkubectl describe deployments x-ray-sample-front-k8s x-ray-sample-back-k8s  For the status of the services, run the following command:\nkubectl describe services x-ray-sample-front-k8s x-ray-sample-back-k8s  Once the front-end service is deployed, run the following command to get the Elastic Load Balancer (ELB) endpoint and open it in a browser.\nkubectl get service x-ray-sample-front-k8s -o wide  After your ELB is deployed and available, open up the endpoint returned by the previous command in your browser and allow it to remain open. The front-end application makes a new request to the /api endpoint once per second, which in turn calls the back-end service. The JSON document displayed in the browser is the result of the request made to the back-end service.\nThis service was configured with a LoadBalancer so, an AWS Elastic Load Balancer (ELB) is launched by Kubernetes for the service. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com\u0026rdquo; - the full value is the DNS address.\n When the front-end service is first deployed, it can take up to several minutes for the ELB to be created and DNS updated.\n "
},
{
	"uri": "/codepipeline/forksample/",
	"title": "Fork Sample Repository",
	"tags": [],
	"description": "",
	"content": "We are now going to fork the sample Kubernetes service so that we will be able modify the repository and trigger builds.\nLogin to GitHub and fork the sample service to your own account:\nhttps://github.com/rnzsgh/eks-workshop-sample-api-service-go\nOnce the repo is forked, you can view it in your your GitHub repositories.\nThe forked repo will look like:\n"
},
{
	"uri": "/assigning_pods/cleaning/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": " Cleaning up To delete the resources used in this chapter:\nkubectl delete -f ~/environment/pod-with-node-affinity.yaml kubectl delete -f ~/environment/redis-with-node-affinity.yaml kubectl delete -f ~/environment/web-with-node-affinity.yaml  "
},
{
	"uri": "/exposing_service/ingress/",
	"title": "Ingress",
	"tags": [],
	"description": "",
	"content": " Ingress What is Ingress? Ingress, added in Kubernetes v1.1, exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.\nInternet---[ Ingress ]--|--|--[ Services ]\nAn Ingress can be configured to give services externally-reachable URLs, load balance traffic, terminate SSL, and offer name based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a loadbalancer, though it may also configure your edge router or additional frontends to help handle the traffic.\nAn Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type NodePort or LoadBalancer.\nThe Ingress Resource A minimal ingress resource example:\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /testpath backend: serviceName: test servicePort: 80  As with all other Kubernetes resources, an Ingress needs apiVersion, kind, and metadata fields. Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which is the rewrite-target annotation. Different Ingress controller support different annotations. Review the documentation for your choice of Ingress controller to learn which annotations are supported.\nThe Ingress spec has all the information needed to configure a loadbalancer or proxy server. Most importantly, it contains a list of rules matched against all incoming requests. Ingress resource only supports rules for directing HTTP traffic.\nIngress rules Each http rule contains the following information: - An optional host. In this example, no host is specified, so the rule applies to all inbound HTTP traffic through the IP address specified. If a host is provided (for example, foo.bar.com), the rules apply to that host. - A list of paths (for example, /testpath), each of which has an associated backend defined with a serviceName and servicePort. Both the host and path must match the content of an incoming request before the loadbalancer will direct traffic to the referenced service. - A backend is a combination of service and port names as described in the services doc. HTTP (and HTTPS) requests to the Ingress matching the host and path of the rule will be sent to the listed backend. - A default backend is often configured in an Ingress controller that will service any requests that do not match a path in the spec.\nDefault Backend An Ingress with no rules sends all traffic to a single default backend. The default backend is typically a configuration option of the Ingress controller and is not specified in your Ingress resources.\nIf none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend.\n"
},
{
	"uri": "/codepipeline/githubcredentials/",
	"title": "GitHub Access Token",
	"tags": [],
	"description": "",
	"content": "In order for CodePipeline to receive callbacks from GitHub, we need to generate a personal access token.\nOnce created, an access token can be stored in a secure enclave and reused, so this step is only required during the first run or when you need to generate new keys.\nOpen up the New personal access page in GitHub.\nYou may be prompted to enter your GitHub password\n Enter a value for Token description, check the repo permission scope and scroll down and click the Generate token button\nCopy the personal access token and save it in a secure place for the next step\n"
},
{
	"uri": "/x-ray/x-ray/",
	"title": "X-Ray Console",
	"tags": [],
	"description": "",
	"content": "We now have the example microservices deployed, so we are going to investigate our Service Graph and Traces in X-Ray section of the AWS Management Console.\nThe Service map in the console provides a visual representation of the steps identified by X-Ray for a particular trace. Each resource that sends data to X-Ray within the same context appears as a service in the graph. In the example below, we can see that the x-ray-sample-front-k8s service is processing 39 transactions per minute with an average latency of 0.99ms per operation. Additionally, the x-ray-sample-back-k8s is showing an average latency of 0.08ms per transaction.\nNext, go to the traces section in the AWS Management Console to view the execution times for the segments in the requests. At the top of the page, we can see the URL for the ELB endpoint and the corresponding traces below.\nIf you click on the link on the left in the Trace list section you will see the overall execution time for the request (0.5ms for the x-ray-sample-front-k8s which wraps other segments and subsegments), as well as a breakdown of the individual segments in the request. In this visualization, you can see the front-end and back-end segments and a subsegment named x-ray-sample-back-k8s-gen In the back-end service source code, we instrumented a subsegment that surrounds a random number generator.\nIn the Go example, the main segment is initialized in the xray.Handler helper, which in turn sets all necessary information in the http.Request context struct, so that it can be used when initializing the subsegment.\nClick on the image to zoom\n "
},
{
	"uri": "/custom_resource_definition/cleaning/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": " Cleaning up To delete the Custom Resource Definitions:\nkubectl delete -f resourcedefinition.yaml kubectl get crontabs  "
},
{
	"uri": "/exposing_service/ingress_controller/",
	"title": "Ingress Controller",
	"tags": [],
	"description": "",
	"content": " Ingress Controllers In order for the Ingress resource to work, the cluster must have an ingress controller running.\nUnlike other types of controllers which run as part of the kube-controller-manager binary, Ingress controllers are not started automatically with a cluster. Let\u0026rsquo;s see some options:\n"
},
{
	"uri": "/exposing_service/ingress_controller_alb/",
	"title": "Ingress Controller ALB",
	"tags": [],
	"description": "",
	"content": " ALB Ingress Controller Deploy RBAC Roles and RoleBindings needed by the AWS ALB Ingress controller:\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/rbac-role.yaml  Download the AWS ALB Ingress controller YAML into a local file:\ncurl -sS \u0026quot;https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/alb-ingress-controller.yaml\u0026quot; \u0026gt; alb-ingress-controller.yaml  Edit the AWS ALB Ingress controller YAML to include the clusterName of the Kubernetes (or) Amazon EKS cluster. Edit the –cluster-name flag to be the real name of our Kubernetes (or) Amazon EKS cluster. You can check the Name of the Cluster with the CLI\naws eks list-clusters  Output\n{ \u0026quot;clusters\u0026quot;: [ \u0026quot;eksworkshop-eksctl-yourusername\u0026quot; ] }  Deploy the AWS ALB Ingress controller YAML:\nkubectl apply -f alb-ingress-controller.yaml  Verify that the deployment was successful and the controller started:\nkubectl logs -n kube-system $(kubectl get po -n kube-system | egrep -o alb-ingress[a-zA-Z0-9-]+)  You should be able to see the following output:\n------------------------------------------------------------------------------- AWS ALB Ingress controller Release: v1.0.0 Build: git-c25bc6c5 Repository: https://github.com/kubernetes-sigs/aws-alb-ingress-controller -------------------------------------------------------------------------------  Deploy Sample Application Now let’s deploy a sample 2048 game into our Kubernetes cluster and use the Ingress resource to expose it to traffic:\nDeploy 2048 game resources:\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-namespace.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-deployment.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-service.yaml  Deploy an Ingress resource for the 2048 game:\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-ingress.yaml  After few seconds, verify that the Ingress resource is enabled:\nkubectl get ingress/2048-ingress -n 2048-game  You should be able to see the following output:\nNAME HOSTS ADDRESS PORTS AGE 2048-ingress * DNS-Name-Of-Your-ALB 80 3m  Open a browser. Copy and paste your “DNS-Name-Of-Your-ALB”. You should be to access your newly deployed 2048 game – have fun!\n"
},
{
	"uri": "/x-ray/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing the Tracing with X-Ray module.\nThe content for this module was based on the Application Tracing on Kubernetes with AWS X-Ray blog post.\nThis module is not used in subsequent steps, so you can remove the resources now or at the end of the workshop.\nDelete the Kubernetes example microservices deployed:\nkubectl delete deployments x-ray-sample-front-k8s x-ray-sample-back-k8s kubectl delete services x-ray-sample-front-k8s x-ray-sample-back-k8s  Delete the X-Ray DaemonSet:\nkubectl delete -f https://eksworkshop.com/x-ray/daemonset.files/xray-k8s-daemonset.yaml  aws iam detach-role-policy --role-name $ROLE_NAME --policy-arn arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess  "
},
{
	"uri": "/codepipeline/codepipeline/",
	"title": "CodePipeline Setup",
	"tags": [],
	"description": "",
	"content": " Now we are going to create the AWS CodePipeline using AWS CloudFormation.\nCloudFormation is an infrastructure as code (IaC) tool which provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts.\nEach EKS deployment/service should have its own CodePipeline and be located in an isolated source repository.\nYou can modify the CloudFormation templates provided with this workshop to meet your system requirements to easily onboard new services to your EKS cluster. For each new service the following steps can be repeated.\nClick the Launch button to create the CloudFormation stack in the AWS Management Console.\n   Launch template       CodePipeline \u0026amp; EKS  Launch    Download      After the console is open, enter your GitHub username, personal access token (created in previous step), check the acknowledge box and then click the \u0026ldquo;Create stack\u0026rdquo; button located at the bottom of the page.\nWait for the status to change from \u0026ldquo;CREATE_IN_PROGRESS\u0026rdquo; to CREATE_COMPLETE before moving on to the next step.\nOpen CodePipeline in the Management Console. You will see a CodePipeline that starts with eks-workshop-codepipeline. Click this link to view the details.\nIf you receive a permissions error similar to User x is not authorized to perform: codepipeline:ListPipelines\u0026hellip; upon clicking the above link, the CodePipeline console may have opened up in the wrong region. To correct this, from the Region dropdown in the console, choose the region you provisioned the workshop in. Select Oregon (us-west-2) if you provisioned the workshow per the \u0026ldquo;Start the workshop at an AWS event\u0026rdquo; instructions.\n Once you are on the detail page for the specific CodePipeline, you can see the status along with links to the change and build details.\nIf you click on the \u0026ldquo;details\u0026rdquo; link in the build/deploy stage, you can see the output from the CodeBuild process.\n To review the status of the deployment, you can run:\nkubectl describe deployment hello-k8s  For the status of the service, run the following command:\nkubectl describe service hello-k8s  Challenge: How can we view our exposed service?\nHINT: Which kubectl command will get you the Elastic Load Balancer (ELB) endpoint for this app?\n  Expand here to see the solution   Once the service is built and delivered, we can run the following command to get the Elastic Load Balancer (ELB) endpoint and open it in a browser. If the message is not updated immediately, give Kubernetes some time to deploy the change.\n kubectl get services hello-k8s -o wide  This service was configured with a LoadBalancer so, an AWS Elastic Load Balancer is launched by Kubernetes for the service. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com\u0026rdquo; - the full value is the DNS address.\n   "
},
{
	"uri": "/prerequisites/workspace/",
	"title": "Create a Workspace",
	"tags": [],
	"description": "",
	"content": " The Cloud9 workspace should be built by an IAM user with Administrator privileges, not the root account user. Please ensure you are logged in as an IAM user, not the root account user.\n This workshop was designed to run in the Oregon (us-west-2) region. Please don\u0026rsquo;t run in any other region. Future versions of this workshop will expand region availability, and this message will be removed.\n -- Ad blockers, javascript disablers, and tracking blockers should be disabled for the cloud9 domain, or connecting to the workspace might be impacted. Cloud9 requires third-party-cookies. You can whitelist the specific domains.\n Launch Cloud9 in your closest region:  Oregon Ireland Ohio Singapore  Create a Cloud9 Environment: https://us-west-2.console.aws.amazon.com/cloud9/home?region=us-west-2\n Create a Cloud9 Environment: https://eu-west-1.console.aws.amazon.com/cloud9/home?region=eu-west-1\n Create a Cloud9 Environment: https://us-east-2.console.aws.amazon.com/cloud9/home?region=us-east-2\n Create a Cloud9 Environment: https://ap-southeast-1.console.aws.amazon.com/cloud9/home?region=ap-southeast-1\n  $(function(){$(\"#region\").tabs();});  Select Create environment Name it eksworkshop, and take all other defaults When it comes up, customize the environment by closing the welcome tab and lower work area, and opening a new terminal tab in the main work area:  Your workspace should now look like this:  If you like this theme, you can choose it yourself by selecting View / Themes / Solarized / Solarized Dark in the Cloud9 workspace menu.\n  "
},
{
	"uri": "/exposing_service/cleaning/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": " Cleaning up To delete the resources used in this chapter:\nkubectl delete -f ~/environment/run-my-nginx.yaml kubectl delete -f ~/environment/pod-with-node-affinity.yaml kubectl delete -f ~/environment/redis-with-node-affinity.yaml kubectl delete -f ~/environment/web-with-node-affinity.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-deployment.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-service.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-ingress.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-namespace.yaml kubectl delete -f ~/environment/alb-ingress-controller.yaml  "
},
{
	"uri": "/monitoring/deploy-grafana/",
	"title": "Deploy Grafana",
	"tags": [],
	"description": "",
	"content": " Deploy Grafana We are now going to install Grafana. For this example, we are primarily using the Grafana defaults, but we are overriding several parameters. As with Prometheus, we are setting the storage class to gp2, admin password, configuring the datasource to point to Prometheus and creating an external load balancer for the service.\nkubectl create namespace grafana helm install stable/grafana \\ --name grafana \\ --namespace grafana \\ --set persistence.storageClassName=\u0026quot;gp2\u0026quot; \\ --set adminPassword=\u0026quot;EKS!sAWSome\u0026quot; \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.apiVersion=1 \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].name=Prometheus \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].type=prometheus \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].url=http://prometheus-server.prometheus.svc.cluster.local \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].access=proxy \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].isDefault=true \\ --set service.type=LoadBalancer  Run the following command to check if Grafana is deployed properly:\nkubectl get all -n grafana  You should see similar results. They should all be Ready and Available\nNAME READY STATUS RESTARTS AGE pod/grafana-b9697f8b5-t9w4j 1/1 Running 0 2m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/grafana LoadBalancer 10.100.49.172 abe57f85de73111e899cf0289f6dc4a4-1343235144.us-west-2.elb.amazonaws.com 80:31570/TCP 3m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/grafana 1 1 1 1 2m NAME DESIRED CURRENT READY AGE replicaset.apps/grafana-b9697f8b5 1 1 1 2m  You can get Grafana ELB URL using this command. Copy \u0026amp; Paste the value into browser to access Grafana web UI.\nexport ELB=$(kubectl get svc -n grafana grafana -o jsonpath='{.status.loadBalancer.ingress[0].hostname}') echo \u0026quot;http://$ELB\u0026quot;  When logging in, use the username admin and get the password hash by running the following:\nkubectl get secret --namespace grafana grafana -o jsonpath=\u0026quot;{.data.admin-password}\u0026quot; | base64 --decode ; echo  It can take several minutes before the ELB is up, DNS is propagated and the nodes are registered.\n "
},
{
	"uri": "/statefulset/services/",
	"title": "Create Services",
	"tags": [],
	"description": "",
	"content": " Introduction Kubernetes Service defines a logical set of Pods and a policy by which to access them. Service can be exposed in different ways by specifying a type in the serviceSpec. StatefulSet currently requires a Headless Service to control the domain of its Pods, directly reach each Pod with stable DNS entries. By specifying \u0026ldquo;None\u0026rdquo; for the clusterIP, you can create Headless Service.\nCreate Services Copy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/templates wget https://eksworkshop.com/statefulset/services.files/mysql-services.yml  Check the configuration of mysql-services.yml by following command.\ncat ~/environment/templates/mysql-services.yml  You can see the mysql service is for DNS resolution so that when pods are placed by StatefulSet controller, pods can be resolved using pod-name.mysql. mysql-read is a client service that does load balancing for all slaves.\n# Headless service for stable DNS entries of StatefulSet members. apiVersion: v1 kind: Service metadata: name: mysql labels: app: mysql spec: ports: - name: mysql port: 3306 clusterIP: None selector: app: mysql --- # Client service for connecting to any MySQL instance for reads. # For writes, you must instead connect to the master: mysql-0.mysql. apiVersion: v1 kind: Service metadata: name: mysql-read labels: app: mysql spec: ports: - name: mysql port: 3306 selector: app: mysql  Create service mysql and mysql-read by following command\nkubectl create -f ~/environment/templates/mysql-services.yml    Related files   mysql-services.yml  (0 ko)    "
},
{
	"uri": "/prerequisites/k8stools/",
	"title": "Install Kubernetes Tools",
	"tags": [],
	"description": "",
	"content": " Amazon EKS clusters require kubectl and kubelet binaries and the aws-cli or aws-iam-authenticator binary to allow IAM authentication for your Kubernetes cluster.\nIn this workshop we will give you the commands to download the Linux binaries. If you are running Mac OSX / Windows, please see the official EKS docs for the download links.\n Install kubectl sudo curl --silent --location -o /usr/local/bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.13.7/bin/linux/amd64/kubectl sudo chmod +x /usr/local/bin/kubectl  Install JQ and envsubst sudo yum -y install jq gettext  Verify the binaries are in the path and executable for command in kubectl jq envsubst do which $command \u0026amp;\u0026gt;/dev/null \u0026amp;\u0026amp; echo \u0026quot;$command in path\u0026quot; || echo \u0026quot;$command NOT FOUND\u0026quot; done  "
},
{
	"uri": "/weave_flux/installweaveflux/",
	"title": "Install Weave Flux",
	"tags": [],
	"description": "",
	"content": "Now we will use Helm to install Weave Flux into our cluster and enable it to interact with our Kubernetes configuration GitHub repo.\nFirst, install the Flux Custom Resource Definition:\nkubectl apply -f https://raw.githubusercontent.com/fluxcd/flux/helm-0.10.1/deploy-helm/flux-helm-release-crd.yaml  Check that Tiller is installed.\nhelm ls  When tiller has already been installed, this command should either return a list of helm charts that have already been deployed or nothing.\nIf you get the message Error: could not find tiller, see installing helm for instructions.\n Next, add the Flux chart repository to Helm and install Flux.\nUpdate the Git URL below to match your user name and Kubernetes configuration manifest repository.\n helm repo add fluxcd https://charts.fluxcd.io helm upgrade -i flux \\ --set helmOperator.create=true \\ --set helmOperator.createCRD=false \\ --set git.url=git@github.com:YOURUSER/k8s-config \\ --namespace flux \\ fluxcd/flux  Watch the install and confirm everything starts. There should be 3 pods.\nkubectl get pods -n flux  Install fluxctl in order to get the SSH key to allow GitHub write access. This allows Flux to keep the configuration in GitHub in sync with the configuration deployed in the cluster.\nsudo wget -O /usr/local/bin/fluxctl https://github.com/fluxcd/flux/releases/download/1.14.1/fluxctl_linux_amd64 sudo chmod 755 /usr/local/bin/fluxctl fluxctl version fluxctl identity --k8s-fwd-ns flux  Copy the provided key and add that as a deploy key in the GitHub repository.\n In GitHub, select your k8s-config GitHub repo. Go to Settings and click Deploy Keys. Alternatively, you can go by direct URL by replacing your user name in this URL: github.com/YOURUSER/k8s-config/settings/keys.\n Click on Add Deploy Key  Name: Flux Deploy Key Paste the key output from fluxctl Click Allow Write Access. This allows Flux to keep the repo in sync with the real state of the cluster Click Add Key   Now Flux is configured and should be ready to pull configuration.\n"
},
{
	"uri": "/codepipeline/change/",
	"title": "Trigger New Release",
	"tags": [],
	"description": "",
	"content": " Update Our Application So far we have walked through setting up CI/CD for EKS using AWS CodePipeline and now we are going to make a change to the GitHub repository so that we can see a new release built and delivered.\nOpen GitHub and select the forked repository with the name eks-workshop-sample-api-service-go.\nClick on main.go file and then click on the edit button, which looks like a pencil.\nChange the text where it says \u0026ldquo;Hello World\u0026rdquo;, add a commit message and then click the \u0026ldquo;Commit changes\u0026rdquo; button.\nYou should leave the master branch selected.\nThe main.go application needs to be compiled, so please ensure that you don\u0026rsquo;t accidentally break the build :)\n After you modify and commit your change in GitHub, in approximately one minute you will see a new build triggered in the AWS Management Console Confirm the Change If you still have the ELB URL open in your browser, refresh to confirm the update. If you need to retrieve the URL again, use kubectl get services hello-k8s -o wide\n"
},
{
	"uri": "/codepipeline/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing the CI/CD with CodePipeline module.\nThis module is not used in subsequent steps, so you can remove the resources now, or at the end of the workshop.\nFirst we need to delete the Kubernetes deployment and service:\nkubectl delete deployments hello-k8s kubectl delete services hello-k8s  Next, we are going to delete the CloudFormation stack created. Open CloudFormation the AWS Management Console.\nCheck the box next to the eksws-codepipeline stack, select the Actions dropdown menu and then click Delete stack:\nNow we are going to delete the ECR repository:\nEmpty and then delete the S3 bucket used by CodeBuild for build artifacts (bucket name starts with eksws-codepipeline). First, select the bucket, then empty the bucket and finally delete the bucket:\n"
},
{
	"uri": "/prerequisites/iamrole/",
	"title": "Create an IAM role for your Workspace",
	"tags": [],
	"description": "",
	"content": " Follow this deep link to create an IAM role with Administrator access. Confirm that AWS service and EC2 are selected, then click Next to view permissions. Confirm that AdministratorAccess is checked, then click Next: Tags to assign tags. Take the defaults, and click Next: Review to review. Enter eksworkshop-admin-yourusername for the Name, and click Create role.   "
},
{
	"uri": "/prerequisites/ec2instance/",
	"title": "Attach the IAM role to your Workspace",
	"tags": [],
	"description": "",
	"content": " Follow this deep link to find your Cloud9 EC2 instance Select the instance, then choose Actions / Instance Settings / Attach/Replace IAM Role  Choose eksworkshop-admin-yourusername from the IAM Role drop down, and select Apply   "
},
{
	"uri": "/prerequisites/workspaceiam/",
	"title": "Update IAM settings for your Workspace",
	"tags": [],
	"description": "",
	"content": " Cloud9 normally manages IAM credentials dynamically. This isn\u0026rsquo;t currently compatible with the EKS IAM authentication, so we will disable it and rely on the IAM role instead.\n  Return to your workspace and click the sprocket, or launch a new tab to open the Preferences tab Select AWS SETTINGS Turn off AWS managed temporary credentials Close the Preferences tab   To ensure temporary credentials aren\u0026rsquo;t already in place we will also remove any existing credentials file:\nrm -vf ${HOME}/.aws/credentials  We should configure our aws cli with our current region as default:\nexport ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region') echo \u0026quot;export ACCOUNT_ID=${ACCOUNT_ID}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile echo \u0026quot;export AWS_REGION=${AWS_REGION}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region  Validate the IAM role Use the GetCallerIdentity CLI command to validate that the Cloud9 IDE is using the correct IAM role.\naws sts get-caller-identity  The output assumed-role name should contain:\neksworkshop-admin-yourusername  VALID If the Arn contains the role name from above and an Instance ID, you may proceed.\n{ \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/eksworkshop-admin-yourusername/i-01234567890abcdef\u0026quot; }  INVALID If the _Arn contains TeamRole, MasterRole, or does not match the role name output above, DO NOT PROCEED. Go back and confirm the steps on this page.\n{ \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/TeamRole/MasterRole\u0026quot; }  "
},
{
	"uri": "/kubeflow/dashboard/",
	"title": "Kubeflow Dashboard",
	"tags": [],
	"description": "",
	"content": " Kubeflow Dashboard Get Kubeflow service endpoint:\nkubectl get ingress -n istio-system -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}'  Access the endpoint address in a browser to see Kubeflow dashboard:\nClick on Start Setup\nSpecify the namespace as eksworkshop\nClick on Finish to view the dashboard\n"
},
{
	"uri": "/advanced-networking/secondary_cidr/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": " Before we configure EKS, we need to enable secondary CIDR blocks in your VPC and make sure they have proper tags and route table configurations\nAdd secondary CIDRs to your VPC There are restrictions on the range of secondary CIDRs you can use to extend your VPC. For more info, see IPv4 CIDR Block Association Restrictions\n You can use below commands to add 100.64.0.0/16 to your EKS cluster VPC. Please note to change the Values parameter to EKS cluster name if you used different name than eksctl-eksworkshop\nVPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=eksctl-eksworkshop* | jq -r '.Vpcs[].VpcId') aws ec2 associate-vpc-cidr-block --vpc-id $VPC_ID --cidr-block 100.64.0.0/16  Next step is to create subnets. Before we do this step, let\u0026rsquo;s check how many subnets we are consuming. You can run this command to see EC2 instance and AZ details\naws ec2 describe-instances --filters \u0026quot;Name=tag:Name,Values=eksworkshop*\u0026quot; --query 'Reservations[*].Instances[*].[PrivateDnsName,Tags[?Key==`Name`].Value|[0],Placement.AvailabilityZone,PrivateIpAddress,PublicIpAddress]' --output table  ------------------------------------------------------------------------------------------------------------------------------------------ | DescribeInstances | +-----------------------------------------------+---------------------------------------+-------------+-----------------+----------------+ | ip-192-168-9-228.us-east-2.compute.internal | eksworkshop-eksctl-yourusername-ng-475d4bc8-Node | us-east-2c | 192.168.9.228 | 18.191.57.131 | | ip-192-168-71-211.us-east-2.compute.internal | eksworkshop-eksctl-yourusername-ng-475d4bc8-Node | us-east-2a | 192.168.71.211 | 18.221.77.249 | | ip-192-168-33-135.us-east-2.compute.internal | eksworkshop-eksctl-yourusername-ng-475d4bc8-Node | us-east-2b | 192.168.33.135 | 13.59.167.90 | +-----------------------------------------------+---------------------------------------+-------------+-----------------+----------------+  I have 3 instances and using 3 subnets in my environment. For simplicity, we will use the same AZ\u0026rsquo;s and create 3 secondary CIDR subnets but you can certainly customize according to your networking requirements. Remember to change the AZ names according to your environment\nexport AZ1=us-east-2a export AZ2=us-east-2b export AZ3=us-east-2c CGNAT_SNET1=$(aws ec2 create-subnet --cidr-block 100.64.0.0/19 --vpc-id $VPC_ID --availability-zone $AZ1 | jq -r .Subnet.SubnetId) CGNAT_SNET2=$(aws ec2 create-subnet --cidr-block 100.64.32.0/19 --vpc-id $VPC_ID --availability-zone $AZ2 | jq -r .Subnet.SubnetId) CGNAT_SNET3=$(aws ec2 create-subnet --cidr-block 100.64.64.0/19 --vpc-id $VPC_ID --availability-zone $AZ3 | jq -r .Subnet.SubnetId)  Next step is to add Kubernetes tags on newer Subnets. You can check these tags by querying your current subnets\naws ec2 describe-subnets --filters Name=cidr-block,Values=192.168.0.0/19 --output text  Output shows similar to this\nTAGS aws:cloudformation:logical-id SubnetPublicUSEAST2C TAGS kubernetes.io/role/elb 1 TAGS eksctl.cluster.k8s.io/v1alpha1/cluster-name eksworkshop-eksctl-yourusername TAGS Name eksctl-eksworkshop-eksctl-yourusername-cluster/SubnetPublicUSEAST2C TAGS aws:cloudformation:stack-name eksctl-eksworkshop-eksctl-yourusername-cluster TAGS kubernetes.io/cluster/eksworkshop-eksctl-yourusername shared TAGS aws:cloudformation:stack-id arn:aws:cloudformation:us-east-2:012345678901:stack/eksctl-eksworkshop-eksctl-yourusername-cluster/8da51fc0-2b5e-11e9-b535-022c6f51bf82  Here are the commands to add tags to both the subnets\naws ec2 create-tags --resources $CGNAT_SNET1 --tags Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=eksworkshop-eksctl-yourusername aws ec2 create-tags --resources $CGNAT_SNET1 --tags Key=kubernetes.io/cluster/eksworkshop-eksctl-yourusername,Value=shared aws ec2 create-tags --resources $CGNAT_SNET1 --tags Key=kubernetes.io/role/elb,Value=1 aws ec2 create-tags --resources $CGNAT_SNET2 --tags Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=eksworkshop-eksctl-yourusername aws ec2 create-tags --resources $CGNAT_SNET2 --tags Key=kubernetes.io/cluster/eksworkshop-eksctl-yourusername,Value=shared aws ec2 create-tags --resources $CGNAT_SNET2 --tags Key=kubernetes.io/role/elb,Value=1 aws ec2 create-tags --resources $CGNAT_SNET3 --tags Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=eksworkshop-eksctl-yourusername aws ec2 create-tags --resources $CGNAT_SNET3 --tags Key=kubernetes.io/cluster/eksworkshop-eksctl-yourusername,Value=shared aws ec2 create-tags --resources $CGNAT_SNET3 --tags Key=kubernetes.io/role/elb,Value=1  As next step, we need to associate three new subnets into a route table. Again for simplicity, we chose to add new subnets to the Public route table that has connectivity to Internet Gateway\nSNET1=$(aws ec2 describe-subnets --filters Name=cidr-block,Values=192.168.0.0/19 | jq -r .Subnets[].SubnetId) RTASSOC_ID=$(aws ec2 describe-route-tables --filters Name=association.subnet-id,Values=$SNET1 | jq -r .RouteTables[].RouteTableId) aws ec2 associate-route-table --route-table-id $RTASSOC_ID --subnet-id $CGNAT_SNET1 aws ec2 associate-route-table --route-table-id $RTASSOC_ID --subnet-id $CGNAT_SNET2 aws ec2 associate-route-table --route-table-id $RTASSOC_ID --subnet-id $CGNAT_SNET3  "
},
{
	"uri": "/batch/jobs/",
	"title": "Kubernetes Jobs",
	"tags": [],
	"description": "",
	"content": " Kubernetes Jobs A job creates one or more pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the job tracks the successful completions. When a specified number of successful completions is reached, the job itself is complete. Deleting a Job will cleanup the pods it created.\nSave the below manifest as \u0026lsquo;job-whalesay.yaml\u0026rsquo; using your favorite editor.\napiVersion: batch/v1 kind: Job metadata: name: whalesay spec: template: spec: containers: - name: whalesay image: docker/whalesay command: [\u0026quot;cowsay\u0026quot;, \u0026quot;This is a Kubernetes Job!\u0026quot;] restartPolicy: Never backoffLimit: 4  Run a sample Kubernetes Job using the whalesay image.\nkubectl apply -f job-whalesay.yaml  Wait until the job has completed successfully.\nkubectl get job/whalesay  NAME DESIRED SUCCESSFUL AGE whalesay 1 1 2m  Confirm the output.\nkubectl logs -l job-name=whalesay  ___________________________ \u0026lt; This is a Kubernetes Job! \u0026gt; --------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/  "
},
{
	"uri": "/servicemesh_with_istio/download/",
	"title": "Download and Install Istio CLI",
	"tags": [],
	"description": "",
	"content": "Before we can get started configuring Istio we’ll need to first install the command line tools that you will interact with. To do this run the following.\ncd ~/environment curl -L https://git.io/getLatestIstio | sh - // version can be different as istio gets upgraded cd istio-* sudo mv -v bin/istioctl /usr/local/bin/  "
},
{
	"uri": "/irsa/oidc-provider/",
	"title": "Create an OIDC identity provider",
	"tags": [],
	"description": "",
	"content": " To create an IAM OIDC identity provider for your cluster with eksctl To use IAM roles for service accounts in your cluster, you must create an OIDC identity provider in the IAM console\n Check your eksctl version that your eksctl version is at least 0.5.1  eksctl version   [ℹ] version.Info{BuiltAt:\u0026ldquo;\u0026rdquo;, GitCommit:\u0026ldquo;\u0026rdquo;, GitTag:\u0026ldquo;0.5.3\u0026rdquo;}\n If your eksctl version is lower than 0.5.1, use Installing or Upgrading eksctl in the user guide\n  Create your OIDC identity provider for your cluster  eksctl utils associate-iam-oidc-provider --name eksworkshop-eksctl-yourusername --approve   [ℹ] using region {AWS_REGION}\n[ℹ] will create IAM Open ID Connect provider for cluster \u0026ldquo;eksworkshop-eksctl-yourusername\u0026rdquo; in \u0026ldquo;{AWS_REGION}\u0026rdquo;\n[✔] created IAM Open ID Connect provider for cluster \u0026ldquo;eksworkshop-eksctl-yourusername\u0026rdquo; in \u0026ldquo;{AWS_REGION}\u0026rdquo;\n If you go to the Identity Providers in IAM Console, you will see OIDC provider has created for your cluster\n"
},
{
	"uri": "/servicemesh_with_appmesh/create_app_mesh_components/",
	"title": "Create the App Mesh Components",
	"tags": [],
	"description": "",
	"content": "In this chapter, we\u0026rsquo;ll deploy the App Mesh sidecar auto-injector, and the App Mesh CRDs.\n"
},
{
	"uri": "/monitoring/dashboards/",
	"title": "Dashboards",
	"tags": [],
	"description": "",
	"content": " Create Dashboards Login into Grafana dashboard using credentials supplied during configuration\nYou will notice that \u0026lsquo;Install Grafana\u0026rsquo; \u0026amp; \u0026lsquo;create your first data source\u0026rsquo; are already completed. We will import community created dashboard for this tutorial\nClick \u0026lsquo;+\u0026rsquo; button on left panel and select \u0026lsquo;Import\u0026rsquo;\nEnter 3131 dashboard id under Grafana.com Dashboard \u0026amp; click \u0026lsquo;Load\u0026rsquo;.\nLeave the defaults, select \u0026lsquo;Prometheus\u0026rsquo; as the endpoint under prometheus data sources drop down, click \u0026lsquo;Import\u0026rsquo;.\nThis will show monitoring dashboard for all cluster nodes\nFor creating dashboard to monitor all pods, repeat same process as above and enter 3146 for dashboard id\n"
},
{
	"uri": "/intro_to_rbac/create_iam_user/",
	"title": "Create a User",
	"tags": [],
	"description": "",
	"content": " For the sake of simplicity, in this chapter, we will save credentials to a file to make it easy to toggle back and forth between users. Never do this in production or with credentials that have priveledged access; It is not a security best practice to store credentials on the filesystem.\n From within the Cloud9 terminal, create a new user called rbac-user, and generate/save credentials for it:\naws iam create-user --user-name rbac-user aws iam create-access-key --user-name rbac-user | tee /tmp/create_output.json  By running the previous step, you should get a response similar to:\n{ \u0026quot;AccessKey\u0026quot;: { \u0026quot;UserName\u0026quot;: \u0026quot;rbac-user\u0026quot;, \u0026quot;Status\u0026quot;: \u0026quot;Active\u0026quot;, \u0026quot;CreateDate\u0026quot;: \u0026quot;2019-07-17T15:37:27Z\u0026quot;, \u0026quot;SecretAccessKey\u0026quot;: \u0026lt; AWS Secret Access Key \u0026gt; , \u0026quot;AccessKeyId\u0026quot;: \u0026lt; AWS Access Key \u0026gt; } }  To make it easy to switch back and forth between the admin user you created the cluster with, and this new rbac-user, run the following command to create a script that when sourced, sets the active user to be rbac-user:\ncat \u0026lt;\u0026lt; EoF \u0026gt; rbacuser_creds.sh export AWS_SECRET_ACCESS_KEY=`grep SecretAccessKey /tmp/create_output.json | awk -F: '{print $2}' | sed 's/,//' | sed 's/^ //g'` export AWS_ACCESS_KEY_ID=`grep AccessKeyId /tmp/create_output.json | awk -F: '{print $2}' | sed 's/,//' | sed 's/^ //g'` EoF  "
},
{
	"uri": "/introduction/basics/",
	"title": "Kubernetes (k8s) Basics",
	"tags": [],
	"description": "",
	"content": "In this section, we\u0026rsquo;ll cover the following topics:\n What is Kubernetes   Kubernetes Nodes   K8s Objects Overview   K8s Objects Detail (1/2)   K8s Objects Detail (2/2)   "
},
{
	"uri": "/deploy/deploycrystal/",
	"title": "Deploy Crystal Backend API",
	"tags": [],
	"description": "",
	"content": "Let’s bring up the Crystal Backend API!\nCopy/Paste the following commands into your Cloud9 workspace:\ncd ~/environment/ecsdemo-crystal kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml  We can watch the progress by looking at the deployment status:\nkubectl get deployment ecsdemo-crystal  "
},
{
	"uri": "/cleanup/undeploy/",
	"title": "Undeploy the applications",
	"tags": [],
	"description": "",
	"content": "To delete the resources created by the applications, we should delete the application deployments and kubernetes dashboard.\nNote that if you followed the cleanup section of the modules, some of the commands below might fail because there is nothing to delete and its ok.\nUndeploy the applications:\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml  "
},
{
	"uri": "/eksctl/launcheks/",
	"title": "Launch EKS",
	"tags": [],
	"description": "",
	"content": " DO NOT PROCEED with this step unless you have validated the IAM role in use by the Cloud9 IDE. You will not be able to run the necessary kubectl commands in the later modules unless the EKS cluster is built using the IAM role.\n Challenge: How do I check the IAM role on the workspace?\n  Expand here to see the solution   Run aws sts get-caller-identity and validate that your Arn contains eksworkshop-admin-yourusername or TeamRole (or the role created when starting the workshop) and an Instance Id.\n{ \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/eksworkshop-admin-yourusername/i-01234567890abcdef\u0026quot; } or { \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/TeamRole/i-01234567890abcdef\u0026quot; }  If you do not see the correct role, please go back and validate the IAM role for troubleshooting.\nIf you do see the correct role, proceed to next step to create an EKS cluster.\n  Create an EKS cluster eksctl create cluster --name=eksworkshop-eksctl-yourusername --nodes=3 --alb-ingress-access --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n "
},
{
	"uri": "/tabs-example/tabs/eksctl/",
	"title": "Launch EKS",
	"tags": [],
	"description": "",
	"content": "To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl-yourusername --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n "
},
{
	"uri": "/spotworkers/deployhandler/",
	"title": "Deploy The Spot Interrupt Handler",
	"tags": [],
	"description": "",
	"content": " In this section, we will prepare our cluster to handle Spot interruptions.\nIf the available On-Demand capacity of a particular instance type is depleted, the Spot Instance is sent an interruption notice two minutes ahead to gracefully wrap up things. We will deploy a pod on each spot instance to detect and redeploy applications elsewhere in the cluster\nThe first thing that we need to do is deploy the Spot Interrupt Handler on each Spot Instance. This will monitor the EC2 metadata service on the instance for a interruption notice.\nThe workflow can be summarized as:\n Identify that a Spot Instance is being reclaimed. Use the 2-minute notification window to gracefully prepare the node for termination. Taint the node and cordon it off to prevent new pods from being placed. Drain connections on the running pods. Replace the pods on remaining nodes to maintain the desired capacity.  We have provided an example K8s DaemonSet manifest. A DaemonSet runs one pod per node.\nmkdir ~/environment/spot cd ~/environment/spot wget https://eksworkshop.com/spot/managespot/deployhandler.files/spot-interrupt-handler-example.yml  As written, the manifest will deploy pods to all nodes including On-Demand, which is a waste of resources. We want to edit our DaemonSet to only be deployed on Spot Instances. Let\u0026rsquo;s use the labels to identify the right nodes.\nUse a nodeSelector to constrain our deployment to spot instances. View this link for more details.\nChallenge Configure our Spot Handler to use nodeSelector   Expand here to see the solution   Place this at the end of the DaemonSet manifest under Spec.Template.Spec.nodeSelector\nnodeSelector: lifecycle: Ec2Spot   \nDeploy the DaemonSet\nkubectl apply -f ~/environment/spot/spot-interrupt-handler-example.yml  If you receive an error deploying the DaemonSet, there is likely a small error in the YAML file. We have provided a solution file at the bottom of this page that you can use to compare.\n View the pods. There should be one for each spot node.\nkubectl get daemonsets    Related files   spot-interrupt-handler-example.yml  (1 ko)   spot-interrupt-handler-solution.yml  (1 ko)    "
},
{
	"uri": "/servicemesh_with_appmesh/create_the_k8s_app/clone_repo/",
	"title": "Clone the Repo",
	"tags": [],
	"description": "",
	"content": "To begin, clone the repository that holds the DJ App\u0026rsquo;s files.\nFirst, be sure you are in your environment directory:\ncd ~/environment  git clone https://github.com/aws/aws-app-mesh-examples  and change to the repo\u0026rsquo;s project directory:\ncd aws-app-mesh-examples/examples/apps/djapp/  "
},
{
	"uri": "/statefulset/statefulset/",
	"title": "Create StatefulSet",
	"tags": [],
	"description": "",
	"content": " Introduction StatefulSet consists of serviceName, replicas, template and volumeClaimTemplates. serviceName is \u0026ldquo;mysql\u0026rdquo;, headless service we created in previous section, replicas is 3, the desired number of pod, template is the configuration of pod, volumeClaimTemplates is to claim volume for pod based on storageClassName, gp2 that we created in \u0026ldquo;Define Storageclass\u0026rdquo; section. Percona Xtrabackup is in template to clone source MySQL server to its slaves.\nCreate StatefulSet Copy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/templates wget https://eksworkshop.com/statefulset/statefulset.files/mysql-statefulset.yml  Create statefulset \u0026ldquo;mysql\u0026rdquo; by following command.\nkubectl create -f ~/environment/templates/mysql-statefulset.yml  Watch StatefulSet Watch the status of statefulset.\nkubectl get -w statefulset  It will take few minutes for pods to initialize and have statefulset created. DESIRED is the replicas number you define at StatefulSet.\nNAME DESIRED CURRENT AGE mysql 3 1 8s mysql 3 2 59s mysql 3 3 2m mysql 3 3 3m  Open another Cloud9 Terminal and watch the progress of pods creation using the following command.\nkubectl get pods -l app=mysql --watch  You can see ordered, graceful deployment with a stable, unique name for each pod.\nNAME READY STATUS RESTARTS AGE mysql-0 0/2 Init:0/2 0 30s mysql-0 0/2 Init:1/2 0 35s mysql-0 0/2 PodInitializing 0 47s mysql-0 1/2 Running 0 48s mysql-0 2/2 Running 0 59s mysql-1 0/2 Pending 0 0s mysql-1 0/2 Pending 0 0s mysql-1 0/2 Pending 0 0s mysql-1 0/2 Init:0/2 0 0s mysql-1 0/2 Init:1/2 0 35s mysql-1 0/2 Init:1/2 0 45s mysql-1 0/2 PodInitializing 0 54s mysql-1 1/2 Running 0 55s mysql-1 2/2 Running 0 1m mysql-2 0/2 Pending 0 \u0026lt;invalid\u0026gt; mysql-2 0/2 Pending 0 \u0026lt;invalid\u0026gt; mysql-2 0/2 Pending 0 0s mysql-2 0/2 Init:0/2 0 0s mysql-2 0/2 Init:1/2 0 32s mysql-2 0/2 Init:1/2 0 43s mysql-2 0/2 PodInitializing 0 50s mysql-2 1/2 Running 0 52s mysql-2 2/2 Running 0 56s  Press Ctrl+C to stop watching.\nCheck the dynamically created PVC by following command.\nkubectl get pvc -l app=mysql  You can see data-mysql-0,1,2 are created by STORAGECLASS mysql-gp2.\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-mysql-0 Bound pvc-83e9dfeb-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-1 Bound pvc-977e7806-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-2 Bound pvc-b3009b02-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d  (Optional) Check 10Gi 3 EBS volumes are created across availability zones at your AWS console.   Related files   mysql-statefulset.yml  (5 ko)    "
},
{
	"uri": "/helm_root/helm_micro/customize/",
	"title": "Customize Defaults",
	"tags": [],
	"description": "",
	"content": " If you look in the newly created eksdemo directory, you\u0026rsquo;ll see several files and directories. Specifically, inside the /templates directory, you\u0026rsquo;ll see:\n NOTES.txt: The “help text” for your chart. This will be displayed to your users when they run helm install. deployment.yaml: A basic manifest for creating a Kubernetes deployment service.yaml: A basic manifest for creating a service endpoint for your deployment _helpers.tpl: A place to put template helpers that you can re-use throughout the chart  We\u0026rsquo;re actually going to create our own files, so we\u0026rsquo;ll delete these boilerplate files\nrm -rf ~/environment/eksdemo/templates/ rm ~/environment/eksdemo/Chart.yaml rm ~/environment/eksdemo/values.yaml  Create a new Chart.yaml file which will describe the chart\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/eksdemo/Chart.yaml apiVersion: v1 appVersion: \u0026quot;1.0\u0026quot; description: A Helm chart for EKS Workshop Microservices application name: eksdemo version: 0.1.0 EoF  Next we\u0026rsquo;ll copy the manifest files for each of our microservices into the templates directory as servicename.yaml\n#create subfolders for each template type mkdir -p ~/environment/eksdemo/templates/deployment mkdir -p ~/environment/eksdemo/templates/service # Copy and rename frontend manifests cp ~/environment/ecsdemo-frontend/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/frontend.yaml cp ~/environment/ecsdemo-frontend/kubernetes/service.yaml ~/environment/eksdemo/templates/service/frontend.yaml # Copy and rename crystal manifests cp ~/environment/ecsdemo-crystal/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/crystal.yaml cp ~/environment/ecsdemo-crystal/kubernetes/service.yaml ~/environment/eksdemo/templates/service/crystal.yaml # Copy and rename nodejs manifests cp ~/environment/ecsdemo-nodejs/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/nodejs.yaml cp ~/environment/ecsdemo-nodejs/kubernetes/service.yaml ~/environment/eksdemo/templates/service/nodejs.yaml  All files in the templates directory are sent through the template engine. These are currently plain YAML files that would be sent to Kubernetes as-is.\nReplace hard-coded values with template directives Let\u0026rsquo;s replace some of the values with template directives to enable more customization by removing hard-coded values.\nOpen ~/environment/eksdemo/templates/deployment/frontend.yaml in your Cloud9 editor.\nThe following steps should be completed seperately for frontend.yaml, crystal.yaml, and nodejs.yaml.\n Under spec, find replicas: 1 and replace with the following:\nreplicas: {{ .Values.replicas }}  Under spec.template.spec.containers.image, replace the image with the correct template value from the table below:\n   Filename Value     frontend.yaml - image: {{ .Values.frontend.image }}:{{ .Values.version }}   crystal.yaml - image: {{ .Values.crystal.image }}:{{ .Values.version }}   nodejs.yaml - image: {{ .Values.nodejs.image }}:{{ .Values.version }}    Create a values.yaml file with our template defaults This file will populate our template directives with default values.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/eksdemo/values.yaml # Default values for eksdemo. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Release-wide Values replicas: 3 version: 'latest' # Service Specific Values nodejs: image: brentley/ecsdemo-nodejs crystal: image: brentley/ecsdemo-crystal frontend: image: brentley/ecsdemo-frontend EoF  "
},
{
	"uri": "/jenkins/deploy/",
	"title": "Deploy Jenkins",
	"tags": [],
	"description": "",
	"content": " With our Storage Class configured we then need to create our jenkins setup. To do this we\u0026rsquo;ll just use the helm cli with a couple flags.\nIn a production system you should be using a values.yaml file so that you can manage the drift as you need to update releases\n Install Jenkins helm install stable/jenkins --set rbac.create=true,master.servicePort=80 --name cicd  The output of this command will give you some additional information such as the admin password and the way to get the host name of the ELB that was provisioned.\nLet\u0026rsquo;s give this some time to provision and while we do let\u0026rsquo;s watch for pods to boot.\nkubectl get pods -w  You should see the pods in init, pending or running state.\nOnce this changes to running we can get the load balancer address.\nexport SERVICE_IP=$(kubectl get svc --namespace default cicd-jenkins --template \u0026quot;{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}\u0026quot;) echo http://$SERVICE_IP/login  This service was configured with a LoadBalancer so, an AWS Elastic Load Balancer (ELB) is launched by Kubernetes for the service. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com\u0026rdquo; - the full value is the DNS address.\n When the front-end service is first deployed, it can take up to several minutes for the ELB to be created and DNS updated. During this time the link above may display a \u0026ldquo;site unreachable\u0026rdquo; message. To check if the instances are in service, follow this deep link to the load balancer console. On the load balancer select the instances tab and ensure that the instance status is listed as \u0026ldquo;InService\u0026rdquo; before proceeding to the jenkins login page.\n "
},
{
	"uri": "/logging/setup_es/",
	"title": "Provision an Elasticsearch Cluster",
	"tags": [],
	"description": "",
	"content": "This example creates a two instance Amazon Elasticsearch cluster named kubernetes-logs. This cluster is created in the same region as the Kubernetes cluster and CloudWatch log group.\nNote that this cluster has an open access policy which will need to be locked down in production environments.\n aws es create-elasticsearch-domain \\ --domain-name kubernetes-logs \\ --elasticsearch-version 6.3 \\ --elasticsearch-cluster-config \\ InstanceType=m4.large.elasticsearch,InstanceCount=2 \\ --ebs-options EBSEnabled=true,VolumeType=standard,VolumeSize=100 \\ --access-policies '{\u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;,\u0026quot;Statement\u0026quot;:[{\u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;,\u0026quot;Principal\u0026quot;:{\u0026quot;AWS\u0026quot;:[\u0026quot;*\u0026quot;]},\u0026quot;Action\u0026quot;:[\u0026quot;es:*\u0026quot;],\u0026quot;Resource\u0026quot;:\u0026quot;*\u0026quot;}]}'  It takes a little while for the cluster to be created and arrive at an active state. The AWS Console should show the following status when the cluster is ready.\nYou could also check this via AWS CLI:\naws es describe-elasticsearch-domain --domain-name kubernetes-logs --query 'DomainStatus.Processing'  If the output value is false that means the domain has been processed and is now available to use.\nFeel free to move on to the next section for now.\n"
},
{
	"uri": "/scaling/test_hpa/",
	"title": "Scale an Application with HPA",
	"tags": [],
	"description": "",
	"content": " Deploy a Sample App We will deploy an application and expose as a service on TCP port 80. The application is a custom-built image based on the php-apache image. The index.php page performs calculations to generate CPU load. More information can be found here\nkubectl run php-apache --image=k8s.gcr.io/hpa-example --requests=cpu=200m --expose --port=80  Create an HPA resource This HPA scales up when CPU exceeds 50% of the allocated container resource.\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10  View the HPA using kubectl. You probably will see \u0026lt;unknown\u0026gt;/50% for 1-2 minutes and then you should be able to see 0%/50%\nkubectl get hpa  Generate load to trigger scaling Open a new terminal in the Cloud9 Environment and run the following command to drop into a shell on a new container\nkubectl run -i --tty load-generator --image=busybox /bin/sh  Execute a while loop to continue getting http:///php-apache\nwhile true; do wget -q -O - http://php-apache; done  In the previous tab, watch the HPA with the following command\nkubectl get hpa -w  You will see HPA scale the pods from 1 up to our configured maximum (10) until the CPU average is below our target (50%)\nYou can now stop (Ctrl + C) load test that was running in the other terminal. You will notice that HPA will slowly bring the replica count to min number based on its configuration. You should also get out of load testing application by pressing Ctrl + D\n"
},
{
	"uri": "/weave_flux/codepipeline/",
	"title": "Create Image with CodePipeline",
	"tags": [],
	"description": "",
	"content": "Now we are going to create the AWS CodePipeline using AWS CloudFormation. This pipeline will be used to build a Docker image from your GitHub source repo (eks-example). Note that this does not deploy the image. Weave Flux will handle that.\nCloudFormation is an infrastructure as code (IaC) tool which provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts.\nEach EKS deployment/service should have its own CodePipeline and be located in an isolated source repository.\nClick the Launch button to create the CloudFormation stack in the AWS Management Console.\n   Launch template       CodePipeline \u0026amp; EKS  Launch    Download      After the console is open, enter your GitHub username, personal access token (created in previous step), check the acknowledge box and then click the \u0026ldquo;Create stack\u0026rdquo; button located at the bottom of the page.\nWait for the status to change from \u0026ldquo;CREATE_IN_PROGRESS\u0026rdquo; to CREATE_COMPLETE before moving on to the next step.\nOpen CodePipeline in the Management Console. You will see a CodePipeline that starts with image-codepipeline. Click this link to view the details.\nIf you receive a permissions error similar to User x is not authorized to perform: codepipeline:ListPipelines\u0026hellip; upon clicking the above link, the CodePipeline console may have opened up in the wrong region. To correct this, from the Region dropdown in the console, choose the region you provisioned the workshop in.\n Currently the image build is likely failed because we have no code in our repository. We will add a sample application to our GitHub repo (eks-example). Clone the repo substituting your GitHub user name.\nUpdate the username (YOURUSER) below to match your GitHub user name.\n git clone https://github.com/YOURUSER/eks-example.git cd eks-example  Next create a base README file, a source directory, and download a sample nginx configuration (hello.conf), home page (index.html), and Dockerfile.\necho \u0026quot;# eks-example\u0026quot; \u0026gt; README.md mkdir src wget -O src/hello.conf https://eksworkshop.com/weave_flux/app.files/hello.conf wget -O src/index.html https://eksworkshop.com/weave_flux/app.files/index.html wget https://raw.githubusercontent.com/aws-samples/eks-workshop/master/content/weave_flux/app.files/Dockerfile  Now that we have a simple hello world app, commit the changes to start the image build pipeline.\ngit add . git commit -am \u0026quot;Initial commit\u0026quot; git push  In the CodePipeline console go to the details page for the specific CodePipeline. You can see status along with links to the change and build details.\nIf you click on the \u0026ldquo;details\u0026rdquo; link in the build/deploy stage, you can see the output from the CodeBuild process.\n To verify the image is built, go to the Amazon ECR console and look for your eks-example image repository.\n"
},
{
	"uri": "/network-policies/tigera/",
	"title": "Integrating Network Policy with VPC Security Groups and CloudWatch",
	"tags": [],
	"description": "",
	"content": "In this module, we will use Tigera\u0026rsquo;s Secure Cloud Edition to extend Kubernetes network policies to EKS\u0026rsquo; VPC security groups and surfacing enhanced flow logs for Kubernetes workloads in CloudWatch.\u0026rdquo;\nWe are assuming that you are doing this within the Cloud9 environment that you used to initially setup the cluster, and are using the Cloud9 CLI shell. If not, please adjust these instructions to your environment.\n"
},
{
	"uri": "/eksctl/",
	"title": "Launch using eksctl",
	"tags": [],
	"description": "",
	"content": " Launch using eksctl eksctl is a tool jointly developed by AWS and Weaveworks that automates much of the experience of creating EKS clusters.\nIn this module, we will use eksctl to launch and configure our EKS cluster and nodes.\n  "
},
{
	"uri": "/network-policies/tigera/environment/",
	"title": "Preparing the environment",
	"tags": [],
	"description": "",
	"content": " If you have setup your kubernetes cluster using the Cloud9 environment and eksctl, as instructed at the start of this workshop, then you can follow the abbreviated instructions here, as much of the work has been done for you. If not please refer to the instructions you received when you downloaded the Tigera Secure Cloud Edition 1.0.1 link.\n The instructions below assume that you have followed all of the initial EKSWorkshop setup instructions when creating your cluster. If you have not, some of the commands or environment settings that we rely on below will not be set correctly. If you encounter problems, please check your initial setup and/or consult the instructions mentioned above.\n First, you need to install tsctl in your Cloud9 environment.\nsudo curl --location -o /usr/local/bin/tsctl https://s3.amazonaws.com/tigera-public/ce/v1.0.6/tsctl-linux-amd64 sudo chmod +x /usr/local/bin/tsctl  Next, you will need to set some environment variables. There are commands for some of them, but a few you need to supply.\nThe $CLUSTER_NAME variable is the same that you used to create the cluster using the \u0026lsquo;eksctl\u0026rsquo; command at the beginning of the workshop. If you followed the directions, it will be \u0026lsquo;eksworkshop-eksctl-yourusername\u0026rsquo;\nCLUSTER_NAME=eksworkshop-eksctl-yourusername  The next thing we need to manually set is your Tigera Secure Cloud Edition $TS_TOKEN. This can be found by checking your Zendesk tickets. The Token can be found in your welcome ticket and is a UUID, or long string of hex digits.\nTS_TOKEN=\u0026lt;token UUID\u0026gt;  The following commands will set the remainder of the environment variables.\nVPC_ID=$(aws eks describe-cluster --name $CLUSTER_NAME --query 'cluster.resourcesVpcConfig.vpcId' --output text) K8S_NODE_SGS=$(aws ec2 describe-security-groups --filters Name=tag:aws:cloudformation:logical-id,Values=SG Name=vpc-id,Values=${VPC_ID} --query \u0026quot;SecurityGroups[0].GroupId\u0026quot; --output text) CONTROL_PLANE_SG=$(aws ec2 describe-security-groups --filters Name=tag:aws:cloudformation:logical-id,Values=ControlPlaneSecurityGroup Name=vpc-id,Values=${VPC_ID} --query \u0026quot;SecurityGroups[0].GroupId\u0026quot; --output text)  If you have any problems, please make sure that you have setup your Cloud9 environment correctly for the workshop.\n"
},
{
	"uri": "/prerequisites/clone/",
	"title": "Clone the Service Repos",
	"tags": [],
	"description": "",
	"content": "cd ~/environment git clone https://github.com/brentley/ecsdemo-frontend.git git clone https://github.com/brentley/ecsdemo-nodejs.git git clone https://github.com/brentley/ecsdemo-crystal.git  "
},
{
	"uri": "/monitoring/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Delete Prometheus and grafana helm delete prometheus helm del --purge prometheus helm delete grafana helm del --purge grafana  "
},
{
	"uri": "/deploy/servicetype/",
	"title": "Let&#39;s check Service Types",
	"tags": [],
	"description": "",
	"content": "Before we bring up the frontend service, let\u0026rsquo;s take a look at the service types we are using: This is kubernetes/service.yaml for our frontend service:\napiVersion: v1 kind: Service metadata: name: ecsdemo-frontend spec: selector: app: ecsdemo-frontend type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 3000  Notice type: LoadBalancer: This will configure an ELB to handle incoming traffic to this service.\nCompare this to kubernetes/service.yaml for one of our backend services:\napiVersion: v1 kind: Service metadata: name: ecsdemo-nodejs spec: selector: app: ecsdemo-nodejs ports: - protocol: TCP port: 80 targetPort: 3000  Notice there is no specific service type described. When we check the kubernetes documentation we find that the default type is ClusterIP. This Exposes the service on a cluster-internal IP. Choosing this value makes the service only reachable from within the cluster.\n"
},
{
	"uri": "/statefulset/testmysql/",
	"title": "Test MySQL",
	"tags": [],
	"description": "",
	"content": "You can use mysql-client to send some data to the master, mysql-0.mysql by following command.\nkubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --\\ mysql -h mysql-0.mysql \u0026lt;\u0026lt;EOF CREATE DATABASE test; CREATE TABLE test.messages (message VARCHAR(250)); INSERT INTO test.messages VALUES ('hello, from mysql-client'); EOF  Run the following to test slaves (mysql-read) received the data.\nkubectl run mysql-client --image=mysql:5.7 -it --rm --restart=Never --\\ mysql -h mysql-read -e \u0026quot;SELECT * FROM test.messages\u0026quot;  The output should look like this.\n+--------------------------+ | message | +--------------------------+ | hello, from mysql-client | +--------------------------+  To test load balancing across slaves, run the following command.\nkubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\\ bash -ic \u0026quot;while sleep 1; do mysql -h mysql-read -e 'SELECT @@server_id,NOW()'; done\u0026quot;  Each MySQL instance is assigned a unique identifier, and it can be retrieved using @@server_id. It will print the server id serving the request and the timestamp.\n+-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 102 | 2018-11-14 12:44:57 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 12:44:58 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 12:44:59 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 12:45:00 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 12:45:01 | +-------------+---------------------+  Leave this open in a separate window while you test failure in the next section.\n"
},
{
	"uri": "/healthchecks/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Our Liveness Probe example used HTTP request and Readiness Probe executed a command to check health of a pod. Same can be accomplished using a TCP request as described in the documentation.\nkubectl delete -f ~/environment/healthchecks/liveness-app.yaml kubectl delete -f ~/environment/healthchecks/readiness-deployment.yaml  "
},
{
	"uri": "/weave_flux/deploymentmanifests/",
	"title": "Deploy from Manifests",
	"tags": [],
	"description": "",
	"content": "Now we are ready to use Weave Flux to deploy the hello world application into our Amazon EKS cluster. To do this we will clone our GitHub config repository (k8s-config) and then commit Kubernetes manifests to deploy.\nUpdate the username (YOURUSER) below to match your GitHub user name.\n cd .. git clone https://github.com/YOURUSER/k8s-config.git cd k8s-config mkdir charts namespaces releases workloads  Create a namespace Kubernetes manifest.\ncat \u0026lt;\u0026lt; EOF \u0026gt; namespaces/eks-example.yaml apiVersion: v1 kind: Namespace metadata: labels: name: eks-example name: eks-example EOF  Create a deployment Kubernetes manifest.\nUpdate the image below to point to your ECR repository and image tag (Do NOT use latest). You can find your Image URI from the Amazon ECR Console. Replace YOURACCOUNT and YOURTAG)\n cat \u0026lt;\u0026lt; EOF \u0026gt; workloads/eks-example-dep.yaml --- apiVersion: apps/v1 kind: Deployment metadata: name: eks-example namespace: eks-example labels: app: eks-example annotations: # Container Image Automated Updates flux.weave.works/automated: \u0026quot;true\u0026quot; # do not apply this manifest on the cluster #flux.weave.works/ignore: \u0026quot;true\u0026quot; spec: replicas: 1 selector: matchLabels: app: eks-example template: metadata: labels: app: eks-example spec: containers: - name: eks-example image: YOURACCOUNT.dkr.ecr.us-east-1.amazonaws.com/eks-example:YOURTAG imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: http protocol: TCP livenessProbe: httpGet: path: / port: http readinessProbe: httpGet: path: / port: http EOF  Above you see 2 Kubernetes annotations for Flux.\n flux.weave.works/automated tells Flux whether the container image should be automatically updated.\n flux.weave.works/ignore is commented out, but could be used to tell Flux to temporarily ignore the deployment.\n  Finally, create a service manifest to enable a load balancer to be created.\ncat \u0026lt;\u0026lt; EOF \u0026gt; workloads/eks-example-svc.yaml apiVersion: v1 kind: Service metadata: name: eks-example namespace: eks-example labels: app: eks-example spec: type: LoadBalancer ports: - port: 80 targetPort: http protocol: TCP name: http selector: app: eks-example EOF  Now commit the changes and push to your repository.\ngit add . git commit -am \u0026quot;eks-example-deployment\u0026quot; git push  Check the logs of your Flux pod. It will pull config from the k8s-config repository every 5 minutes. Ensure you replace the pod name below with the name in your deployment.\nkubectl get pods -n flux kubectl logs flux-5bd7fb6bb6-4sc78 -n flux  Now get the URL for the load balancer (LoadBalancer Ingress) and connect via your browser (this may take a couple minutes for DNS).\nkubectl describe service eks-example -n eks-example  Make a change to the eks-example source code and push a new change.\ncd ../eks-example vi src/index.html # Change the \u0026lt;title\u0026gt; AND \u0026lt;h\u0026gt; to Hello World Version 2 git commit -am \u0026quot;v2 Updating home page\u0026quot; git push  Now you can watch in the CodePipeline console for the new image build to complete. This will take a couple minutes. Once complete, you will see a new image land in your Amazon ECR repository. Monitor the kubectl logs for the Flux pod and you should see it update the configuration within five minutes.\nVerify the web page has updated by refreshing the page in your browser.\nYour boss calls you late at night and tells you that people are complaining about the deployment. We need to back it out immediately! We could modify the code in eks-example and trigger a new image build and deploy. However, we can also use git to revert the config change in k8s-config. Lets take that approach.\ncd ../k8s-config git pull git log --oneline git revert HEAD # Save the commit message git log --oneline git push  You should now be able to watch logs for the Flux pod and it will pull the config change and roll out the previous image. Check your URL in the browser to ensure it is reverted.\nPhew! Disaster averted.\n"
},
{
	"uri": "/deploy/servicerole/",
	"title": "Ensure the ELB Service Role exists",
	"tags": [],
	"description": "",
	"content": "In AWS accounts that have never created a load balancer before, it\u0026rsquo;s possible that the service role for ELB might not exist yet.\nWe can check for the role, and create it if it\u0026rsquo;s missing.\nCopy/Paste the following commands into your Cloud9 workspace:\naws iam get-role --role-name \u0026quot;AWSServiceRoleForElasticLoadBalancing\u0026quot; || aws iam create-service-linked-role --aws-service-name \u0026quot;elasticloadbalancing.amazonaws.com\u0026quot;  "
},
{
	"uri": "/kubeflow/jupyter/",
	"title": "Jupyter Notebook",
	"tags": [],
	"description": "",
	"content": " Jupyter Notebook using Kubeflow on Amazon EKS The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. It is often used for data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and more.\nIn Kubeflow dashboard, click on Create a new Notebook server:\nSelect the namespace created in previous step:\nThis pre-populates the namespace field on the dashboard. Specify a name myjupyter for the notebook and change the CPU value to 1.0:\nScroll to the bottom, take all other defaults, and click on LAUNCH.\nIt takes a few seconds for the Jupyter notebook to come online. Click on CONNECT\nThis connects to the notebook and opens the notebook interface in a new browser tab.\nCLick on New, select Python3\nThis creates an empty Python 3 Jupyter notebook\nCopy the sample training code and paste it in the first code block. This Python sample code uses TensorFlow to create a training model for MNIST database. Click on Run to load this code in notebook.\nThis also creates a new code block. Copy main() in this new code block and click on Run again\nThis starts the model training and the output is shown on the notebook:\nThe first few lines shows that TensorFlow and Keras dataset is downloaded. Training data set is 60k images and test data set is 10k images. Hyperparameters used for the training, outputs from five epochs, and finally the model accuracy is shown.\n"
},
{
	"uri": "/servicemesh_with_istio/install/",
	"title": "Install Istio",
	"tags": [],
	"description": "",
	"content": " Define service account for Tiller Helm and Tiller are required for the following examples. If you have not installed Helm yet, please first reference the Helm chapter before proceeding.\nFirst create a service account for Tiller:\nkubectl apply -f install/kubernetes/helm/helm-service-account.yaml  Install Istio CRDs The Custom Resource Definitions, also known as CRDs are API resources which allow you to define custom resources.\nhelm install install/kubernetes/helm/istio-init --name istio-init --namespace istio-system  You can check the installation by running:\nkubectl get crds --namespace istio-system | grep 'istio.io'  This should return around 50 CRDs.\nInstall Istio The last step installs Istio\u0026rsquo;s core components:\nhelm install install/kubernetes/helm/istio --name istio --namespace istio-system --set global.configValidation=false --set sidecarInjectorWebhook.enabled=false --set grafana.enabled=true --set servicegraph.enabled=true  You can verify that the services have been deployed using\nkubectl get svc -n istio-system  and check the corresponding pods with:\nkubectl get pods -n istio-system  NAME READY STATUS RESTARTS AGE grafana-7b46bf6b7c-4rh5z 1/1 Running 0 10m istio-citadel-75fdb679db-jnn4z 1/1 Running 0 10m istio-galley-c864b5c86-sq952 1/1 Running 0 10m istio-ingressgateway-668676fbdb-p5c8c 1/1 Running 0 10m istio-init-crd-10-zgzn9 0/1 Completed 0 12m istio-init-crd-11-9v626 0/1 Completed 0 12m istio-pilot-f4c98cfbf-v8bss 2/2 Running 0 10m istio-policy-6cbbd844dd-ccnph 2/2 Running 1 10m istio-telemetry-ccc4df498-pjht7 2/2 Running 1 10m prometheus-89bc5668c-f866j 1/1 Running 0 10m servicegraph-5d4b49848-qvdtr 1/1 Running 0 10m  "
},
{
	"uri": "/advanced-networking/secondary_cidr/configure-cni/",
	"title": "Configure CNI",
	"tags": [],
	"description": "",
	"content": " Before we start making changes to VPC CNI, let\u0026rsquo;s make sure we are using latest CNI version\nRun this command to find CNI version\nkubectl describe daemonset aws-node --namespace kube-system | grep Image | cut -d \u0026quot;/\u0026quot; -f 2  Here is a sample response\namazon-k8s-cni:1.4.1  Upgrade version to 1.3 if you have an older version\nkubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.3/aws-k8s-cni.yaml  Wait till all the pods are recycled. You can check the status of pods by using this command\nkubectl get pods -n kube-system -w  Configure Custom networking Edit aws-node configmap and add AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG environment variable to the node container spec and set it to true\nNote: You only need to add two lines into configmap\nkubectl edit daemonset -n kube-system aws-node  ... spec: containers: - env: - name: AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG value: \u0026quot;true\u0026quot; - name: AWS_VPC_K8S_CNI_LOGLEVEL value: DEBUG - name: MY_NODE_NAME ...  Save the file and exit your text editor\nTerminate worker nodes so that Autoscaling launches newer nodes that come bootstrapped with custom network config\nUse caution before you run the next command because it terminates all worker nodes including running pods in your workshop\n INSTANCE_IDS=(`aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId' --filters \u0026quot;Name=tag:Name,Values=eksworkshop*\u0026quot; --output text` ) for i in \u0026quot;${INSTANCE_IDS[@]}\u0026quot; do echo \u0026quot;Terminating EC2 instance $i ...\u0026quot; aws ec2 terminate-instances --instance-ids $i done  "
},
{
	"uri": "/batch/install/",
	"title": "Install Argo CLI",
	"tags": [],
	"description": "",
	"content": " Install Argo CLI Before we can get started configuring argo we\u0026rsquo;ll need to first install the command line tools that you will interact with. To do this run the following.\nsudo curl -sSL -o /usr/local/bin/argo https://github.com/argoproj/argo/releases/download/v2.2.1/argo-linux-amd64 sudo chmod +x /usr/local/bin/argo  "
},
{
	"uri": "/irsa/iam-role-for-sa-1/",
	"title": "Creating an IAM Role for Service Account",
	"tags": [],
	"description": "",
	"content": " To create an IAM role for your service accounts with eksctl You must create an IAM policy that specifies the permissions that you would like the containers in your pods to have. In this workshop we will use AWS maanged policy named \u0026ldquo;AmazonS3ReadOnlyAccess\u0026rdquo; which allow get and list for all S3 resources.\nYou must also create a role for your service accounts to use before you associate it with a service account. Then you can then attach a specific IAM policy to the role that gives the containers in your pods the permissions you desire.\n Get ARN for AmazonS3ReadOnlyAccess  aws iam list-policies --query 'Policies[?PolicyName==`AmazonS3ReadOnlyAccess`].Arn'   \u0026ldquo;arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\u0026rdquo;\n  Create an IAM role for your service accounts  eksctl create iamserviceaccount --name iam-test --namespace default --cluster eksworkshop-eksctl-yourusername --attach-policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess --approve --override-existing-serviceaccounts   [ℹ] using region {AWS_REGION}\n[ℹ] 1 iamserviceaccount (default/iam-test) was included (based on the include/exclude rules)\n[!] metadata of serviceaccounts that exist in Kubernetes will be updated, as \u0026ndash;override-existing-serviceaccounts was set\n[ℹ] 1 task: { 2 sequential sub-tasks: { create IAM role for serviceaccount \u0026ldquo;default/iam-test\u0026rdquo;, create serviceaccount \u0026ldquo;default/iam-test\u0026rdquo; } }\n[ℹ] building iamserviceaccount stack \u0026ldquo;eksctl-eksworkshop-eksctl-yourusername-addon-iamserviceaccount-default-iam-test\u0026rdquo;\n[ℹ] deploying stack \u0026ldquo;eksctl-eksworkshop-eksctl-yourusername-addon-iamserviceaccount-default-iam-test\u0026rdquo;\n[ℹ] created serviceaccount \u0026ldquo;default/iam-test\u0026rdquo;\n If you go to the CloudFormation in IAM Console, you will find the stack \u0026ldquo;eksctl-eksworkshop-eksctl-yourusername-addon-iamserviceaccount-default-iam-test\u0026rdquo; has been created a role for your service account\n "
},
{
	"uri": "/servicemesh_with_appmesh/port_to_app_mesh/",
	"title": "Porting DJ to App Mesh",
	"tags": [],
	"description": "",
	"content": "Now that the Injector, CRDs, and App Mesh Controller are defined and running, we\u0026rsquo;re ready to define the App Mesh components required for our service mesh-enabled version of the app.\nAs we move to this new architecture, what will it look like, and how will it be different?\nFunctionally, the service mesh-enabled version will do exactly what the current version does; requests made by dj will be served by either the metal-v1, or the jazz-v1 services. The difference will be that we\u0026rsquo;ll use App Mesh to create new Virtual Services called metal and jazz.\nThe metal and jazz Virtual Services will then call their metal-v1 and jazz-v1 counterparts accordingly, based on the Virtual Services\u0026rsquo; routing rules.\n"
},
{
	"uri": "/intro_to_rbac/map_iam_user_to_k8s_user/",
	"title": "Map an IAM User to K8s",
	"tags": [],
	"description": "",
	"content": "Next, we\u0026rsquo;ll define a k8s user called rbac-user, and map to it\u0026rsquo;s IAM user counterpart. Run the following to create a ConfigMap called aws-auth.yaml that creates this mapping:\ncat \u0026lt;\u0026lt; EoF \u0026gt; aws-auth.yaml apiVersion: v1 kind: ConfigMap metadata: name: aws-auth namespace: kube-system data: mapUsers: | - userarn: arn:aws:iam::${ACCOUNT_ID}:user/rbac-user username: rbac-user EoF  Some of the values may be dynamically populated when the file is created. To verify everything populated and was created correctly, run the following:\ncat aws-auth.yaml  And the output should reflect that rolearn and userarn populated, similar to:\napiVersion: v1 kind: ConfigMap metadata: name: aws-auth namespace: kube-system data: mapUsers: | - userarn: arn:aws:iam::123456789:user/rbac-user username: rbac-user  Next, apply the ConfigMap to apply this mapping to the system:\nkubectl apply -f aws-auth.yaml  "
},
{
	"uri": "/introduction/basics/what_is_k8s/",
	"title": "What is Kubernetes",
	"tags": [],
	"description": "",
	"content": " Builds on over a decade of experience and best practices Utilizes declarative configuration and automation Draws upon a large ecosystem of tools, services, support  More information on what Kubernetes is all about can be found on the official Kubernetes website.\n"
},
{
	"uri": "/deploy/deployfrontend/",
	"title": "Deploy Frontend Service",
	"tags": [],
	"description": "",
	"content": "Let’s bring up the Ruby Frontend!\nCopy/Paste the following commands into your Cloud9 workspace:\ncd ~/environment/ecsdemo-frontend kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml  We can watch the progress by looking at the deployment status:\nkubectl get deployment ecsdemo-frontend  "
},
{
	"uri": "/spotworkers/preferspot/",
	"title": "Deploy an Application on Spot",
	"tags": [],
	"description": "",
	"content": " We are redesigning our Microservice example and want our frontend service to be deployed on Spot Instances when they are available. We will use Node Affinity in our manifest file to configure this.\nConfigure Node Affinity and Tolerations Open the deployment manifest in your Cloud9 editor - ~/environment/ecsdemo-frontend/kubernetes/deployment.yaml\nEdit the spec to configure NodeAffinity to prefer Spot Instances, but not require them. This will allow the pods to be scheduled on On-Demand nodes if no spot instances were available or correctly labelled.\nWe also want to configure a toleration which will allow the pods to \u0026ldquo;tolerate\u0026rdquo; the taint that we configured on our EC2 Spot Instances.\nFor examples of Node Affinity, check this link\nFor examples of Taints and Tolerations, check this link\nChallenge Configure Affinity and Toleration\n  Expand here to see the solution   Add this to your deployment file under spec.template.spec\naffinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: lifecycle operator: In values: - Ec2Spot tolerations: - key: \u0026quot;spotInstance\u0026quot; operator: \u0026quot;Equal\u0026quot; value: \u0026quot;true\u0026quot; effect: \u0026quot;PreferNoSchedule\u0026quot;  We have provided a solution file below that you can use to compare.\n     Related files   deployment-solution.yml  (1 ko)    Redeploy the Frontend on Spot First let\u0026rsquo;s take a look at all pods deployed on Spot instances\nfor n in $(kubectl get nodes -l lifecycle=Ec2Spot --no-headers | cut -d \u0026quot; \u0026quot; -f1); do kubectl get pods --all-namespaces --no-headers --field-selector spec.nodeName=${n} ; done  Now we will redeploy our microservices with our edited Frontend Manifest\ncd ~/environment/ecsdemo-frontend kubectl apply -f kubernetes/service.yaml kubectl apply -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl apply -f kubernetes/service.yaml kubectl apply -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl apply -f kubernetes/service.yaml kubectl apply -f kubernetes/deployment.yaml  We can again check all pods deployed on Spot Instances and should now see the frontend pods running on Spot instances\nfor n in $(kubectl get nodes -l lifecycle=Ec2Spot --no-headers | cut -d \u0026quot; \u0026quot; -f1); do kubectl get pods --all-namespaces --no-headers --field-selector spec.nodeName=${n} ; done  "
},
{
	"uri": "/cleanup/eksctl/",
	"title": "Delete the EKSCTL Cluster",
	"tags": [],
	"description": "",
	"content": "In order to delete the resources created for this EKS cluster, run the following commands:\nDelete the cluster:\neksctl delete cluster --name=eksworkshop-eksctl-yourusername  The nodegroup will have to complete the deletion process before the EKS cluster can be deleted. The total process will take approximately 15 minutes, and can be monitored via the CloudFormation Console\n "
},
{
	"uri": "/eksctl/test/",
	"title": "Test the Cluster",
	"tags": [],
	"description": "",
	"content": " Test the cluster: Confirm your Nodes:\nkubectl get nodes # if we see our 3 nodes, we know we have authenticated correctly  Export the Worker Role Name for use throughout the workshop\nSTACK_NAME=$(eksctl get nodegroup --cluster eksworkshop-eksctl-yourusername -o json | jq -r '.[].StackName') INSTANCE_PROFILE_ARN=$(aws cloudformation describe-stacks --stack-name $STACK_NAME | jq -r '.Stacks[].Outputs[] | select(.OutputKey==\u0026quot;InstanceProfileARN\u0026quot;) | .OutputValue') ROLE_NAME=$(aws cloudformation describe-stacks --stack-name $STACK_NAME | jq -r '.Stacks[].Outputs[] | select(.OutputKey==\u0026quot;InstanceRoleARN\u0026quot;) | .OutputValue' | cut -f2 -d/) echo \u0026quot;export ROLE_NAME=${ROLE_NAME}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile echo \u0026quot;export INSTANCE_PROFILE_ARN=${INSTANCE_PROFILE_ARN}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile  Congratulations! You now have a fully working Amazon EKS Cluster that is ready to use!\n"
},
{
	"uri": "/servicemesh_with_appmesh/create_app_mesh_components/about_sidecar/",
	"title": "About Sidecars",
	"tags": [],
	"description": "",
	"content": "As decoupled logic, an App Mesh sidecar container must run alongside each pod in the DJ App deployment.\nThis can be setup in few different ways:\n Before installing the deployment, we could modify the DJ App deployment\u0026rsquo;s container specs to include App Mesh sidecar containers. When deployed, it would run the sidecar.\n After installing the deployment, we could patch the deployment to include the sidecar container specs. Upon applying this patch, the old pods would be torn down, and the new pods would come up with the sidecar.\n We can implement the App Mesh Injector Controller, which watches for new pods to be created, and automatically adds the sidecar data to the pods as they are deployed.\n  For this tutorial, we\u0026rsquo;ll walk through the App Mesh Injector Controller option, as it will enable subsequent pod deployments to come up with the App Mesh sidecar automatically. This is not only quicker in the long run, but it also reduces the chances of typos that manual editing may introduce.\n"
},
{
	"uri": "/dashboard/connect/",
	"title": "Access the Dashboard",
	"tags": [],
	"description": "",
	"content": "Now we can access the Kubernetes Dashboard\n In your Cloud9 environment, click Tools / Preview / Preview Running Application Scroll to the end of the URL and append:  /api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/  Open a New Terminal Tab and enter\naws eks get-token --cluster-name eksworkshop-eksctl-yourusername | jq -r '.status.token'  Copy the output of this command and then click the radio button next to Token then in the text field below paste the output from the last command.\nThen press Sign In.\nIf you want to see the dashboard in a full tab, click the Pop Out button, like below: "
},
{
	"uri": "/scaling/deploy_ca/",
	"title": "Configure Cluster Autoscaler (CA)",
	"tags": [],
	"description": "",
	"content": " Cluster Autoscaler for AWS provides integration with Auto Scaling groups. It enables users to choose from four different options of deployment:\n One Auto Scaling group - This is what we will use Multiple Auto Scaling groups Auto-Discovery Master Node setup  Configure the Cluster Autoscaler (CA) We have provided a manifest file to deploy the CA. Copy the commands below into your Cloud9 Terminal.\nmkdir ~/environment/cluster-autoscaler cd ~/environment/cluster-autoscaler wget https://eksworkshop.com/scaling/deploy_ca.files/cluster_autoscaler.yml  Configure the ASG We will need to provide the name of the Autoscaling Group that we want CA to manipulate. Collect the name of the Auto Scaling Group (ASG) containing your worker nodes. Record the name somewhere. We will use this later in the manifest file.\nYou can find it in the console by following this link.\nCheck the box beside the ASG and click Actions and Edit\nChange the following settings:\n Min: 2 Max: 8  Click Save\nConfigure the Cluster Autoscaler Using the file browser on the left, open cluster_autoscaler.yml\nSearch for command: and within this block, replace the placeholder text \u0026lt;AUTOSCALING GROUP NAME\u0026gt; with the ASG name that you copied in the previous step. Also, update AWS_REGION value to reflect the region you are using and Save the file.\ncommand: - ./cluster-autoscaler - --v=4 - --stderrthreshold=info - --cloud-provider=aws - --skip-nodes-with-local-storage=false - --nodes=2:8:eksctl-eksworkshop-eksctl-yourusername-nodegroup-0-NodeGroup-SQG8QDVSR73G env: - name: AWS_REGION value: us-east-1  This command contains all of the configuration for the Cluster Autoscaler. The primary config is the --nodes flag. This specifies the minimum nodes (2), max nodes (8) and ASG Name.\nAlthough Cluster Autoscaler is the de facto standard for automatic scaling in K8s, it is not part of the main release. We deploy it like any other pod in the kube-system namespace, similar to other management pods.\nCreate an IAM Policy We need to configure an inline policy and add it to the EC2 instance profile of the worker nodes\nEnsure ROLE_NAME is set in your environment:\ntest -n \u0026quot;$ROLE_NAME\u0026quot; \u0026amp;\u0026amp; echo ROLE_NAME is \u0026quot;$ROLE_NAME\u0026quot; || echo ROLE_NAME is not set  If ROLE_NAME is not set, please review: /eksctl/test/\nmkdir ~/environment/asg_policy cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/asg_policy/k8s-asg-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;autoscaling:DescribeAutoScalingGroups\u0026quot;, \u0026quot;autoscaling:DescribeAutoScalingInstances\u0026quot;, \u0026quot;autoscaling:SetDesiredCapacity\u0026quot;, \u0026quot;autoscaling:TerminateInstanceInAutoScalingGroup\u0026quot;, \u0026quot;autoscaling:DescribeTags\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name ASG-Policy-For-Worker --policy-document file://~/environment/asg_policy/k8s-asg-policy.json  Validate that the policy is attached to the role\naws iam get-role-policy --role-name $ROLE_NAME --policy-name ASG-Policy-For-Worker  Deploy the Cluster Autoscaler kubectl apply -f ~/environment/cluster-autoscaler/cluster_autoscaler.yml  Watch the logs\nkubectl logs -f deployment/cluster-autoscaler -n kube-system  We are now ready to scale our cluster   Related files   cluster_autoscaler.yml  (3 ko)    "
},
{
	"uri": "/logging/deploy/",
	"title": "Deploy Fluentd",
	"tags": [],
	"description": "",
	"content": "mkdir ~/environment/fluentd cd ~/environment/fluentd wget https://eksworkshop.com/logging/deploy.files/fluentd.yml  Explore the fluentd.yml to see what is being deployed. There is a link at the bottom of this page. The Fluentd log agent configuration is located in the Kubernetes ConfigMap. Fluentd will be deployed as a DaemonSet, i.e. one pod per worker node. In our case, a 3 node cluster is used and so 3 pods will be shown in the output when we deploy.\nUpdate REGION and CLUSTER_NAME environment variables in fluentd.yml as required. They are set to us-west-2 and eksworkshop-eksctl-yourusername by default.\n kubectl apply -f ~/environment/fluentd/fluentd.yml  Watch for all of the pods to change to running status\nkubectl get pods -w --namespace=kube-system  We are now ready to check that logs are arriving in CloudWatch Logs\nSelect the region that is mentioned in fluentd.yml to browse the Cloudwatch Log Group if required.\n  Related files   fluentd.yml  (6 ko)    "
},
{
	"uri": "/helm_root/helm_micro/deploy/",
	"title": "Deploy the eksdemo Chart",
	"tags": [],
	"description": "",
	"content": " Use the dry-run flag to test our templates To test the syntax and validity of the Chart without actually deploying it, we\u0026rsquo;ll use the dry-run flag.\nThe following command will build and output the rendered templates without installing the Chart:\nhelm install --debug --dry-run --name workshop ~/environment/eksdemo  Confirm that the values created by the template look correct.\nDeploy the chart Now that we have tested our template, lets install it.\nhelm install --name workshop ~/environment/eksdemo  Similar to what we saw previously in the NGINX Helm Chart example, an output of the Deployment, Pod, and Service objects are output, similar to:\nNAME: workshop LAST DEPLOYED: Fri Nov 16 21:42:00 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Service NAME AGE ecsdemo-crystal 0s ecsdemo-frontend 0s ecsdemo-nodejs 0s ==\u0026gt; v1/Deployment ecsdemo-crystal 0s ecsdemo-frontend 0s ecsdemo-nodejs 0s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE ecsdemo-crystal-764b9cb9bc-4dwqt 0/1 ContainerCreating 0 0s ecsdemo-crystal-764b9cb9bc-hcb62 0/1 ContainerCreating 0 0s ecsdemo-crystal-764b9cb9bc-vl7nr 0/1 ContainerCreating 0 0s ecsdemo-frontend-67876457f6-2xrtb 0/1 ContainerCreating 0 0s ecsdemo-frontend-67876457f6-bfnc5 0/1 ContainerCreating 0 0s ecsdemo-frontend-67876457f6-rb6rg 0/1 ContainerCreating 0 0s ecsdemo-nodejs-c458bf55d-994cq 0/1 ContainerCreating 0 0s ecsdemo-nodejs-c458bf55d-9qtbm 0/1 ContainerCreating 0 0s ecsdemo-nodejs-c458bf55d-s9zkh 0/1 ContainerCreating 0 0s  "
},
{
	"uri": "/jenkins/loggingin/",
	"title": "Logging In",
	"tags": [],
	"description": "",
	"content": "Now that we have the ELB address of your jenkins instance we can go an navigate to that address in another window.\nFrom here we can log in using:\n   Username Password     admin command from below    printf $(kubectl get secret --namespace default cicd-jenkins -o jsonpath=\u0026quot;{.data.jenkins-admin-password}\u0026quot; | base64 --decode);echo  The output of this command will give you the default password for your admin user. Log into the jenkins login screen using these credentials.\n"
},
{
	"uri": "/servicemesh_with_appmesh/create_the_k8s_app/set_perms/",
	"title": "Set Permissions",
	"tags": [],
	"description": "",
	"content": " Next, we\u0026rsquo;ll setup the workers to have the correct permissions to run App Mesh API calls.\nVerify that $ROLE_NAME is defined by running the following command:\necho $ROLE_NAME  If this variable is not defined (the above command returns an empty value), please visit the cluster test chapter and run through the export role name step.\nAlso be sure you have your region defined. Verify its set by running the following command:\necho $AWS_REGION  If this variable is not defined (the above command returns an empty value), please run the following to define the AWS_REGION environmental variable:\nexport ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region') echo \u0026quot;export ACCOUNT_ID=${ACCOUNT_ID}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile echo \u0026quot;export AWS_REGION=${AWS_REGION}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region  Copy and paste the below code to add the permissions as an inline policy to your worker node instances:\nSetup Permissions for the Worker Nodes cat \u0026lt;\u0026lt;EoF \u0026gt; k8s-appmesh-worker-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;appmesh:DescribeMesh\u0026quot;, \u0026quot;appmesh:DescribeVirtualNode\u0026quot;, \u0026quot;appmesh:DescribeVirtualService\u0026quot;, \u0026quot;appmesh:DescribeVirtualRouter\u0026quot;, \u0026quot;appmesh:DescribeRoute\u0026quot;, \u0026quot;appmesh:CreateMesh\u0026quot;, \u0026quot;appmesh:CreateVirtualNode\u0026quot;, \u0026quot;appmesh:CreateVirtualService\u0026quot;, \u0026quot;appmesh:CreateVirtualRouter\u0026quot;, \u0026quot;appmesh:CreateRoute\u0026quot;, \u0026quot;appmesh:UpdateMesh\u0026quot;, \u0026quot;appmesh:UpdateVirtualNode\u0026quot;, \u0026quot;appmesh:UpdateVirtualService\u0026quot;, \u0026quot;appmesh:UpdateVirtualRouter\u0026quot;, \u0026quot;appmesh:UpdateRoute\u0026quot;, \u0026quot;appmesh:ListMeshes\u0026quot;, \u0026quot;appmesh:ListVirtualNodes\u0026quot;, \u0026quot;appmesh:ListVirtualServices\u0026quot;, \u0026quot;appmesh:ListVirtualRouters\u0026quot;, \u0026quot;appmesh:ListRoutes\u0026quot;, \u0026quot;appmesh:DeleteMesh\u0026quot;, \u0026quot;appmesh:DeleteVirtualNode\u0026quot;, \u0026quot;appmesh:DeleteVirtualService\u0026quot;, \u0026quot;appmesh:DeleteVirtualRouter\u0026quot;, \u0026quot;appmesh:DeleteRoute\u0026quot;, \u0026quot;appmesh:StreamAggregatedResources\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name AM-Policy-For-Worker --policy-document file://k8s-appmesh-worker-policy.json  To verify the policy was attached to the role, run the following command:\naws iam get-role-policy --role-name $ROLE_NAME --policy-name AM-Policy-For-Worker  "
},
{
	"uri": "/statefulset/testfailure/",
	"title": "Test Failure",
	"tags": [],
	"description": "",
	"content": " Unhealthy container MySQL container uses readiness probe by running mysql -h 127.0.0.1 -e \u0026lsquo;SELECT 1\u0026rsquo; on the server to make sure MySQL server is still active. Open a new terminal and simulate MySQL as being unresponsive by following command.\nkubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql /usr/bin/mysql.off  This command renames the /usr/bin/mysql command so that readiness probe can\u0026rsquo;t find it. During the next health check, the pod should report one of it\u0026rsquo;s containers is not healthy. This can be verified by following command.\nkubectl get pod mysql-2  NAME READY STATUS RESTARTS AGE mysql-2 1/2 Running 0 12m  mysql-read load balancer detects failures and takes action by not sending traffic to the failed container, @@server_id 102. You can check this by the loop running in separate window from previous section. The loop shows the following output.\n+-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:00:45 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:00:46 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:00:47 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:00:48 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:00:49 | +-------------+---------------------+  Revert back to its initial state at the previous terminal.\nkubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql.off /usr/bin/mysql  Check the status again to see that both containers are running and healthy\n$ kubectl get pod -w mysql-2  NAME READY STATUS RESTARTS AGE mysql-2 2/2 Running 0 5h  The loop in another terminal is now showing @@server_id 102 is back and all three servers are running. Press Ctrl+C to stop watching.\nFailed pod To simulate a failed pod, delete mysql-2 pod by following command.\nkubectl delete pod mysql-2  pod \u0026quot;mysql-2\u0026quot; deleted  StatefulSet controller recognizes failed pod and creates a new one to maintain the number of replicas with them same name and link to the same PersistentVolumeClaim.\n$ kubectl get pod -w mysql-2  NAME READY STATUS RESTARTS AGE mysql-2 2/2 Terminating 0 1d mysql-2 0/2 Terminating 0 1d mysql-2 0/2 Terminating 0 1d mysql-2 0/2 Terminating 0 1d mysql-2 0/2 Pending 0 0s mysql-2 0/2 Pending 0 0s mysql-2 0/2 Init:0/2 0 0s mysql-2 0/2 Init:1/2 0 10s mysql-2 0/2 PodInitializing 0 11s mysql-2 1/2 Running 0 12s mysql-2 2/2 Running 0 16s  Press Ctrl+C to stop watching.\n"
},
{
	"uri": "/weave_flux/deploymenthelm/",
	"title": "Deploy from Helm",
	"tags": [],
	"description": "",
	"content": "You can use this same approach to deploy Helm charts. These charts can exist within the configuration Git repository (k8s-config), or hosted from an external chart repository. In this example we will use an external chart to keep things simple.\nIn your k8s-config directory, create a namespace manifest.\nThe git pull command ensures we have the latest configuration in case Flux modified anything.\n cd ../k8s-config git pull cat \u0026lt;\u0026lt; EOF \u0026gt; namespaces/nginx.yaml apiVersion: v1 kind: Namespace metadata: labels: name: nginx name: nginx EOF  Now create a Helm release manifest. This is a custom resource definition provided by Weave Flux.\ncat \u0026lt;\u0026lt; EOF \u0026gt; releases/nginx.yaml --- apiVersion: flux.weave.works/v1beta1 kind: HelmRelease metadata: name: mywebserver namespace: nginx annotations: flux.weave.works/automated: \u0026quot;true\u0026quot; flux.weave.works/tag.nginx: semver:~1.16 flux.weave.works/locked: 'true' flux.weave.works/locked_msg: '\u0026quot;Halt updates for now\u0026quot;' flux.weave.works/locked_user: User Name \u0026lt;user@example.com\u0026gt; spec: releaseName: mywebserver chart: repository: https://charts.bitnami.com/bitnami/ name: nginx version: 3.3.2 values: usePassword: true image: registry: docker.io repository: bitnami/nginx tag: 1.16.0-debian-9-r46 service: type: LoadBalancer port: 80 nodePorts: http: \u0026quot;\u0026quot; externalTrafficPolicy: Cluster ingress: enabled: false livenessProbe: httpGet: path: / port: http initialDelaySeconds: 30 timeoutSeconds: 5 failureThreshold: 6 readinessProbe: httpGet: path: / port: http initialDelaySeconds: 5 timeoutSeconds: 3 periodSeconds: 5 metrics: enabled: false EOF  You will notice a few additional annotations above.\n flux.weave.works/locked tells Flux to lock the deployment so a new image version will not be deployed.\n flux.weave.works/tag.nginx filters the images available by semantic versioning.\n  Now commit the changes and wait up to 5 minutes for Flux to pull in the configuration.\ngit add . git commit -am \u0026quot;Adding nginx helm release\u0026quot; git push  Verify the deployment as follows.\nUse your pod name below for kubectl logs\n kubectl get pods -n flux kubectl logs flux-5bd7fb6bb6-4sc78 -n flux helm ls kubectl get all -n nginx  If this doesn\u0026rsquo;t deploy, check to ensure helm was initialized. Also, look at the Flux Helm operator to see if there are any errors.\nkubectl get pods -n kube-system | grep tiller kubectl get pods -n flux kubectl logs flux-helm-operator-df5746688-84kw8 -n flux  You\u0026rsquo;ve now seen how Weave Flux can enable a GitOps approach to deployment.\n"
},
{
	"uri": "/dashboard/",
	"title": "Deploy the Kubernetes Dashboard",
	"tags": [],
	"description": "",
	"content": " Deploy the Kubernetes Dashboard In this Chapter, we will deploy the official Kubernetes dashboard, and connect through our Cloud9 Workspace.\n"
},
{
	"uri": "/network-policies/tigera/install/",
	"title": "Installing Tigera Secure Cloud Edition",
	"tags": [],
	"description": "",
	"content": " Now that your environment variables are set, and tsctl is installed, we need to install TSCE itself. To do so, run the following command. The instructions in the Tigera Secure CE v1.0.1 document that you downloaded earlier are almost exactly the same as what is shown here. The only difference is that we\u0026rsquo;ve changed a variable name from $TOKEN to $TS_TOKEN to avoid colliding with other $TOKEN variables that might be set in your environment.\ntsctl install --token $TS_TOKEN \\ --kubeconfig ~/.kube/config \\ --cluster-name $CLUSTER_NAME \\ --vpc-id $VPC_ID \\ --control-plane-sg $CONTROL_PLANE_SG \\ --node-sgs $K8S_NODE_SGS  Copy that text and run it in your Cloud9 shell. If all goes well, in a few minutes, you should be running TSCE, which augments Project Calico with a number of interesting capabilities which we will explore next.\nKnown Issues  It may take up to five seconds for pods to gain network connectivity after starting up.\n Network Load Balancers (NLBs) may lose their ability to balance traffic to pods after installing Tigera Secure CE. To resolve this issue, manually modify the pods’ security group to allow ingress traffic from the original source of the traffic (not the NLB). See the User Guide for more information or contact Tigera support for assistance.\n  "
},
{
	"uri": "/prerequisites/sshkey/",
	"title": "Create an SSH key",
	"tags": [],
	"description": "",
	"content": " Starting from here, when you see command to be entered such as below, you will enter these commands into Cloud9 IDE. You can use the Copy to clipboard feature (right hand upper corner) to simply copy and paste into Cloud9. In order to paste, you can use Ctrl + V for Windows or Command + V for Mac.\n Please run this command to generate SSH Key in Cloud9. This key will be used on the worker node instances to allow ssh access if necessary.\nssh-keygen  Press enter 3 times to take the default choices\n Upload the public key to your EC2 region:\naws ec2 import-key-pair --key-name \u0026quot;eksworkshop-yourusername\u0026quot; --public-key-material file://~/.ssh/id_rsa.pub  "
},
{
	"uri": "/deploy/",
	"title": "Deploy the Example Microservices",
	"tags": [],
	"description": "",
	"content": " Deploy the Example Microservices  Deploy our Sample Applications   Deploy NodeJS Backend API   Deploy Crystal Backend API   Let\u0026#39;s check Service Types   Ensure the ELB Service Role exists   Deploy Frontend Service   Find the Service Address   Scale the Backend Services   Scale the Frontend   Cleanup the applications   "
},
{
	"uri": "/helm_root/",
	"title": "Helm",
	"tags": [],
	"description": "",
	"content": " Kubernetes Helm Helm is a package manager for Kubernetes that packages multiple Kubernetes resources into a single logical deployment unit called Chart.\nIn this chapter, we\u0026rsquo;ll cover installing Helm. Once installed, we\u0026rsquo;ll demonstrate how Helm can be used to deploy a simple NGINX webserver, and a more sophisticated microservice.\n"
},
{
	"uri": "/statefulset/testscaling/",
	"title": "Test Scaling",
	"tags": [],
	"description": "",
	"content": " More slaves can be added to the MySQL Cluster to increase read capacity. This can be done by following command.\nkubectl scale statefulset mysql --replicas=5  You can see the message that statefulset \u0026ldquo;mysql\u0026rdquo; scaled.\nstatefulset \u0026quot;mysql\u0026quot; scaled  Watch the progress of ordered and graceful scaling.\nkubectl get pods -l app=mysql -w  NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 1d mysql-1 2/2 Running 0 1d mysql-2 2/2 Running 0 24m mysql-3 0/2 Init:0/2 0 8s mysql-3 0/2 Init:1/2 0 9s mysql-3 0/2 PodInitializing 0 11s mysql-3 1/2 Running 0 12s mysql-3 2/2 Running 0 16s mysql-4 0/2 Pending 0 0s mysql-4 0/2 Pending 0 0s mysql-4 0/2 Init:0/2 0 0s mysql-4 0/2 Init:1/2 0 10s mysql-4 0/2 PodInitializing 0 11s mysql-4 1/2 Running 0 12s mysql-4 2/2 Running 0 17s  It may take few minutes to launch all the pods.\n Press Ctrl+C to stop watching. Open another terminal to check loop if you closed it.\nkubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\\ bash -ic \u0026quot;while sleep 1; do mysql -h mysql-read -e 'SELECT @@server_id,NOW()'; done\u0026quot;  You will see 5 servers are running.\n+-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:56:42 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 102 | 2018-11-14 13:56:43 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 104 | 2018-11-14 13:56:44 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:56:45 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 104 | 2018-11-14 13:56:46 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:56:47 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:56:48 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 103 | 2018-11-14 13:56:49 | +-------------+---------------------+  Verify if the newly deployed slave (mysql-3) have the same data set by following command.\nkubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\\ mysql -h mysql-3.mysql -e \u0026quot;SELECT * FROM test.messages\u0026quot;  It will show the same data that master has.\n+--------------------------+ | message | +--------------------------+ | hello, from mysql-client | +--------------------------+  Scale down replicas to 3 by following command.\nkubectl scale statefulset mysql --replicas=3  You can see statefulset \u0026ldquo;mysql\u0026rdquo; scaled\nstatefulset \u0026quot;mysql\u0026quot; scaled  Note that scale in doesn\u0026rsquo;t delete the data or PVCs attached to the pods. You have to delete them manually. Check scale in is completed by following command.\nkubectl get pods -l app=mysql  NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 1d mysql-1 2/2 Running 0 1d mysql-2 2/2 Running 0 35m  Check 2 PVCs(data-mysql-3, data-mysql-4) still exist by following command.\nkubectl get pvc -l app=mysql  NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-mysql-0 Bound pvc-83e9dfeb-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-1 Bound pvc-977e7806-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-2 Bound pvc-b3009b02-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-3 Bound pvc-de14acd8-e811-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 34m data-mysql-4 Bound pvc-e916c3ec-e812-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 26m  Challenge: By default, deleting a PersistentVolumeClaim will delete its associated persistent volume. What if you wanted to keep the volume? Change the reclaim policy of the PersistentVolume associated with PVC \u0026ldquo;data-mysql-3\u0026rdquo; to \u0026ldquo;Retain\u0026rdquo;. Please see Kubernetes documentation for help\n  Expand here to see the solution   Change the reclaim policy:\nkubectl patch pv \u0026lt;your-pv-name\u0026gt; -p '{\u0026quot;spec\u0026quot;:{\u0026quot;persistentVolumeReclaimPolicy\u0026quot;:\u0026quot;Retain\u0026quot;}}'  Now, if you delete the PersistentVolumeClaim data-mysql-3, you can still see the EBS volume in your AWS EC2 console, with its state as \u0026ldquo;available\u0026rdquo;.\nLet\u0026rsquo;s change the reclaim policy back to \u0026ldquo;Delete\u0026rdquo; to avoid orphaned volumes:\nkubectl patch pv \u0026lt;your-pv-name\u0026gt; -p '{\u0026quot;spec\u0026quot;:{\u0026quot;persistentVolumeReclaimPolicy\u0026quot;:\u0026quot;Delete\u0026quot;}}'    Delete data-mysql-3, data-mysql-4 by following command.\nkubectl delete pvc data-mysql-3 kubectl delete pvc data-mysql-4  persistentvolumeclaim \u0026quot;data-mysql-3\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-4\u0026quot; deleted  "
},
{
	"uri": "/weave_flux/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing the GitOps with Weave Flux module.\nThis module is not used in subsequent steps, so you can remove the resources now, or at the end of the workshop.\nFirst, delete all images from the Amazon ECR Repository.\nNext, go to the CloudFormation Console and delete the stack used to deploy the image build CodePipeline\nNow, delete Weave Flux and your load balanced services\nhelm delete flux --purge kubectl delete ns flux kubectl delete crd helmreleases.flux.weave.works helm delete mywebserver --purge kubectl delete ns nginx kubectl delete svc eks-example -n eks-example kubectl delete deployment eks-example -n eks-example kubectl delete ns eks-example  Optionally go to GitHub and delete your k8s-config and eks-example repositories.\nIf you are using your own account for this workshop, continue with the below steps. If doing this at an AWS event, skip the steps below.\n Remove IAM roles you previously created\naws iam delete-role-policy --role-name eksworkshop-CodePipelineServiceRole --policy-name codepipeline-access aws iam delete-role --role-name eksworkshop-CodePipelineServiceRole aws iam delete-role-policy --role-name eksworkshop-CodeBuildServiceRole --policy-name codebuild-access aws iam delete-role --role-name eksworkshop-CodeBuildServiceRole  Remove the artifact bucket you previously created\nACCOUNT_ID=$(aws sts get-caller-identity | jq -r '.Account') aws s3 rb s3://eksworkshop-${ACCOUNT_ID}-codepipeline-artifacts --force  "
},
{
	"uri": "/kubeflow/training/",
	"title": "Model training",
	"tags": [],
	"description": "",
	"content": " Model Training While Jupyter notebook is good for interactive model training, you may like to package the training code as Docker image and run it in Amazon EKS cluster.\nThis chapter explains how to build a training model for Fashion-MNIST dataset using TensorFlow and Keras on Amazon EKS. This databset contains 70,000 grayscale images in 10 categories and is meant to be a drop-in replace of MNIST.\nDocker image You can use a pre-built Docker image seedjeffwan/mnist_tensorflow_keras:1.13.1. This image uses tensorflow/tensorflow:1.13.1 as the base image. The image has training code and downloads training and test data sets. It also stores the generated model in an S3 bucket.\nAlternatively, you can use Dockerfile to build the image:\ndocker build -t \u0026lt;dockerhub_username\u0026gt;/\u0026lt;repo_name\u0026gt;:\u0026lt;tag_name\u0026gt; .  Create S3 bucket Create an S3 bucket where training model will be saved:\nexport S3_BUCKET=eks-ml-data aws s3 mb s3://$S3_BUCKET --region $AWS_REGION  This name will be used in the pod specification later. This bucket is also used for serving the model.\nIf you want to use an existing bucket in a different region, then make sure to specify the exact region as the value of AWS_REGION environment variable in mnist-training.yaml.\nSetup AWS credentials in EKS cluster AWS credentials are required to save model on S3 bucket. These credentials are stored in EKS cluster as Kubernetes secrets.\nGet your AWS access key id and secret access key.\nReplace AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in the following command with values specific to your environment.\nexport AWS_ACCESS_KEY_ID_VALUE=$(echo -n 'AWS_ACCESS_KEY_ID' | base64) export AWS_SECRET_ACCESS_KEY_VALUE=$(echo -n 'AWS_SECRET_ACCESS_KEY' | base64)  Apply to EKS cluster:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: aws-secret type: Opaque data: AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID_VALUE AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY_VALUE EOF  Run training using pod Create pod:\ncurl -LO https://eksworkshop.com/kubeflow/kubeflow.files/mnist-training.yaml envsubst \u0026lt;mnist-training.yaml | kubectl create -f -  This will start a pod which will start the training and save the generated model in S3 bucket. Check status:\nkubectl get pods NAME READY STATUS RESTARTS AGE mnist-training 1/1 Running 0 2m45s    Expand here to see complete logs   kubectl logs mnist-training -f Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 1us/step 40960/29515 [=========================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 0s 0us/step 26435584/26421880 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 16384/5148 [===============================================================================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step 4431872/4422102 [==============================] - 0s 0us/step WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer. 2019-08-29 00:32:10.652905: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 2019-08-29 00:32:10.659233: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300070000 Hz 2019-08-29 00:32:10.661111: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x45baf40 executing computations on platform Host. Devices: 2019-08-29 00:32:10.661139: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): \u0026lt;undefined\u0026gt;, \u0026lt;undefined\u0026gt; 2019-08-29 00:32:10.718125: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /root//.aws/config and using profilePrefix = 1 2019-08-29 00:32:10.718160: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /root//.aws/credentials and using profilePrefix = 0 2019-08-29 00:32:10.718174: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /root//.aws/credentials for credentials file and /root//.aws/config for the config file , for use with profile default 2019-08-29 00:32:10.718184: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating HttpClient with max connections2 and scheme http 2019-08-29 00:32:10.718196: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2 2019-08-29 00:32:10.718207: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 900000 2019-08-29 00:32:10.718224: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:10.718275: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25 2019-08-29 00:32:10.718341: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:10.718468: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2 2019-08-29 00:32:10.718490: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:11.036616: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:32:11.036661: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:32:11.036724: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:11.036807: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:11.204229: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:11.204327: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:11.281479: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:32:11.281513: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:32:11.281551: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:11.281615: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:11.388175: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:11.388285: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:11.550463: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:11.550639: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:11.628831: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:11.628915: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:11.709359: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:11.709455: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:12.017431: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:12.017573: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:12.096831: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:12.096933: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. train_images.shape: (60000, 28, 28, 1), of float64 test_images.shape: (10000, 28, 28, 1), of float64 _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= Conv1 (Conv2D) (None, 13, 13, 8) 80 _________________________________________________________________ flatten (Flatten) (None, 1352) 0 _________________________________________________________________ Softmax (Dense) (None, 10) 13530 ================================================================= Total params: 13,610 Trainable params: 13,610 Non-trainable params: 0 _________________________________________________________________ Epoch 1/40 2019-08-29 00:32:16.840512: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:16.840633: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:17.280630: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:17.280744: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:17.384333: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:17.384520: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 87us/sample - loss: 0.5496 - acc: 0.8082 Epoch 2/40 2019-08-29 00:32:21.952054: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:21.952176: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:22.369041: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:22.369238: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:22.446531: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:22.446629: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.4137 - acc: 0.8548 Epoch 3/40 2019-08-29 00:32:27.021467: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:27.021592: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:27.454086: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:27.454230: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:27.534720: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:27.534816: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.3763 - acc: 0.8685 Epoch 4/40 2019-08-29 00:32:32.130604: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:32.130728: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:32.517514: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:32.517630: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:32.629178: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:32.629262: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.3555 - acc: 0.8746 Epoch 5/40 2019-08-29 00:32:37.235765: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:37.235889: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:37.736414: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:37.736525: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:37.813549: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:37.813632: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 86us/sample - loss: 0.3415 - acc: 0.8794 Epoch 6/40 2019-08-29 00:32:42.400365: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:42.400527: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:42.809268: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:42.809409: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:42.887120: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:42.887209: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.3283 - acc: 0.8835 Epoch 7/40 2019-08-29 00:32:47.474549: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:47.474676: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:47.885577: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:47.885686: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:47.963577: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:47.963662: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.3188 - acc: 0.8868 Epoch 8/40 2019-08-29 00:32:52.571365: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:52.571487: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:52.973365: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:52.973461: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:53.051547: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:53.051711: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.3112 - acc: 0.8887 Epoch 9/40 2019-08-29 00:32:57.620454: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:57.620579: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:58.045196: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:58.045301: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:32:58.123871: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:32:58.123956: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.3036 - acc: 0.8924 Epoch 10/40 2019-08-29 00:33:02.735621: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:02.735784: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:03.155609: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:03.155717: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:03.237484: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:03.237568: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 86us/sample - loss: 0.2964 - acc: 0.8943 Epoch 11/40 2019-08-29 00:33:07.847167: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:07.847295: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:08.308130: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:08.308233: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:08.385677: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:08.385761: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2904 - acc: 0.8966 Epoch 12/40 2019-08-29 00:33:12.989568: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:12.989709: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:13.425758: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:13.425871: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:13.503980: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:13.504066: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2850 - acc: 0.8979 Epoch 13/40 2019-08-29 00:33:18.084636: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:18.084799: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:18.505749: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:18.505889: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:18.584930: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:18.585086: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2788 - acc: 0.8994 Epoch 14/40 2019-08-29 00:33:23.165093: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:23.165216: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:23.583005: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:23.583125: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:23.660931: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:23.661017: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2743 - acc: 0.9016 Epoch 15/40 2019-08-29 00:33:28.273507: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:28.273630: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:28.656655: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:28.656805: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:28.735635: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:28.735757: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 84us/sample - loss: 0.2702 - acc: 0.9025 Epoch 16/40 2019-08-29 00:33:33.340967: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:33.341091: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:33.797569: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:33.797673: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:33.876101: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:33.876187: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 86us/sample - loss: 0.2668 - acc: 0.9032 Epoch 17/40 2019-08-29 00:33:38.485389: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:38.485516: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:38.911662: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:38.911776: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:38.990577: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:38.990673: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2627 - acc: 0.9059 Epoch 18/40 2019-08-29 00:33:43.586335: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:43.586462: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:43.982270: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:43.982444: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:44.061595: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:44.061765: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2587 - acc: 0.9072 Epoch 19/40 2019-08-29 00:33:48.666451: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:48.666582: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:49.113733: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:49.113835: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:49.191768: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:49.191853: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2542 - acc: 0.9082 Epoch 20/40 2019-08-29 00:33:53.778720: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:53.778845: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:54.275408: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:54.275506: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:54.354271: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:54.354356: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 86us/sample - loss: 0.2521 - acc: 0.9092 Epoch 21/40 2019-08-29 00:33:58.946098: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:58.946222: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:59.369881: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:59.369985: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:33:59.449359: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:33:59.449538: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2481 - acc: 0.9108 Epoch 22/40 2019-08-29 00:34:04.040611: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:04.040733: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:04.459577: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:04.459698: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:04.537060: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:04.537154: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2457 - acc: 0.9116 Epoch 23/40 2019-08-29 00:34:09.122286: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:09.122409: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:09.542468: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:09.542659: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:09.633226: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:09.633310: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 86us/sample - loss: 0.2419 - acc: 0.9119 Epoch 24/40 2019-08-29 00:34:14.283736: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:14.283861: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:14.759453: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:14.759588: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:14.840762: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:14.840865: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:14.924147: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:14.924254: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:15.297162: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:15.297277: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:15.374905: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:15.375009: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 6s 95us/sample - loss: 0.2388 - acc: 0.9141 Epoch 25/40 2019-08-29 00:34:20.010218: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:20.010338: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:20.431755: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:20.431867: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:20.511302: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:20.511404: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2368 - acc: 0.9146 Epoch 26/40 2019-08-29 00:34:25.085846: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:25.085965: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:25.497865: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:25.497980: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:25.575489: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:25.575573: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 84us/sample - loss: 0.2345 - acc: 0.9151 Epoch 27/40 2019-08-29 00:34:30.165576: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:30.165696: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:30.585389: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:30.585504: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:30.663307: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:30.663409: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2308 - acc: 0.9172 Epoch 28/40 2019-08-29 00:34:35.239820: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:35.239945: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:35.664925: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:35.665038: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:35.743716: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:35.743799: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2294 - acc: 0.9172 Epoch 29/40 2019-08-29 00:34:40.319353: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:40.319497: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:40.729421: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:40.729536: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:40.807044: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:40.807129: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2273 - acc: 0.9182 Epoch 30/40 2019-08-29 00:34:45.400274: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:45.400403: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:46.006187: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:46.006303: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:46.080739: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:46.080829: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 87us/sample - loss: 0.2253 - acc: 0.9193 Epoch 31/40 2019-08-29 00:34:50.675446: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:50.675569: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:51.083387: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:51.083492: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:51.158345: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:51.158437: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2238 - acc: 0.9199 Epoch 32/40 2019-08-29 00:34:55.735525: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:55.735650: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:56.186660: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:56.186764: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:34:56.260818: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:34:56.260911: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2213 - acc: 0.9203 Epoch 33/40 2019-08-29 00:35:00.860052: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:00.860199: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:01.251599: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:01.251755: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:01.327938: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:01.328027: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 84us/sample - loss: 0.2196 - acc: 0.9205 Epoch 34/40 2019-08-29 00:35:05.913785: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:05.913909: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:06.448875: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:06.448994: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:06.523964: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:06.524112: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 87us/sample - loss: 0.2184 - acc: 0.9206 Epoch 35/40 2019-08-29 00:35:11.114671: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:11.114823: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:11.521477: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:11.521598: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:11.596112: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:11.596214: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2159 - acc: 0.9218 Epoch 36/40 2019-08-29 00:35:16.230868: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:16.230993: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:16.631740: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:16.631860: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:16.709297: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:16.709410: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2145 - acc: 0.9225 Epoch 37/40 2019-08-29 00:35:21.293198: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:21.293319: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:21.807158: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:21.807261: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:21.930544: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:21.930631: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 87us/sample - loss: 0.2136 - acc: 0.9232 Epoch 38/40 2019-08-29 00:35:26.531272: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:26.531393: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:26.934413: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:26.934524: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:27.041029: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:27.041135: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2117 - acc: 0.9235 Epoch 39/40 2019-08-29 00:35:31.632210: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:31.632333: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:32.032924: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:32.033065: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:32.107077: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:32.107193: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 84us/sample - loss: 0.2108 - acc: 0.9241 Epoch 40/40 2019-08-29 00:35:36.705902: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:36.706024: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:37.106458: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:37.106617: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:37.183601: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:37.183817: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 60000/60000 [==============================] - 5s 85us/sample - loss: 0.2098 - acc: 0.9239 2019-08-29 00:35:37.263849: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:37.263982: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:37.457410: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0032111567038731387 10000/10000 [==============================] - 0s 44us/sample - loss: 0.3531 - acc: 0.8830 WARNING:tensorflow:From mnist.py:69: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version. Instructions for updating: This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save. WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version. Instructions for updating: This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info. 2019-08-29 00:35:37.903206: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:37.903336: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:37.978201: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:37.978248: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:37.978318: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:37.978431: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.060440: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.060574: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.133815: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:38.133858: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:38.133913: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.134018: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.211956: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.212154: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.287561: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:38.287603: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:38.287662: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.287762: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.365346: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.365482: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.437001: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:38.437062: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:38.437133: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.437263: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.618714: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.618821: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.703515: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035381567038938618 2019-08-29 00:35:38.703638: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.703727: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.796327: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035381567038938703 2019-08-29 00:35:38.796732: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.796826: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:38.871391: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:38.871426: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:38.871468: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:38.871535: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.000565: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.000661: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.074122: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:39.074157: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:39.074197: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.074271: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.151349: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.151439: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.225500: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:39.225536: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:39.225574: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.225640: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.305893: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.305997: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.393168: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035391567038939305 2019-08-29 00:35:39.451779: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.451888: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.534538: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035391567038939451 2019-08-29 00:35:39.539846: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.539981: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.790995: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035391567038939534 2019-08-29 00:35:39.791131: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.791234: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:39.871382: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:39.871496: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.027665: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.027772: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.115533: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.115638: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.273357: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035401567038940115 2019-08-29 00:35:40.273461: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.273543: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.394230: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.394419: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.495666: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.495803: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.578868: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.578965: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.658188: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035401567038940578 2019-08-29 00:35:40.658293: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.658393: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.733400: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.733490: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.813995: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.814163: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.907589: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.907716: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:40.987771: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:40.987873: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.064912: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.065012: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.149777: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.149924: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.304768: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.304904: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.388975: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.389106: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.547755: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035411567038941388 2019-08-29 00:35:41.547853: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.547992: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.636644: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.636728: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.719947: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.720068: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.897549: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.897646: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:41.971144: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 2019-08-29 00:35:41.971186: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 2019-08-29 00:35:41.971243: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:41.971367: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:42.059414: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 2019-08-29 00:35:42.059526: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 2019-08-29 00:35:42.219035: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20190829T0035421567038942059 Test accuracy: 0.883000016212 Saved model: s3://eks-ml-data/mnist/tf_saved_model/1    The last line shows that the exported model is saved to S3 bucket.\n"
},
{
	"uri": "/advanced-networking/secondary_cidr/eniconfig_crd/",
	"title": "Create CRDs",
	"tags": [],
	"description": "",
	"content": " Create custom resources for ENIConfig CRD As next step, we will add custom resources to ENIConfig custom resource definition (CRD). CRD\u0026rsquo;s are extensions of Kubernetes API that stores collection of API objects of certain kind. In this case, we will store VPC Subnet and SecurityGroup configuration information in these CRD\u0026rsquo;s so that Worker nodes can access them to configure VPC CNI plugin.\nYou should have ENIConfig CRD already installed with latest CNI version (1.3+). You can check if its installed by running this command.\nkubectl get crd  You should see response similar to this\nNAME CREATED AT eniconfigs.crd.k8s.amazonaws.com 2019-03-07T20:06:48Z  If you don\u0026rsquo;t have ENIConfig installed, you can install it by using this command\nkubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.3/aws-k8s-cni.yaml  Create custom resources for each subnet by replacing Subnet and SecurityGroup IDs. Since we created three secondary subnets, we need create three custom resources.\nHere is the template for custom resource. Notice the values for Subnet ID and SecurityGroup ID needs to be replaced with appropriate values\napiVersion: crd.k8s.amazonaws.com/v1alpha1 kind: ENIConfig metadata: name: group1-pod-netconfig spec: subnet: $SUBNETID1 securityGroups: - $SECURITYGROUPID1 - $SECURITYGROUPID2  Check the AZ\u0026rsquo;s and Subnet IDs for these subnets. Make note of AZ info as you will need this when you apply annotation to Worker nodes using custom network config\naws ec2 describe-subnets --filters \u0026quot;Name=cidr-block,Values=100.64.*\u0026quot; --query 'Subnets[*].[CidrBlock,SubnetId,AvailabilityZone]' --output table  -------------------------------------------------------------- | DescribeSubnets | +-----------------+----------------------------+-------------+ | 100.64.32.0/19 | subnet-07dab05836e4abe91 | us-east-2a | | 100.64.64.0/19 | subnet-0692cd08cc4df9b6a | us-east-2c | | 100.64.0.0/19 | subnet-04f960ffc8be6865c | us-east-2b | +-----------------+----------------------------+-------------+  Check your Worker Node SecurityGroup\nINSTANCE_IDS=(`aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId' --filters \u0026quot;Name=tag:Name,Values=eksworkshop*\u0026quot; --output text`) for i in \u0026quot;${INSTANCE_IDS[@]}\u0026quot; do echo \u0026quot;SecurityGroup for EC2 instance $i ...\u0026quot; aws ec2 describe-instances --instance-ids $INSTANCE_IDS | jq -r '.Reservations[].Instances[].SecurityGroups[].GroupId' done  SecurityGroup for EC2 instance i-03ea1a083c924cd78 ... sg-070d03008bda531ad sg-06e5cab8e5d6f16ef SecurityGroup for EC2 instance i-0a635aed890c7cc3e ... sg-070d03008bda531ad sg-06e5cab8e5d6f16ef SecurityGroup for EC2 instance i-048e5ec8815e5ea8a ... sg-070d03008bda531ad sg-06e5cab8e5d6f16ef  Create custom resource group1-pod-netconfig.yaml for first subnet (100.64.0.0/19). Replace the SubnetId and SecuritGroupIds with the values from above. Here is how it looks with the configuration values for my environment\nNote: We are using same SecurityGroup for pods as your Worker Nodes but you can change these and use custom SecurityGroups for your Pod Networking\napiVersion: crd.k8s.amazonaws.com/v1alpha1 kind: ENIConfig metadata: name: group1-pod-netconfig spec: subnet: subnet-04f960ffc8be6865c securityGroups: - sg-070d03008bda531ad - sg-06e5cab8e5d6f16ef  Create custom resource group2-pod-netconfig.yaml for second subnet (100.64.32.0/19). Replace the SubnetId and SecuritGroupIds as above.\nSimilarly, create custom resource group3-pod-netconfig.yaml for third subnet (100.64.64.0/19). Replace the SubnetId and SecuritGroupIds as above.\nCheck the instance details using this command as you will need AZ info when you apply annotation to Worker nodes using custom network config\naws ec2 describe-instances --filters \u0026quot;Name=tag:Name,Values=eksworkshop*\u0026quot; --query 'Reservations[*].Instances[*].[PrivateDnsName,Tags[?Key==`Name`].Value|[0],Placement.AvailabilityZone,PrivateIpAddress,PublicIpAddress]' --output table  ------------------------------------------------------------------------------------------------------------------------------------------ | DescribeInstances | +-----------------------------------------------+---------------------------------------+-------------+-----------------+----------------+ | ip-192-168-9-228.us-east-2.compute.internal | eksworkshop-eksctl-yourusername-ng-475d4bc8-Node | us-east-2c | 192.168.9.228 | 18.191.57.131 | | ip-192-168-71-211.us-east-2.compute.internal | eksworkshop-eksctl-yourusername-ng-475d4bc8-Node | us-east-2a | 192.168.71.211 | 18.221.77.249 | | ip-192-168-33-135.us-east-2.compute.internal | eksworkshop-eksctl-yourusername-ng-475d4bc8-Node | us-east-2b | 192.168.33.135 | 13.59.167.90 | +-----------------------------------------------+---------------------------------------+-------------+-----------------+----------------+  Apply the CRD\u0026rsquo;s\nkubectl apply -f group1-pod-netconfig.yaml kubectl apply -f group2-pod-netconfig.yaml kubectl apply -f group3-pod-netconfig.yaml  As last step, we will annotate nodes with custom network configs.\nBe sure to annotate the instance with config that matches correct AZ. For ex, in my environment instance ip-192-168-33-135.us-east-2.compute.internal is in us-east-2b. So, I will apply group1-pod-netconfig.yaml to this instance. Similarly, I will apply group2-pod-netconfig.yaml to ip-192-168-71-211.us-east-2.compute.internal and group3-pod-netconfig.yaml to ip-192-168-9-228.us-east-2.compute.internal\n kubectl annotate node \u0026lt;nodename\u0026gt;.\u0026lt;region\u0026gt;.compute.internal k8s.amazonaws.com/eniConfig=group1-pod-netconfig  As an example, here is what I would run in my environment\nkubectl annotate node ip-192-168-33-135.us-east-2.compute.internal k8s.amazonaws.com/eniConfig=group1-pod-netconfig  You should now see secondary IP address from extended CIDR assigned to annotated nodes.\n"
},
{
	"uri": "/batch/deploy/",
	"title": "Deploy Argo",
	"tags": [],
	"description": "",
	"content": " Deploy Argo Argo run in its own namespace and deploys as a CustomResourceDefinition.\nDeploy the Controller and UI.\nkubectl create ns argo kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo/v2.2.1/manifests/install.yaml  namespace/argo created customresourcedefinition.apiextensions.k8s.io/workflows.argoproj.io created serviceaccount/argo created serviceaccount/argo-ui created clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-admin created clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-edit created clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-view created clusterrole.rbac.authorization.k8s.io/argo-cluster-role created clusterrole.rbac.authorization.k8s.io/argo-ui-cluster-role created clusterrolebinding.rbac.authorization.k8s.io/argo-binding created clusterrolebinding.rbac.authorization.k8s.io/argo-ui-binding created configmap/workflow-controller-configmap created service/argo-ui created deployment.apps/argo-ui created deployment.apps/workflow-controller created  To use advanced features of Argo for this demo, create a RoleBinding to grant admin privileges to the \u0026lsquo;default\u0026rsquo; service account.\nThis is for demo purposes only. In any other environment, you should use Workflow RBAC to set appropriate permissions.\n kubectl create rolebinding default-admin --clusterrole=admin --serviceaccount=default:default  "
},
{
	"uri": "/servicemesh_with_istio/deploy/",
	"title": "Deploy Sample Apps",
	"tags": [],
	"description": "",
	"content": " Now that we have all the resources installed for Istio, we will use sample application called BookInfo to review key capabilities of the service mesh such as intelligent routing, and review telemetry data using Prometheus \u0026amp; Grafana.\nSample Apps The Bookinfo application is broken into four separate microservices:\n productpage\n The productpage microservice calls the details and reviews microservices to populate the page.  details\n The details microservice contains book information.  reviews\n The reviews microservice contains book reviews. It also calls the ratings microservice.  ratings\n The ratings microservice contains book ranking information that accompanies a book review.   There are 3 versions of the reviews microservice:\n Version v1\n doesn’t call the ratings service.  Version v2\n calls the ratings service, and displays each rating as 1 to 5 black stars.  Version v3\n calls the ratings service, and displays each rating as 1 to 5 red stars.   Deploy Sample Apps Deploy sample apps by manually injecting istio proxy and confirm pods, services are running correctly\nkubectl apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml)  The output from\nkubectl get pod,svc  Should look similar to:\nNAME READY STATUS RESTARTS AGE details-v1-64558cf56b-dxbx2 2/2 Running 0 14s productpage-v1-5b796957dd-hqllk 2/2 Running 0 14s ratings-v1-777b98fcc4-5bfr8 2/2 Running 0 14s reviews-v1-866dcb7ff-k69jm 2/2 Running 0 14s reviews-v2-6d7959c9d-5ppnc 2/2 Running 0 14s reviews-v3-7ddf94f545-m7vls 2/2 Running 0 14s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.100.102.153 \u0026lt;none\u0026gt; 9080/TCP 17s kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 138d productpage ClusterIP 10.100.222.154 \u0026lt;none\u0026gt; 9080/TCP 17s ratings ClusterIP 10.100.1.63 \u0026lt;none\u0026gt; 9080/TCP 17s reviews ClusterIP 10.100.255.157 \u0026lt;none\u0026gt; 9080/TCP 17s  Next we\u0026rsquo;ll define the virtual service and ingress gateway:\nkubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml  Next, we\u0026rsquo;ll query the DNS name of the ingress gateway and use it to connect via the browser.\nkubectl get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' -n istio-system ; echo  This may take a minute or two, first for the Ingress to be created, and secondly for the Ingress to hook up with the services it exposes.\nTo test, do the following:\n Open a new browser tab Paste the DNS endpoint returned from the previous get service istiogateway command Add /productpage to the end of that DNS endpoint Hit enter to retrieve the page.  Remember to add /productpage to the end of the URI in the browser to see the sample webpage!\n Click reload multiple times to see how the layout and content of the reviews changes as differnt versions (v1, v2, v3) of the app are called.\n"
},
{
	"uri": "/irsa/iam-role-for-sa-2/",
	"title": "Specifying an IAM Role for Service Account",
	"tags": [],
	"description": "",
	"content": " Specifying an IAM Role for your Service Account In the previous step, we created the IAM role that associated with a service account named iam-test in the cluster and this has already been done for you with the service account you specified when creating the role.\n Be sure your service account iam-test is exist  kubectl get sa     NAME SECRETS AGE     default 1 85m   iam-test 1 44m     Make sure your service account with the ARN of the IAM role is annotated  kubectl describe sa iam-test   Name: iam-test\nNamespace: default\nLabels: \nAnnotations: eks.amazonaws.com/role-arn: arn:aws:iam::14xxxxxxxx84:role/eksctl-eksworkshop-eksctl-yourusername-addon-iamserviceac-Role1-1PJ5Q3H39Z5M9\nImage pull secrets: \nMountable secrets: iam-test-token-5n9cb\nTokens: iam-test-token-5n9cb\nEvents: \n "
},
{
	"uri": "/servicemesh_with_appmesh/cleanup/",
	"title": "App Mesh Cleanup",
	"tags": [],
	"description": "",
	"content": "When you\u0026rsquo;re done experimenting and want to delete all resources created during this tutorial, you can uninstall the components by running \u0026lsquo;kubectl delete \u0026hellip;\u0026rsquo; , along with de-install scripts for the injector.\nFor ease of use, these commands have all been packaged into the cleanup script, which can be run via:\n./cleanup.sh  The above script will not delete any nodes in your k8s cluster.\n"
},
{
	"uri": "/introduction/basics/concepts_nodes/",
	"title": "Kubernetes Nodes",
	"tags": [],
	"description": "",
	"content": "The machines that make up a Kubernetes cluster are called nodes.\nNodes in a Kubernetes cluster may be physical, or virtual.\nThere are two types of nodes:\n A Master-node type, which makes up the Control Plane, acts as the “brains” of the cluster.\n A Worker-node type, which makes up the Data Plane, runs the actual container images (via pods).\n  We’ll dive deeper into how nodes interact with each other later in the presentation.\n"
},
{
	"uri": "/intro_to_rbac/test_rbac_user_without_roles/",
	"title": "Test the new user",
	"tags": [],
	"description": "",
	"content": "Up until now, as the cluster operator, you\u0026rsquo;ve been accessing the cluster as the admin user. Let\u0026rsquo;s now see what happens when we access the cluster as the newly created rbac-user.\nIssue the following command to source the rbac-user\u0026rsquo;s AWS IAM user environmental variables:\n. rbacuser_creds.sh  By running the above command, you\u0026rsquo;ve now set AWS environmental variables which should override the default admin user or role. To verify we\u0026rsquo;ve overrode the default user settings, run the following command:\naws sts get-caller-identity  You should see something similar to below, where we\u0026rsquo;re now making API calls as rbac-user:\n { \u0026quot;Account\u0026quot;: \u0026lt;AWS Account ID\u0026gt;, \u0026quot;UserId\u0026quot;: \u0026lt;AWS User ID\u0026gt;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:iam::\u0026lt;AWS Account ID\u0026gt;:user/rbac-user\u0026quot; }  Now that we\u0026rsquo;re making calls in the context of the rbac-user, lets quickly make a request to get all pods:\nkubectl get pods -n rbac-test  You should get a response back similar to:\nNo resources found. Error from server (Forbidden): pods is forbidden: User \u0026quot;rbac-user\u0026quot; cannot list resource \u0026quot;pods\u0026quot; in API group \u0026quot;\u0026quot; in the namespace \u0026quot;rbac-test\u0026quot;  We already created the rbac-user, so why did we get that error?\nJust creating the user doesn\u0026rsquo;t give that user access to any resources in the cluster. In order to achieve that, we\u0026rsquo;ll need to define a role, and then bind the user to that role. We\u0026rsquo;ll do that next.\n"
},
{
	"uri": "/deploy/viewservices/",
	"title": "Find the Service Address",
	"tags": [],
	"description": "",
	"content": "Now that we have a running service that is type: LoadBalancer we need to find the ELB\u0026rsquo;s address. We can do this by using the get services operation of kubectl:\nkubectl get service ecsdemo-frontend  Notice the field isn\u0026rsquo;t wide enough to show the FQDN of the ELB. We can adjust the output format with this command:\nkubectl get service ecsdemo-frontend -o wide  If we wanted to use the data programatically, we can also output via json. This is an example of how we might be able to make use of json output:\nELB=$(kubectl get service ecsdemo-frontend -o json | jq -r '.status.loadBalancer.ingress[].hostname') curl -m3 -v $ELB  It will take several minutes for the ELB to become healthy and start passing traffic to the frontend pods.\n You should also be able to copy/paste the loadBalancer hostname into your browser and see the application running. Keep this tab open while we scale the services up on the next page.\n"
},
{
	"uri": "/statefulset/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " First delete the StatefulSet. This will also terminates the pods. It may take some while.\nkubectl delete statefulset mysql  Verify there are no pods running by following command.\nkubectl get pods -l app=mysql  No resources found.  Delete ConfigMap, Service and PVC by following command.\nkubectl delete configmap,service,pvc -l app=mysql  configmap \u0026quot;mysql-config\u0026quot; deleted service \u0026quot;mysql\u0026quot; deleted service \u0026quot;mysql-read\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-0\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-1\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-2\u0026quot; deleted  Congratulation! You\u0026rsquo;ve finished the StatefulSets lab. "
},
{
	"uri": "/logging/configurecwl/",
	"title": "Configure CloudWatch Logs and Kibana",
	"tags": [],
	"description": "",
	"content": " Configure CloudWatch Logs Subscription CloudWatch Logs can be delivered to other services such as Amazon Elasticsearch for custom processing. This can be achieved by subscribing to a real-time feed of log events. A subscription filter defines the filter pattern to use for filtering which log events gets delivered to Elasticsearch, as well as information about where to send matching log events to.\nIn this section, we’ll subscribe to the CloudWatch log events from the fluent-cloudwatch stream from the eks/eksworkshop-eksctl-yourusername log group. This feed will be streamed to the Elasticsearch cluster.\nOriginal instructions for this are available at:\nhttp://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/iam_policy/lambda.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;lambda.amazonaws.com\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] } EoF aws iam create-role --role-name lambda_basic_execution --assume-role-policy-document file://~/environment/iam_policy/lambda.json aws iam attach-role-policy --role-name lambda_basic_execution --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole  Go to the CloudWatch Logs console\nSelect the log group /eks/eksworkshop-eksctl-yourusername/containers. Click on Actions and select Stream to Amazon ElasticSearch Service. Select the ElasticSearch Cluster kubernetes-logs and IAM role lambda_basic_execution\nClick Next\nSelect Common Log Format and click Next\nReview the configuration. Click Next and then Start Streaming\nCloudwatch page is refreshed to show that the filter was successfully created\nConfigure Kibana In Amazon Elasticsearch console, select the kubernetes-logs under My domains\nOpen the Kibana dashboard from the link. After a few minutes, records will begin to be indexed by ElasticSearch. You\u0026rsquo;ll need to configure an index patterns in Kibana.\nSet Index Pattern as cwl-* and click Next\nSelect @timestamp from the dropdown list and select Create index pattern\nClick on Discover and explore your logs\n"
},
{
	"uri": "/servicemesh_with_appmesh/port_to_app_mesh/create_the_mesh/",
	"title": "Create the Mesh",
	"tags": [],
	"description": "",
	"content": "The mesh component serves as the App Mesh foundation, and must be created first. We\u0026rsquo;ll call our mesh dj-app, and define it in the prod namespace by executing the following command from the repository\u0026rsquo;s base directory:\nkubectl create -f 4_create_initial_mesh_components/mesh.yaml  You should see output similar to:\nmesh.appmesh.k8s.aws/dj-app created  Since an App Mesh mesh is a custom resource, we can also use kubectl to view it via the get command. Running the below command:\nkubectl get meshes -nprod  yields:\nNAME AGE dj-app 1h  And similarly, as is the case for any of the custom resources we\u0026rsquo;ll be interacting with in this tutorial, you can also view AWS App Mesh resources via the AWS CLI to list meshes:\naws appmesh list-meshes  which would output:\n{ \u0026quot;meshes\u0026quot;: [ { \u0026quot;meshName\u0026quot;: \u0026quot;dj-app\u0026quot;, \u0026quot;arn\u0026quot;: \u0026quot;arn:aws:appmesh:us-west-2:123586676:mesh/dj-app\u0026quot; } ] }  or for example, to describe a mesh:\naws appmesh describe-mesh --mesh-name dj-app  would output:\n{ \u0026quot;mesh\u0026quot;: { \u0026quot;status\u0026quot;: { \u0026quot;status\u0026quot;: \u0026quot;ACTIVE\u0026quot; }, \u0026quot;meshName\u0026quot;: \u0026quot;dj-app\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;version\u0026quot;: 1, \u0026quot;lastUpdatedAt\u0026quot;: 1553233281.819, \u0026quot;createdAt\u0026quot;: 1553233281.819, \u0026quot;arn\u0026quot;: \u0026quot;arn:aws:appmesh:us-west-2:123586676:mesh/dj-app\u0026quot;, \u0026quot;uid\u0026quot;: \u0026quot;10d86ae0-ece7-4b1d-bc2d-08064d9b55e1\u0026quot; } } }  "
},
{
	"uri": "/servicemesh_with_appmesh/create_app_mesh_components/create_injector_controller/",
	"title": "Creating the Injector Controller",
	"tags": [],
	"description": "",
	"content": "To create the injector controller, we\u0026rsquo;ll run a script that creates a namespace, generates certificates, and then installs the injector deployment.\nFrom the base repository directory, change into the injector directory:\ncd 2_create_injector  Next, run the create.sh script:\n./create.sh  Output should look similar to:\nnamespace/appmesh-inject created creating certs in tmpdir /var/folders/02/qfw6pbm501xbw4scnk20w80h0_xvht/T/tmp.LFO95khQ Generating RSA private key, 2048 bit long modulus .........+++ ..............................+++ e is 65537 (0x10001) certificatesigningrequest.certificates.k8s.io/aws-app-mesh-inject.appmesh-inject created NAME AGE REQUESTOR CONDITION aws-app-mesh-inject.appmesh-inject 0s kubernetes-admin Pending certificatesigningrequest.certificates.k8s.io/aws-app-mesh-inject.appmesh-inject approved secret/aws-app-mesh-inject created processing templates Created injector manifest at:/2_create_injector/inject.yaml serviceaccount/aws-app-mesh-inject-sa created clusterrole.rbac.authorization.k8s.io/aws-app-mesh-inject-cr unchanged clusterrolebinding.rbac.authorization.k8s.io/aws-app-mesh-inject-binding configured service/aws-app-mesh-inject created deployment.apps/aws-app-mesh-inject created mutatingwebhookconfiguration.admissionregistration.k8s.io/aws-app-mesh-inject unchanged Waiting for pods to come up... App Inject Pods and Services After Install: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE aws-app-mesh-inject ClusterIP 10.100.165.254 \u0026lt;none\u0026gt; 443/TCP 16s NAME READY STATUS RESTARTS AGE aws-app-mesh-inject-5d84d8c96f-gc6bl 1/1 Running 0 16s  If you\u0026rsquo;re seeing the above output, the injector controller has been installed correctly.\n"
},
{
	"uri": "/scaling/test_ca/",
	"title": "Scale a Cluster with CA",
	"tags": [],
	"description": "",
	"content": " Deploy a Sample App We will deploy an sample nginx application as a ReplicaSet of 1 Pod\ncat \u0026lt;\u0026lt;EoF\u0026gt; ~/environment/cluster-autoscaler/nginx.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-to-scaleout spec: replicas: 1 template: metadata: labels: service: nginx app: nginx spec: containers: - image: nginx name: nginx-to-scaleout resources: limits: cpu: 500m memory: 512Mi requests: cpu: 500m memory: 512Mi EoF kubectl apply -f ~/environment/cluster-autoscaler/nginx.yaml kubectl get deployment/nginx-to-scaleout  Scale our ReplicaSet OK, let\u0026rsquo;s scale out the replicaset to 10\nkubectl scale --replicas=10 deployment/nginx-to-scaleout  Some pods will be in the Pending state, which triggers the cluster-autoscaler to scale out the EC2 fleet.\nkubectl get pods -o wide --watch  NAME READY STATUS RESTARTS AGE nginx-to-scaleout-7cb554c7d5-2d4gp 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-2nh69 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-45mqz 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-4qvzl 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-5jddd 1/1 Running 0 34s nginx-to-scaleout-7cb554c7d5-5sx4h 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-5xbjp 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-6l84p 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-7vp7l 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-86pr6 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-88ttw 0/1 Pending 0 12s  View the cluster-autoscaler logs\nkubectl logs -f deployment/cluster-autoscaler -n kube-system  You will notice Cluster Autoscaler events similar to below Check the AWS Management Console to confirm that the Auto Scaling groups are scaling up to meet demand. This may take a few minutes. You can also follow along with the pod deployment from the command line. You should see the pods transition from pending to running as nodes are scaled up.\n"
},
{
	"uri": "/servicemesh_with_appmesh/create_the_k8s_app/test_worker_perms/",
	"title": "Test Permissions",
	"tags": [],
	"description": "",
	"content": "To test that your worker nodes are able to use these permissions correctly, we\u0026rsquo;ll run a job that attempts to list all existing meshes.\nRun this command to set the script to run against the correct region:\nsed -i'.old' -e 's/\\\u0026quot;us-west-2\\\u0026quot;/\\\u0026quot;'$AWS_REGION'\\\u0026quot;/' awscli.yaml  Next, execute the job:\nkubectl apply -f awscli.yaml  Make sure its completed by issuing the command:\nkubectl get jobs  And see that desired and successful are both one:\nNAME DESIRED SUCCESSFUL AGE awscli 1 1 1m  Inspect the output of the job:\nkubectl logs jobs/awscli  The output of this command will illustrate if your nodes can make App Mesh API calls successfully as well.\nThis output shows the workers have proper access:\n{ \u0026quot;meshes\u0026quot;: [] }  And this output shows they don\u0026rsquo;t:\nAn error occurred (AccessDeniedException) when calling the ListMeshes operation: User: arn:aws:iam::123abc:user/foo is not authorized to perform: appmesh:ListMeshes on resource: *  If you need to troubleshoot further, in order to run the job again to test, you must first delete it:\nkubectl delete jobs/awscli  Once you\u0026rsquo;ve successfully tested for the proper permissions, continue on to the next step.\n"
},
{
	"uri": "/helm_root/helm_micro/service/",
	"title": "Test the Service",
	"tags": [],
	"description": "",
	"content": "To test the service our eksdemo Chart created, we\u0026rsquo;ll need to get the name of the ELB endpoint that was generated when we deployed the Chart:\nkubectl get svc ecsdemo-frontend -o jsonpath=\u0026quot;{.status.loadBalancer.ingress[*].hostname}\u0026quot;; echo  Copy that address, and paste it into a new tab in your browser. You should see something similar to:\n"
},
{
	"uri": "/healthchecks/",
	"title": "Health Checks",
	"tags": [],
	"description": "",
	"content": " Health Checks By default, Kubernetes will restart a container if it crashes for any reason. It uses Liveness and Readiness probes which can be configured for running a robust application by identifying the healthy containers to send traffic to and restarting the ones when required.\nIn this section, we will understand how liveness and readiness probes are defined and test the same against different states of a pod. Below is the high level description of how these probes work.\nLiveness probes are used in Kubernetes to know when a pod is alive or dead. A pod can be in a dead state for a variety of reasons; Kubernetes will kill and recreate the pod when a liveness probe does not pass.\nReadiness probes are used in Kubernetes to know when a pod is ready to serve traffic. Only when the readiness probe passes will a pod receive traffic from the service; if a readiness probe fails traffic will not be sent to the pod.\nWe will review some examples in this module to understand different options for configuring liveness and readiness probes.\n"
},
{
	"uri": "/scaling/",
	"title": "Autoscaling our Applications and Clusters",
	"tags": [],
	"description": "",
	"content": " Implement AutoScaling with HPA and CA In this Chapter, we will show patterns for scaling your worker nodes and applications deployments automatically. Automatic scaling in K8s comes in two forms:\n Horizontal Pod Autoscaler (HPA) scales the pods in a deployment or replica set. It is implemented as a K8s API resource and a controller. The controller manager queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. It obtains the metrics from either the resource metrics API (for per-pod resource metrics), or the custom metrics API (for all other metrics).\n Cluster Autoscaler (CA) is the default K8s component that can be used to perform pod scaling as well as scaling nodes in a cluster. It automatically increases the size of an Auto Scaling group so that pods have a place to run. And it attempts to remove idle nodes, that is, nodes with no running pods.\n  "
},
{
	"uri": "/codepipeline/",
	"title": "CI/CD with CodePipeline",
	"tags": [],
	"description": "",
	"content": " CI/CD with CodePipeline This chapter does not work in the AWS supplied event environments yet. If you are at an AWS event, please skip this chapter. If you are working in your own account, you should have no issues.\n Continuous integration (CI) and continuous delivery (CD) are essential for deft organizations. Teams are more productive when they can make discrete changes frequently, release those changes programmatically and deliver updates without disruption.\nIn this module, we will build a CI/CD pipeline using AWS CodePipeline. The CI/CD pipeline will deploy a sample Kubernetes service, we will make a change to the GitHub repository and observe the automated delivery of this change to the cluster.\n"
},
{
	"uri": "/spotworkers/",
	"title": "Using Spot Instances with EKS",
	"tags": [],
	"description": "",
	"content": " Using Spot Instances with EKS In this module, you will learn how to provision, manage, and maintain your Kubernetes clusters with Amazon EKS at any scale on Spot Instances to optimize cost and scale.\n"
},
{
	"uri": "/jenkins/",
	"title": "Deploying Jenkins",
	"tags": [],
	"description": "",
	"content": " Deploy Jenkins In this Chapter, we will deploy Jenkins using the helm package manager we installed in the last module.\n"
},
{
	"uri": "/x-ray/",
	"title": "Tracing with X-Ray",
	"tags": [],
	"description": "",
	"content": " Tracing with X-Ray As distributed systems evolve, monitoring and debugging services becomes challenging. Container-orchestration platforms like Kubernetes solve a lot of problems, but they also introduce new challenges for developers and operators in understanding how services interact and where latency exists. AWS X-Ray helps developers analyze and debug distributed services.\nIn this module, we are going to deploy the X-Ray agent as a DaemonSet, deploy sample front-end and back-end services that are instrumented with the X-Ray SDK, make some sample requests and then examine the traces and service maps in the AWS Management Console.\n"
},
{
	"uri": "/batch/",
	"title": "Batch Processing with Argo",
	"tags": [],
	"description": "",
	"content": " Batch Processing In this Chapter, we will deploy common batch processing scenarios using Kubernetes and Argo.\nWhat is Argo? Argo is an open source container-native workflow engine for getting work done on Kubernetes. Argo is implemented as a Kubernetes CRD (Custom Resource Definition).\n Define workflows where each step in the workflow is a container. Model multi-step workflows as a sequence of tasks or capture the dependencies between tasks using a graph (DAG). Easily run compute intensive jobs for machine learning or data processing in a fraction of the time using Argo workflows on Kubernetes.  "
},
{
	"uri": "/irsa/",
	"title": "IAM Roles for Service Accounts",
	"tags": [],
	"description": "",
	"content": " Fine-Grained IAM Roles for Service Accounts In Kubernetes version 1.12, support was added for a new ProjectedServiceAccountToken feature, which is an OIDC JSON web token that also contains the service account identity, and supports a configurable audience.\nAmazon EKS now hosts a public OIDC discovery endpoint per cluster containing the signing keys for the ProjectedServiceAccountToken JSON web tokens so external systems, like IAM, can validate and accept the Kubernetes-issued OIDC tokens.\nOIDC federation access allows you to assume IAM roles via the Secure Token Service (STS), enabling authentication with an OIDC provider, receiving a JSON Web Token (JWT), which in turn can be used to assume an IAM role. Kubernetes, on the other hand, can issue so-called projected service account tokens, which happen to be valid OIDC JWTs for pods. Our setup equips each pod with a cryptographically-signed token that can be verified by STS against the OIDC provider of your choice to establish the pod’s identity.\u000bnew credential provider ”sts:AssumeRoleWithWebIdentity”\n"
},
{
	"uri": "/weave_flux/",
	"title": "GitOps with Weave Flux",
	"tags": [],
	"description": "",
	"content": " GitOps with Weave Flux GitOps, a term coined by Weaveworks, is a way to do continuous delivery. Git is used as single source of truth for deploying into your cluster. This is easy for a development team as they are already familiar with git and do not need to know other tools. Weave Flux is a tool that runs in your Kubernetes cluster and implements changes based on monitoring Git and image repositories.\nIn this module, we will create a Docker image build pipeline using AWS CodePipeline for a sample application in a GitHub repository. We will then commit Kubernetes manifests to GitHub and monitor Weave Flux managing the deployment.\nBelow is a diagram of what will be created:\n"
},
{
	"uri": "/kubeflow/",
	"title": "Machine Learning using Kubeflow",
	"tags": [],
	"description": "",
	"content": " Machine Learning using Kubeflow Kubeflow provides a simple, portable, and scalable way of running Machine Learning workloads on Kubernetes.\nIn this module, we will install Kubeflow on Amazon EKS, run a single-node training and inference using TensorFlow, and run distributed node training using Horovod.\n"
},
{
	"uri": "/kubeflow/inference/",
	"title": "Model inference",
	"tags": [],
	"description": "",
	"content": " Model Inference After the model is trained and stored in S3 bucket, the next step is to use that model for inference.\nThis chapter explains how to use the previously trained model and run inference using TensorFlow and Keras on Amazon EKS.\nRun inference pod A model from training was stored in the S3 bucket in previous section. Make sure S3_BUCKET and AWS_REGION environment variables are set correctly.\ncurl -LO https://eksworkshop.com/kubeflow/kubeflow.files/mnist-inference.yaml envsubst \u0026lt;mnist-inference.yaml | kubectl apply -f -  Wait for the containers to start:\nkubectl get pods -l app=mnist,type=inference NAME READY STATUS RESTARTS AGE mnist-96fb6f577-k8pm6 1/1 Running 0 116s  Port forward inference endpoint for local testing:\nkubectl port-forward `kubectl get pods -l=app=mnist,type=inference -o jsonpath='{.items[0].metadata.name}' --field-selector=status.phase=Running` 8500:8500 \u0026amp;  Run inference Use the script inference_client.py to make prediction request. It will randomly pick one image from test dataset and make prediction.\ncurl -LO https://eksworkshop.com/kubeflow/kubeflow.files/inference_client.py $ python inference_client.py --endpoint http://localhost:8500/v1/models/mnist:predict Data: {\u0026quot;instances\u0026quot;: [[[[0.0], [0.0], [0.0], [0.0], [0.0] ... 0.0], [0.0]]]], \u0026quot;signature_name\u0026quot;: \u0026quot;serving_default\u0026quot;} The model thought this was a Ankle boot (class 9), and it was actually a Ankle boot (class 9)  "
},
{
	"uri": "/advanced-networking/secondary_cidr/test_networking/",
	"title": "Test Networking",
	"tags": [],
	"description": "",
	"content": " Launch pods into Secondary CIDR network Let\u0026rsquo;s launch few pods and test networking\nkubectl run nginx --image=nginx kubectl scale --replicas=3 deployments/nginx kubectl expose deployment/nginx --type=NodePort --port 80 kubectl get pods -o wide  NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE nginx-64f497f8fd-k962k 1/1 Running 0 40m 100.64.6.147 ip-192-168-52-113.us-east-2.compute.internal \u0026lt;none\u0026gt; nginx-64f497f8fd-lkslh 1/1 Running 0 40m 100.64.53.10 ip-192-168-74-125.us-east-2.compute.internal \u0026lt;none\u0026gt; nginx-64f497f8fd-sgz6f 1/1 Running 0 40m 100.64.80.186 ip-192-168-26-65.us-east-2.compute.internal \u0026lt;none\u0026gt;  You can use busybox pod and ping pods within same host or across hosts using IP address\nkubectl run -i --rm --tty debug --image=busybox -- sh  Test access to internet and to nginx service\n# connect to internet / # wget google.com -O - Connecting to google.com (172.217.5.238:80) Connecting to www.google.com (172.217.5.228:80) \u0026lt;!doctype html\u0026gt;\u0026lt;html itemscope=\u0026quot;\u0026quot; itemtype=\u0026quot;http://schema.org/WebPage\u0026quot; lang=\u0026quot;en\u0026quot;\u0026gt;\u0026lt;head\u0026gt;\u0026lt;meta content=\u0026quot;Search the world's information, including webpages, images, videos and more. Google has many special ... # connect to service (testing core-dns) / # wget nginx -O - Connecting to nginx (10.100.170.156:80) \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; ...  "
},
{
	"uri": "/batch/artifact/",
	"title": "Configure Artifact Repository",
	"tags": [],
	"description": "",
	"content": " Configure Artifact Repository Argo uses an artifact repository to pass data between jobs in a workflow, known as artifacts. Amazon S3 can be used as an artifact repository.\nLet\u0026rsquo;s create a S3 bucket using the AWS CLI.\naws s3 mb s3://batch-artifact-repository-${ACCOUNT_ID}/  Next, edit the workflow-controller ConfigMap to use the S3 bucket.\nkubectl edit -n argo configmap/workflow-controller-configmap  Add the following lines to the end of the ConfigMap, substituting your Account ID for {{ACCOUNT_ID}}:\ndata: config: | artifactRepository: s3: bucket: batch-artifact-repository-{{ACCOUNT_ID}} endpoint: s3.amazonaws.com  Create an IAM Policy In order for Argo to read from/write to the S3 bucket, we need to configure an inline policy and add it to the EC2 instance profile of the worker nodes.\nFirst, we will need to ensure the Role Name our workers use is set in our environment:\ntest -n \u0026quot;$ROLE_NAME\u0026quot; \u0026amp;\u0026amp; echo ROLE_NAME is \u0026quot;$ROLE_NAME\u0026quot; || echo ROLE_NAME is not set  If you receive an error or empty response, expand the steps below to export.\n  Expand here if you need to export the Role Name   If ROLE_NAME is not set, please review: /eksctl/test/\n  # Example Output ROLE_NAME is eks-workshop-nodegroup  Create and policy and attach to the worker node role.\nmkdir ~/environment/batch_policy cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/batch_policy/k8s-s3-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:*\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:s3:::batch-artifact-repository-${ACCOUNT_ID}\u0026quot;, \u0026quot;arn:aws:s3:::batch-artifact-repository-${ACCOUNT_ID}/*\u0026quot; ] } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name S3-Policy-For-Worker --policy-document file://~/environment/batch_policy/k8s-s3-policy.json  Validate that the policy is attached to the role\naws iam get-role-policy --role-name $ROLE_NAME --policy-name S3-Policy-For-Worker  "
},
{
	"uri": "/servicemesh_with_istio/routing/",
	"title": "Intelligent Routing",
	"tags": [],
	"description": "",
	"content": " Intelligent Routing Deploying a microservice-based application in an Istio service mesh allows one to externally control service monitoring and tracing, request (version) routing, resiliency testing, security and policy enforcement, and more in a consistent manner across the services, and the application.\nBefore you can use Istio to control the Bookinfo version routing, you\u0026rsquo;ll need to define the available versions, called subsets, in destination rules.\nService versions (a.k.a. subsets) - In a continuous deployment scenario, for a given service, there can be distinct subsets of instances running different variants of the application binary. These variants are not necessarily different API versions. They could be iterative changes to the same service, deployed in different environments (prod, staging, dev, etc.). Common scenarios where this occurs include A/B testing, canary rollouts, etc. The choice of a particular version can be decided based on various criterion (headers, url, etc.) and/or by weights assigned to each version. Each service has a default version consisting of all its instances.\n kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml kubectl get destinationrules -o yaml  To route to one version only, you apply virtual services that set the default version for the microservices. In this case, the virtual services will route all traffic to reviews:v1 of microservice.\nkubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml kubectl get virtualservices reviews -o yaml  The subset is set to v1 for all reviews request.\nspec: hosts: - reviews http: - route: - destination: host: reviews subset: v1  Try now to reload the page multiple times, and note how only version 1 of reviews is displayed each time.\nNext, we\u0026rsquo;ll change the route configuration so that all traffic from a specific user is routed to a specific service version. In this case, all traffic from a user named Jason will be routed to the service reviews:v2.\nkubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml kubectl get virtualservices reviews -o yaml  The subset is set to v1 in default and route v2 if the logged user is match with \u0026lsquo;jason\u0026rsquo; for reviews request.\nspec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v1  To test, click Sign in from the top right corner of the page, and login using jason as user name with a blank password. You will only see reviews:v2 all the time. Others will see reviews:v1.\nTo test for resiliency, inject a 7s delay between the reviews:v2 and ratings microservices for user jason. This test will uncover a bug that was intentionally introduced into the Bookinfo app.\nkubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-delay.yaml kubectl get virtualservice ratings -o yaml  The subset is set to v1 in default and added 7s delay for all the request if the logged user is match with \u0026lsquo;jason\u0026rsquo; for ratings.\nspec: hosts: - ratings http: - fault: delay: fixedDelay: 7s percent: 100 match: - headers: end-user: exact: jason route: - destination: host: ratings subset: v1 - route: - destination: host: ratings subset: v1  Logout, then click Sign in from the top right corner of the page, using jason as the user name with a blank password. You will see the delays and it ends up display error for reviews. Others will see reviews without error.\nThe timeout between the productpage and the reviews service is 6 seconds - coded as 3s + 1 retry for 6s total.\nTo test for another resiliency, introduce an HTTP abort to the ratings microservices for the test user jason. The page will immediately display the “Ratings service is currently unavailable”\nkubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-abort.yaml kubectl get virtualservice ratings -o yaml  The subset is set to v1 and by default returns an error message of \u0026ldquo;Ratings service is currently unavailable\u0026rdquo; below the reviewer name if the logged username matches \u0026lsquo;jason\u0026rsquo;.\nspec: hosts: - ratings http: - fault: abort: httpStatus: 500 percent: 100 match: - headers: end-user: exact: jason route: - destination: host: ratings subset: v1 - route: - destination: host: ratings subset: v1  To test, click Sign in from the top right corner of the page and login using jason for the user name with a blank password. As jason you will see the error message. Others (not logged in as jason) will see no error message.\nNext, we\u0026rsquo;ll demonstrate how to gradually migrate traffic from one version of a microservice to another. In our example, we\u0026rsquo;ll send 50% of traffic to reviews:v1 and 50% to reviews:v3.\nkubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml kubectl get virtualservice reviews -o yaml  The subset is set to 50% of traffic to v1 and 50% of traffic to v3 for all reviews request.\nspec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 50 - destination: host: reviews subset: v3 weight: 50  To test it, refresh your browser over and over, and you\u0026rsquo;ll see only reviews:v1 and reviews:v3.\n"
},
{
	"uri": "/irsa/deploy/",
	"title": "Deploy Sample Pod",
	"tags": [],
	"description": "",
	"content": "Now that we have completed all the necessary configuration to run a Pod with IAM role. We will deploy sample Pod to the cluster, and run a test command to see whether it works correctly or not.\ncurl -LO https://eksworkshop.com/irsa/deploy.files/iam-pod.yaml kubectl apply -f iam-pod.yaml   Make sure your pod is in Running status  kubectl get pod     NAME READY STATUS RESTARTS AGE     eks-iam-test-7fb8c5ffb8-fdr6c 1\u0026frasl;1 Running 0 5m23s     Get into the Pod  kubectl exec -it \u0026lt;place Pod Name\u0026gt; /bin/bash   Manually Call sts:AssumeRoleWithWebIdentity, and you will see AccessKeyId, SecretAccessKey information if configuration is set appropriately  aws sts assume-role-with-web-identity \\ --role-arn $AWS_ROLE_ARN \\ --role-session-name mh9test \\ --web-identity-token file://$AWS_WEB_IDENTITY_TOKEN_FILE \\ --duration-seconds 1000   Run awscli to see if it retrives list of Amazon S3 buckets  aws s3 ls   Run awscli to see if it retrives list of Amazon EC2 instances which does not have privileges in the allocated IAM policy  aws ec2 describe-instances --region us-west-2  You will get this error message.\n An error occurred (UnauthorizedOperation) when calling the DescribeInstances operation: You are not authorized to perform this operation.\n "
},
{
	"uri": "/introduction/basics/concepts_objects/",
	"title": "K8s Objects Overview",
	"tags": [],
	"description": "",
	"content": "Kubernetes objects are entities that are used to represent the state of the cluster.\nAn object is a “record of intent” – once created, the cluster does its best to ensure it exists as defined. This is known as the cluster’s “desired state.”\nKubernetes is always working to make an object’s “current state” equal to the object’s “desired state.” A desired state can describe:\n What pods (containers) are running, and on which nodes IP endpoints that map to a logical group of containers How many replicas of a container are running And much more\u0026hellip;  Let’s explain these k8s objects in a bit more detail\u0026hellip;\n"
},
{
	"uri": "/intro_to_rbac/create_role_and_binding/",
	"title": "Create the Role and Binding",
	"tags": [],
	"description": "",
	"content": "As mentioned earlier, we have our new user rbac-user, but its not yet bound to any roles. In order to do that, we\u0026rsquo;ll need to switch back to our default admin user.\nRun the following to unset the environmental variables that define us as rbac-user:\nunset AWS_SECRET_ACCESS_KEY unset AWS_ACCESS_KEY_ID  To verify we\u0026rsquo;re the admin user again, and no longer rbac-user, issue the following command:\naws sts get-caller-identity  The output should show the user is no longer rbac-user:\n{ \u0026quot;Account\u0026quot;: \u0026lt;AWS Account ID\u0026gt;, \u0026quot;UserId\u0026quot;: \u0026lt;AWS User ID\u0026gt;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:iam::\u0026lt;your AWS account ID\u0026gt;:assumed-role/eksworkshop-admin-yourusername/i-123456789\u0026quot; }  Now that we\u0026rsquo;re the admin user again, we\u0026rsquo;ll create a role called pod-reader that provides list, get, and watch access for pods and deployments, but only for the rbac-test namespace. Run the following to create this role:\ncat \u0026lt;\u0026lt; EoF \u0026gt; rbacuser-role.yaml kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: rbac-test name: pod-reader rules: - apiGroups: [\u0026quot;\u0026quot;] # \u0026quot;\u0026quot; indicates the core API group resources: [\u0026quot;pods\u0026quot;] verbs: [\u0026quot;list\u0026quot;,\u0026quot;get\u0026quot;,\u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;extensions\u0026quot;,\u0026quot;apps\u0026quot;] resources: [\u0026quot;deployments\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] EoF  We have the user, we have the role, and now we\u0026rsquo;re bind them together with a RoleBinding resource. Run the following to create this RoleBinding:\ncat \u0026lt;\u0026lt; EoF \u0026gt; rbacuser-role-binding.yaml kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: read-pods namespace: rbac-test subjects: - kind: User name: rbac-user apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io EoF  Next, we apply the Role, and RoleBindings we created:\nkubectl apply -f rbacuser-role.yaml kubectl apply -f rbacuser-role-binding.yaml  "
},
{
	"uri": "/deploy/scalebackend/",
	"title": "Scale the Backend Services",
	"tags": [],
	"description": "",
	"content": "When we launched our services, we only launched one container of each. We can confirm this by viewing the running pods:\nkubectl get deployments  Now let\u0026rsquo;s scale up the backend services:\nkubectl scale deployment ecsdemo-nodejs --replicas=3 kubectl scale deployment ecsdemo-crystal --replicas=3  Confirm by looking at deployments again:\nkubectl get deployments  Also, check the browser tab where we can see our application running. You should now see traffic flowing to multiple backend services.\n"
},
{
	"uri": "/servicemesh_with_appmesh/create_the_k8s_app/about_the_k8s_app/",
	"title": "About DJ App",
	"tags": [],
	"description": "",
	"content": "The example app we\u0026rsquo;ll walk you through creating on App Mesh is called DJ. DJ is an application used for a cloud-based music service.\nThis application is composed of three microservices:\n dj metal-v1 and jazz-v1  The dj service makes requests to either the jazz or metal backends for artist lists. If the dj service requests from the jazz backend, musical artists such as Miles Davis or Astrud Gilberto will be returned. Requests made to the metal backend may return artists such as Judas Priest or Megadeth.\nToday, dj is hardwired to make requests to metal-v1 for metal requests, and hardwired to jazz-v1 for jazz requests. Each time there is a new metal or jazz release, we also need to release a new version of dj as to point to its new upstream endpoints. It works, but it\u0026rsquo;s not an optimal configuration to maintain for the long term.\nWe\u0026rsquo;re going to demonstrate how App Mesh can be used to simplify this architecture; by virtualizing the metal and jazz service, we can dynamically make them route to the endpoints and versions of our choosing, minimizing the need for complete re-deployment of the DJ app each time there is a new metal or jazz service release.\nWhen we\u0026rsquo;re done, our app will look more like the following:\nSeven total services with App Mesh sidecars proxying traffic, and the App Mesh control plane managing the sidecars\u0026rsquo; configuration rulesets.\n"
},
{
	"uri": "/logging/cleanup/",
	"title": "Cleanup Logging",
	"tags": [],
	"description": "",
	"content": "cd ~/environment kubectl delete -f ~/environment/fluentd/fluentd.yml rm -rf ~/environment/fluentd/ aws es delete-elasticsearch-domain --domain-name kubernetes-logs aws logs delete-log-group --log-group-name /eks/eksworkshop-eksctl-yourusername/containers rm -rf ~/environment/iam_policy/  "
},
{
	"uri": "/scaling/cleanup/",
	"title": "Cleanup Scaling",
	"tags": [],
	"description": "",
	"content": "kubectl delete -f ~/environment/cluster-autoscaler/cluster_autoscaler.yml kubectl delete -f ~/environment/cluster-autoscaler/nginx.yaml kubectl delete hpa,svc php-apache kubectl delete deployment php-apache load-generator rm -rf ~/environment/cluster-autoscaler  "
},
{
	"uri": "/servicemesh_with_appmesh/port_to_app_mesh/create_the_virtual_nodes/",
	"title": "Create the Virtual Nodes",
	"tags": [],
	"description": "",
	"content": "With the foundational mesh component created, we\u0026rsquo;ll continue onward to define the App Mesh Virtual Node and Virtual Service components.\nAll services (physical or virtual) that will interact in any way with each other in App Mesh must first be defined as Virtual Node objects. Abstracting out services as Virtual Nodes helps App Mesh build rulesets around inter-service communication. In addition, as we define Virtual Service objects, Virtual Nodes are referenced as the ingress and target endpoints for those Virtual Services. Because of this, it makes sense to define the Virtual Nodes first.\nBased on our first App Mesh-enabled architecture, our physical service dj will make requests to two new Virtual Services, metal, and jazz. Metal and jazz will route requests to the physical services metal-v1, and jazz-v1 accordingly.\nSince there will be five services involved in this configuration, we\u0026rsquo;ll need to define five Virtual Nodes.\nWe\u0026rsquo;ll first define the Virtual Nodes that will represent our virtual jazz and metal services. To define these services as App Mesh Virtual Nodes, enter the following:\nkubectl create -f 4_create_initial_mesh_components/nodes_representing_virtual_services.yaml  You should see output similar to:\nvirtualnode.appmesh.k8s.aws/metal created virtualnode.appmesh.k8s.aws/jazz created  If you open the YAML up in your favorite editor, you\u0026rsquo;ll notice a few things about these Virtual Nodes. They\u0026rsquo;re both similar, but for purposes of this tutorial, let\u0026rsquo;s examine just the metal.prod.svc.cluster.local VirtualNode:\napiVersion: appmesh.k8s.aws/v1beta1 kind: VirtualNode metadata: name: metal namespace: prod spec: meshName: dj-app listeners: - portMapping: port: 9080 protocol: http serviceDiscovery: dns: hostName: metal.prod.svc.cluster.local ...  According to this YAML, we see that this Virtual Node points to a service (spec.serviceDiscovery.dns.hostName: metal.prod.svc.cluster.local) that listens on a given port for requests (spec.listeners.portMapping.port: 9080).\nWe\u0026rsquo;ll finish up creating the dj, metal-v1, and jazz-v1 Virtual Nodes next. Run the following command:\nkubectl create -nprod -f 4_create_initial_mesh_components/nodes_representing_physical_services.yaml  Output should be similar to:\nvirtualnode.appmesh.k8s.aws/dj created virtualnode.appmesh.k8s.aws/jazz-v1 created virtualnode.appmesh.k8s.aws/metal-v1 created  If you view the YAML we used to create the above Virtual Nodes, you\u0026rsquo;ll notice jazz-v1 and metal-v1 are very similar (aside from name) to the previous metal and jazz Virtual Nodes we created earlier. The one key difference is to be found in the dj Virtual Node, which contains a backends attribute:\napiVersion: appmesh.k8s.aws/v1beta1 kind: VirtualNode metadata: name: dj namespace: prod spec: meshName: dj-app listeners: - portMapping: port: 9080 protocol: http serviceDiscovery: dns: hostName: dj.prod.svc.cluster.local backends: - virtualService: virtualServiceName: jazz.prod.svc.cluster.local - virtualService: virtualServiceName: metal.prod.svc.cluster.local  The backend attribute specifies that dj is allowed to make requests to the jazz and metal Virtual Services only.\nWe\u0026rsquo;ve now created five Virtual Nodes which can be view with the following command:\nkubectl get virtualnodes -nprod  yielding:\nNAME AGE dj 6m jazz 3h jazz-v1 6m metal 3h metal-v1 6m  "
},
{
	"uri": "/servicemesh_with_appmesh/create_app_mesh_components/set_injector_criteria/",
	"title": "Define the Injector Targets",
	"tags": [],
	"description": "",
	"content": "By default, the injector won\u0026rsquo;t act on any pods — we\u0026rsquo;ll need to give it criteria on what its auto-inject targets should be.\nFor the purpose of this tutorial, we\u0026rsquo;ll make it inject the App Mesh sidecar into any new pods created in the prod namespace. To do that, we\u0026rsquo;ll label our prod namespace with appmesh.k8s.aws/sidecarInjectorWebhook=enabled.\nReturn to the repo\u0026rsquo;s base dir:\ncd ..  And run the following command to label the prod namespace:\nkubectl label namespace prod appmesh.k8s.aws/sidecarInjectorWebhook=enabled  Output should be similar to:\nnamespace/prod labeled  Next, we\u0026rsquo;ll verify the Injector Controller is running:\nkubectl get pods -nappmesh-inject  You should see output similar to:\nNAME READY STATUS RESTARTS AGE aws-app-mesh-inject-78c59cc699-9jrb4 1/1 Running 0 1h  With the injector portion of the setup complete, lets move on to creating the App Mesh components.\n"
},
{
	"uri": "/helm_root/helm_micro/rolling_back/",
	"title": "Rolling Back",
	"tags": [],
	"description": "",
	"content": " Mistakes will happen during deployment, and when they do, Helm makes it easy to undo, or \u0026ldquo;roll back\u0026rdquo; to the previously deployed version.\nUpdate the demo application chart with a breaking change Open values.yaml and modify the image name under nodejs.image to brentley/ecsdemo-nodejs-non-existing. This image does not exist, so this will break our deployment.\nDeploy the updated demo application chart:\nhelm upgrade workshop ~/environment/eksdemo  The rolling upgrade will begin by creating a new nodejs pod with the new image. The new ecsdemo-nodejs Pod should fail to pull non-existing image. Run helm status command to see the ImagePullBackOff error:\nhelm status workshop  Rollback the failed upgrade Now we are going to rollback the application to the previous working release revision.\nFirst, list Helm release revisions:\nhelm history workshop  Then, rollback to the previous application revision (can rollback to any revision too):\n# rollback to the 1st revision helm rollback workshop 1  Validate workshop release status with:\nhelm status workshop  "
},
{
	"uri": "/spotworkers/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Cleanup our Microservices deployment\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml  Cleanup the Spot Handler Daemonset\nkubectl delete -f ~/environment/spot/spot-interrupt-handler-example.yml  To clean up the worker created by this module, run the following commands:\nRemove the Worker nodes from EKS:\naws cloudformation delete-stack --stack-name \u0026quot;eksworkshop-spot-workers\u0026quot;  "
},
{
	"uri": "/cleanup/workspace/",
	"title": "Cleanup the Workspace",
	"tags": [],
	"description": "",
	"content": "Since we no longer need the Cloud9 instance to have Administrator access to our account, we can delete the workspace we created:\n Go to your Cloud9 Environment Select the environment named eksworkshop and pick delete  "
},
{
	"uri": "/network-policies/",
	"title": "Securing Your Cluster with Network Policies",
	"tags": [],
	"description": "",
	"content": " Securing your cluster with network policies In this chapter, we are going to use two tools to secure our cluster by using network policies and then integrating our cluster\u0026rsquo;s network policies with EKS security groups.\nFirst we will use Project Calico to enforce Kubernetes network policies in our cluster, protecting our various microservices.\nAfter that, we will use Tigera\u0026rsquo;s Secure Cloud Edition to integrate the Kubernetes network policies with Amazon\u0026rsquo;s VPC security groups.\n"
},
{
	"uri": "/network-policies/tigera/tsce-feature-intro/",
	"title": "Walk through TSCE&#39;s extensions to Calico",
	"tags": [],
	"description": "",
	"content": "Now that TSCE is installed in your cluster, you could go back and re-run the examples in the Project Calico section, just to convince yourself that all those features remain working in a TSCE environment.\nThe two feature areas that we are going to showcase in this section are:\n VPC Security Group integration. Enhanced flow visibility in CloudWatch  Let\u0026rsquo;s get started.\n"
},
{
	"uri": "/logging/",
	"title": "Logging with Elasticsearch, Fluentd, and Kibana (EFK)",
	"tags": [],
	"description": "",
	"content": " Implement Logging with EFK In this Chapter, we will deploy a common Kubernetes logging pattern which consists of the following:\n Fluentd is an open source data collector providing a unified logging layer, supported by 500+ plugins connecting to many types of systems. Elasticsearch is a distributed, RESTful search and analytics engine. Kibana lets you visualize your Elasticsearch data.  Together, Fluentd, Elasticsearch and Kibana is also known as “EFK stack”. Fluentd will forward logs from the individual instances in the cluster to a centralized logging backend (CloudWatch Logs) where they are combined for higher-level reporting using ElasticSearch and Kibana.\n"
},
{
	"uri": "/monitoring/",
	"title": "Monitoring using Prometheus and Grafana",
	"tags": [],
	"description": "",
	"content": " Monitoring using Prometheus and Grafana In this Chapter, we will deploy Prometheus and Grafana to monitor Kubernetes cluster\n"
},
{
	"uri": "/servicemesh_with_istio/",
	"title": "Service Mesh with Istio",
	"tags": [],
	"description": "",
	"content": " Service Mesh With Istio A service mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application.\nService mesh solutions have two distinct components that behave somewhat differently: 1) a data plane, and 2) a control plane. The following diagram illustrates the basic architecture.\n The data plane is composed of a set of intelligent proxies (Envoy) deployed as sidecars. These proxies mediate and control all network communication between microservices along with Mixer, a general-purpose policy and telemetry hub.\n The control plane manages and configures the proxies to route traffic. Additionally, the control plane configures Mixers to enforce policies and collect telemetry.\n  "
},
{
	"uri": "/servicemesh_with_appmesh/",
	"title": "Service Mesh with App Mesh",
	"tags": [],
	"description": "",
	"content": "At re:invent 2018, we announced AWS App Mesh, a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. App Mesh standardizes how your services communicate, giving you end-to-end visibility and ensuring high-availability for your applications.\nService meshes like AWS App Mesh help you to run and monitor HTTP and TCP services at scale. Whether your application consists of AWS Fargate, Amazon EC2, Amazon ECS, Amazon Kubernetes Service, or Kubernetes clusters or instances, App Mesh provides consistent routing and traffic monitoring functionality, giving you insight into problems and the ability to re-route traffic after failures or code changes.\nApp Mesh uses the open source Envoy proxy, giving you access to a wide range of tools from AWS partners and the open source community. Since all traffic in and out of each service goes through the Envoy proxy, all traffic can be routed, shaped, measured, and logged. This extra level of indirection lets you build your services in any desired languages without having to use a common set of communication libraries.\nIn this tutorial, we\u0026rsquo;ll walk you through many popular App Mesh use cases.\nThe first two sections, \u0026ldquo;Create the k8s app\u0026rdquo;, and \u0026ldquo;Create the App Mesh Components\u0026rdquo; should be performed in order.\nThey will take you through building an easy to understand standalone k8s microservices-based application, and then enabling App Mesh service mesh functionality for it.\n"
},
{
	"uri": "/assigning_pods/",
	"title": "Assigning Pods to Nodes",
	"tags": [],
	"description": "",
	"content": " Assigning Pods to Nodes Introduction In this Chapter, we will review how the strategy of assigning Pods works, alternatives and recommended approaches.\nYou can constrain a pod to only be able to run on particular nodes or to prefer to run on particular nodes.\nGenerally such constraints are unnecessary, as the scheduler will automatically do a reasonable placement (e.g. spread your pods across nodes, not place the pod on a node with insufficient free resources, etc.) but there are some circumstances where you may want more control on a node where a pod lands, e.g. to ensure that a pod ends up on a machine with an SSD attached to it, or to co-locate pods from two different services that communicate a lot into the same availability zone.\n"
},
{
	"uri": "/advanced-networking/",
	"title": "Advanced VPC Networking with EKS",
	"tags": [],
	"description": "",
	"content": " Advanced VPC Networking with EKS In this Chapter, we will review some of the advanced VPC networking features with EKS.\n"
},
{
	"uri": "/exposing_service/",
	"title": "Exposing a Service",
	"tags": [],
	"description": "",
	"content": " Exposing a Service Introduction In this Chapter, we will review how to configure a Service, Deployment or Pod to be exposed outside our cluster. We will also review the different ways to do so.\n"
},
{
	"uri": "/statefulset/",
	"title": "Stateful containers using StatefulSets",
	"tags": [],
	"description": "",
	"content": " Stateful containers using StatefulSets StatefulSets manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods, suitable for applications that require one or more of the following.\n Stable, unique network identifiers Stable, persistent storage Ordered, graceful deployment and scaling Ordered, automated rolling updates  In this Chapter, we will review how to deploy MySQL database using StatefulSets and EBS as PersistentVolume. The example is a MySQL single master topology with multiple slaves running asynchronous replication.\n"
},
{
	"uri": "/irsa/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "To cleanup, follow the below steps.\nTo remove sample application\nkubectl delete -f iam-pod.yaml  To remove IAM role and Service Account stack from cloudformation\neksctl delete iamserviceaccount --name iam-test --namespace default --cluster eksworkshop-eksctl-yourusername  "
},
{
	"uri": "/advanced-networking/secondary_cidr/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s cleanup this tutorial\nkubectl delete deployments --all  Edit aws-node configmap and comment AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG and its value\nkubectl edit daemonset -n kube-system aws-node  ... spec: containers: - env: #- name: AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG # value: \u0026quot;true\u0026quot; - name: AWS_VPC_K8S_CNI_LOGLEVEL value: DEBUG - name: MY_NODE_NAME ...  Delete custom resource objects from ENIConfig CRD\nkubectl delete eniconfig/group1-pod-netconfig kubectl delete eniconfig/group2-pod-netconfig kubectl delete eniconfig/group3-pod-netconfig  Terminate EC2 instances so that fresh instances are launched with default CNI configuration\nUse caution before you run the next command because it terminates all worker nodes including running pods in your workshop\n INSTANCE_IDS=(`aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId' --filters \u0026quot;Name=tag:Name,Values=eksworkshop*\u0026quot; --output text` ) for i in \u0026quot;${INSTANCE_IDS[@]}\u0026quot; do echo \u0026quot;Terminating EC2 instance $i ...\u0026quot; aws ec2 terminate-instances --instance-ids $i done  Delete secondary CIDR from your VPC\nVPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=eksctl-eksworkshop* | jq -r '.Vpcs[].VpcId') ASSOCIATION_ID=$(aws ec2 describe-vpcs --vpc-id $VPC_ID | jq -r '.Vpcs[].CidrBlockAssociationSet[] | select(.CidrBlock == \u0026quot;100.64.0.0/16\u0026quot;) | .AssociationId') aws ec2 delete-subnet --subnet-id $CGNAT_SNET1 aws ec2 delete-subnet --subnet-id $CGNAT_SNET2 aws ec2 delete-subnet --subnet-id $CGNAT_SNET3 aws ec2 disassociate-vpc-cidr-block --association-id $ASSOCIATION_ID  "
},
{
	"uri": "/batch/workflow-simple/",
	"title": "Simple Batch Workflow",
	"tags": [],
	"description": "",
	"content": " Simple Batch Workflow Save the below manifest as \u0026lsquo;workflow-whalesay.yaml\u0026rsquo; using your favorite editor and let\u0026rsquo;s deploy the whalesay example from before using Argo.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: whalesay- spec: entrypoint: whalesay templates: - name: whalesay container: image: docker/whalesay command: [cowsay] args: [\u0026quot;This is an Argo Workflow!\u0026quot;]  Now deploy the workflow using the argo CLI.\nYou can also run workflow specs directly using kubectl but the argo CLI provides syntax checking, nicer output, and requires less typing. For the equivalent kubectl commands, see Argo CLI.\n argo submit --watch workflow-whalesay.yaml  Name: whalesay-2kfxb Namespace: default ServiceAccount: default Status: Succeeded Created: Sat Nov 17 10:32:13 -0500 (3 seconds ago) Started: Sat Nov 17 10:32:13 -0500 (3 seconds ago) Finished: Sat Nov 17 10:32:16 -0500 (now) Duration: 3 seconds STEP PODNAME DURATION MESSAGE ✔ whalesay-2kfxb whalesay-2kfxb 2s  Make a note of the workflow\u0026rsquo;s name from your output (It should be similar to whalesay-xxxxx).\nConfirm the output by running the following command, substituting name of your workflow for \u0026ldquo;whalesay-xxxxx\u0026rdquo;:\nargo logs whalesay-xxxxx  ___________________________ \u0026lt; This is an Argo Workflow! \u0026gt; --------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/  "
},
{
	"uri": "/servicemesh_with_istio/visualize/",
	"title": "Monitor &amp; Visualize",
	"tags": [],
	"description": "",
	"content": " Collecting new telemetry data Next, download a YAML file to hold configuration for the new metric and log stream that Istio will generate and collect automatically.\ncurl -LO https://eksworkshop.com/servicemesh_with_istio/deploy.files/istio-telemetry.yaml kubectl apply -f istio-telemetry.yaml  Make sure Prometheus and Grafana are running\nkubectl -n istio-system get svc prometheus kubectl -n istio-system get svc grafana  Setup port-forwarding for Grafana by executing the following command:\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 8080:3000 \u0026amp;  Open the Istio Dashboard via the Grafana UI\n In your Cloud9 environment, click Preview / Preview Running Application Scroll to the end of the URL and append:  /dashboard/db/istio-mesh-dashboard  Open a new terminal tab and enter to send a traffic to the mesh\nexport SMHOST=$(kubectl get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname} ' -n istio-system) SMHOST=\u0026quot;$(echo -e \u0026quot;${SMHOST}\u0026quot; | tr -d '[:space:]')\u0026quot; while true; do curl -o /dev/null -s \u0026quot;${SMHOST}/productpage\u0026quot;; done  You will see that the traffic is evenly spread between reviews:v1 and reviews:v3\nWe encourage you to explore other Istio dashboards that are available by clicking the Istio Mesh Dashboard menu on top left of the page, and selecting a different dashboard.\n"
},
{
	"uri": "/introduction/basics/concepts_objects_details_1/",
	"title": "K8s Objects Detail (1/2)",
	"tags": [],
	"description": "",
	"content": " Pod  A thin wrapper around one or more containers  DaemonSet  Implements a single instance of a pod on a worker node  Deployment  Details how to roll out (or roll back) across versions of your application  "
},
{
	"uri": "/intro_to_rbac/verify_user_role_binding/",
	"title": "Verify the Role and Binding",
	"tags": [],
	"description": "",
	"content": "Now that the user, Role, and RoleBinding are defined, lets switch back to rbac-user, and test.\nTo switch back to rbac-user, issue the following command that sources the rbac-user env vars, and verifies they\u0026rsquo;ve taken:\n. rbacuser_creds.sh; aws sts get-caller-identity  You should see output reflecting that you are logged in as rbac-user.\nAs rbac-user, issue the following to get pods in the rbac namespace:\nkubectl get pods -n rbac-test  The output should be similar to:\nNAME READY STATUS RESTARTS AGE nginx-55bd7c9fd-kmbkf 1/1 Running 0 23h  Try running the same command again, but outside of the rbac-test namespace:\nkubectl get pods -n kube-system  You should get an error similar to:\nNo resources found. Error from server (Forbidden): pods is forbidden: User \u0026quot;rbac-user\u0026quot; cannot list resource \u0026quot;pods\u0026quot; in API group \u0026quot;\u0026quot; in the namespace \u0026quot;kube-system\u0026quot;  Because the role you are bound to does not give you access to any namespace other than rbac-test.\n"
},
{
	"uri": "/deploy/scalefrontend/",
	"title": "Scale the Frontend",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s also scale our frontend service the same way:\nkubectl get deployments kubectl scale deployment ecsdemo-frontend --replicas=3 kubectl get deployments  Check the browser tab where we can see our application running. You should now see traffic flowing to multiple frontend services.\n"
},
{
	"uri": "/servicemesh_with_appmesh/create_app_mesh_components/adding_crds/",
	"title": "Adding the CRDs",
	"tags": [],
	"description": "",
	"content": "There are two ways to create the components of the App Mesh service mesh:\n With the AWS CLI With kubectl via CRDs  For this tutorial, we\u0026rsquo;ll use kubectl to define the App Mesh components.\nTo do this, we\u0026rsquo;ll add Custom Resource Definitions (CRDs), and the App Mesh controller logic that syncs our kubernetes cluster\u0026rsquo;s CRD state with the AWS cloud-side App Mesh control plane.\nTo add the CRDs, from the repository base directory, execute the following commands:\nkubectl apply -f 3_add_crds/mesh-definition.yaml kubectl apply -f 3_add_crds/virtual-node-definition.yaml kubectl apply -f 3_add_crds/virtual-service-definition.yaml  Output should be similar to:\ncustomresourcedefinition.apiextensions.k8s.io/meshes.appmesh.k8s.aws (http://customresourcedefinition.apiextensions.k8s.io/meshes.appmesh.k8s.aws) created customresourcedefinition.apiextensions.k8s.io/virtualnodes.appmesh.k8s.aws (http://customresourcedefinition.apiextensions.k8s.io/virtualnodes.appmesh.k8s.aws) created customresourcedefinition.apiextensions.k8s.io/virtualservices.appmesh.k8s.aws (http://customresourcedefinition.apiextensions.k8s.io/virtualservices.appmesh.k8s.aws) created  Next, add the controller by executing the following command:\nkubectl apply -f 3_add_crds/controller-deployment.yaml  Output should be similar to:\nnamespace/appmesh-system created deployment.apps/app-mesh-controller created serviceaccount/app-mesh-sa created clusterrole.rbac.authorization.k8s.io/app-mesh-controller (http://clusterrole.rbac.authorization.k8s.io/app-mesh-controller) created clusterrolebinding.rbac.authorization.k8s.io/app-mesh-controller-binding (http://clusterrolebinding.rbac.authorization.k8s.io/app-mesh-controller-binding) created  Execute the following command to verify the App Mesh Controller is running:\nkubectl get pods -nappmesh-system  You should see output similar to:\nNAME READY STATUS RESTARTS AGE app-mesh-controller-85f9d4b48f-j9vz4 1/1 Running 0 7m  NOTE: The CRD and Injector are AWS supported open source projects. If you plan to deploy the CRD and/or Injector for production projects, always build them from the latest AWS Github repos to stay up to date on the latest features and bug fixes, and deploy them from your own container registry.\n"
},
{
	"uri": "/helm_root/helm_micro/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "To delete the workshop release, run:\nhelm del --purge workshop  "
},
{
	"uri": "/jenkins/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "To delete Jenkins, run:\nhelm del --purge cicd  "
},
{
	"uri": "/servicemesh_with_appmesh/create_the_k8s_app/create_the_app/",
	"title": "Create DJ App",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s create the DJ App!\nTo create the prod namespace, issue the following command:\nkubectl apply -f 1_create_the_initial_architecture/1_prod_ns.yaml  Output should be similar to:\nnamespace/prod created  Now that we have the prod namespace created, we\u0026rsquo;ll deploy the DJ App (dj, metal, and jazz microservices) into it.\nCreate the DJ App deployment in the prod namespace by issuing the following command:\nkubectl apply -nprod -f 1_create_the_initial_architecture/1_initial_architecture_deployment.yaml  Output should be similar to:\ndeployment.apps \u0026quot;dj\u0026quot; created deployment.apps \u0026quot;metal-v1\u0026quot; created deployment.apps \u0026quot;jazz-v1\u0026quot; created  Create the services that front these deployments by issuing the following command:\nkubectl apply -nprod -f 1_create_the_initial_architecture/1_initial_architecture_services.yaml  Output should be similar to:\nservice \u0026quot;dj\u0026quot; created service \u0026quot;metal-v1\u0026quot; created service \u0026quot;jazz-v1\u0026quot; created  Let\u0026rsquo;s verify everything has been setup correctly by getting all resources from the prod namespace. Issue this command:\nkubectl get all -nprod  Output should display dj, jazz, and metal pods, services, deployments, and replica sets, similar to:\nNAME READY STATUS RESTARTS AGE pod/dj-5b445fbdf4-qf8sv 1/1 Running 0 1m pod/jazz-v1-644856f4b4-mshnr 1/1 Running 0 1m pod/metal-v1-84bffcc887-97qzw 1/1 Running 0 1m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dj ClusterIP 10.100.247.180 \u0026lt;none\u0026gt; 9080/TCP 15s service/jazz-v1 ClusterIP 10.100.157.174 \u0026lt;none\u0026gt; 9080/TCP 15s service/metal-v1 ClusterIP 10.100.187.186 \u0026lt;none\u0026gt; 9080/TCP 15s NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/dj 1 1 1 1 1m deployment.apps/jazz-v1 1 1 1 1 1m deployment.apps/metal-v1 1 1 1 1 1m NAME DESIRED CURRENT READY AGE replicaset.apps/dj-5b445fbdf4 1 1 1 1m replicaset.apps/jazz-v1-644856f4b4 1 1 1 1m replicaset.apps/metal-v1-84bffcc887 1 1 1 1m  Once you\u0026rsquo;ve verified all resources have been created correctly in the prod namespace, next we\u0026rsquo;ll test out this initial version of the DJ App.\n"
},
{
	"uri": "/servicemesh_with_appmesh/port_to_app_mesh/create_the_virtual_services/",
	"title": "Create the Virtual Services",
	"tags": [],
	"description": "",
	"content": "The next step is to create the two App Mesh Virtual Services that will intercept and route requests made to jazz and metal.\nTo accomplish this, execute the following command:\nkubectl apply -nprod -f 4_create_initial_mesh_components/virtual-services.yaml  Output should be similar to:\nvirtualservice.appmesh.k8s.aws/jazz.prod.svc.cluster.local created virtualservice.appmesh.k8s.aws/metal.prod.svc.cluster.local created  If we inspect the YAML we just applied, we\u0026rsquo;ll see that we\u0026rsquo;ve created two VirtualService resources, where requests made to jazz.prod.svc.cluster.local (via the placeholder service IP of 10.100.220.118) will be intercepted by App Mesh, and routed to the Virtual Node jazz-v1.\nSimilarly, requests made to metal.prod.svc.cluster.local (via the placeholder service IP of 10.100.122.192) will be routed to the Virtual Node metal-v1:\napiVersion: appmesh.k8s.aws/v1beta1 kind: VirtualService metadata: name: jazz.prod.svc.cluster.local namespace: prod spec: meshName: dj-app virtualRouter: name: jazz-router routes: - name: jazz-route http: match: prefix: / action: weightedTargets: - virtualNodeName: jazz-v1 weight: 100 --- apiVersion: appmesh.k8s.aws/v1beta1 kind: VirtualService metadata: name: metal.prod.svc.cluster.local namespace: prod spec: meshName: dj-app virtualRouter: name: metal-router routes: - name: metal-route http: match: prefix: / action: weightedTargets: - virtualNodeName: metal-v1 weight: 100  Remember to use fully qualified DNS names for the the Virtual Service\u0026rsquo;s metadata.name field to prevent the chance of name collisions when using App Mesh cross-cluster.\n With these Virtual Services defined, to access them by name, clients (in our case, the dj container) will first perform a DNS lookup request to jazz.prod.svc.cluster.local, or metal.prod.svc.cluster.local before making the request.\nIf the dj container (or any other client) cannot resolve that name to an IP, the subsequent HTTP request will fail with a name lookup error.\nOur other physical services (jazz-v1, metal-v1, dj) are defined as physical kubernetes services, and therefore have discoverable names and IPs. However, these Virtual Services don\u0026rsquo;t (yet).\nkubectl get svc -nprod  yields:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE dj ClusterIP 10.100.247.180 \u0026lt;none\u0026gt; 9080/TCP 16h jazz-v1 ClusterIP 10.100.157.174 \u0026lt;none\u0026gt; 9080/TCP 16h metal-v1 ClusterIP 10.100.187.186 \u0026lt;none\u0026gt; 9080/TCP 16h  To provide the the jazz and metal virtual services with resolvable IPs and hostnames, we\u0026rsquo;ll define them as kubernetes services that do not map to any deployments or pods; we\u0026rsquo;ll do this by creating them as k8s services without defining selectors for them.\nSince App Mesh will be intercepting and routing requests made for them, they won\u0026rsquo;t need to map to any pods or deployments on the k8s-side.\nTo register the placeholder names and IPs for these Virtual Services, execute the following:\nkubectl create -nprod -f 4_create_initial_mesh_components/metal_and_jazz_placeholder_services.yaml  Output should be similar to:\nservice/jazz created service/metal created  We can now use kubectl to get the registered metal and jazz Virtual Services:\nkubectl get -nprod virtualservices  yields:\nNAME AGE jazz.prod.svc.cluster.local 10m metal.prod.svc.cluster.local 10m  along with the Virtual Service placeholder IPs, and physical service IPs:\nkubectl get svc -nprod  yields:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE dj ClusterIP 10.100.247.180 \u0026lt;none\u0026gt; 9080/TCP 17h jazz ClusterIP 10.100.220.118 \u0026lt;none\u0026gt; 9080/TCP 27s jazz-v1 ClusterIP 10.100.157.174 \u0026lt;none\u0026gt; 9080/TCP 17h metal ClusterIP 10.100.122.192 \u0026lt;none\u0026gt; 9080/TCP 27s metal-v1 ClusterIP 10.100.187.186 \u0026lt;none\u0026gt; 9080/TCP 17h  As such, when name lookup requests are made to our Virtual Services, alongside their physical service counterparts, they will resolve.\n"
},
{
	"uri": "/custom_resource_definition/",
	"title": "Custom Resource Definition",
	"tags": [],
	"description": "",
	"content": " Custom Resource Definition Introduction In this Chapter, we will review the Custom Resource Definition (CRD) concept, and some examples of usage.\nIn Kubernetes API, a resource is an endpoint storing the API objects in a collection. As an example, the pods resource contains a collection of Pod objects.\nCRD’s are extensions of Kubernetes API that stores collection of API objects of certain kind. They extend the Kubernetes API or allow you to add your own API into the cluster.\nTo create a CRD, you need to create a file, that defines your object kinds and lets the API Server manage the lifecycle. Applying a CRD into the cluster makes the Kubernetes API server to serve the specified custom resource.\nWhen a CRD is created, the Kubernetes API creates a new RESTful resource path, that can be accesed by a cluster or a single namespace.\n"
},
{
	"uri": "/helm_root/helm_intro/",
	"title": "Install Helm on EKS",
	"tags": [],
	"description": "",
	"content": " Install Helm on EKS Helm is a package manager and application management tool for Kubernetes that packages multiple Kubernetes resources into a single logical deployment unit called Chart.\nHelm helps you to:\n Achieve a simple (one command) and repeatable deployment Manage application dependency, using specific versions of other application and services Manage multiple deployment configurations: test, staging, production and others Execute post/pre deployment jobs during application deployment Update/rollback and test application deployments  "
},
{
	"uri": "/conclusion/survey/",
	"title": "Let us know what you think!",
	"tags": [],
	"description": "",
	"content": " Please take our survey! (function(t,e,s,n){var o,a,c;t.SMCX=t.SMCX||[],e.getElementById(n)||(o=e.getElementsByTagName(s),a=o[o.length-1],c=e.createElement(s),c.type=\"text/javascript\",c.async=!0,c.id=n,c.src=[\"https:\"===location.protocol?\"https://\":\"http://\",\"widget.surveymonkey.com/collect/website/js/tRaiETqnLgj758hTBazgd_2BU860jlhPrsKW9DSM0aec7fijRMWQEdDb7y2zM_2FUrIx.js\"].join(\"\"),a.parentNode.insertBefore(c,a))})(window,document,\"script\",\"smcx-sdk\");Create your own user feedback survey    "
},
{
	"uri": "/network-policies/calico/stars_policy_demo/",
	"title": "Stars Policy Demo",
	"tags": [],
	"description": "",
	"content": " Stars Policy Demo In this sub-chapter we create frontend, backend, client and UI services on the EKS cluster and define network policies to allow or block communication between these services. This demo also has a management UI that shows the available ingress and egress paths between each service.\n"
},
{
	"uri": "/servicemesh_with_istio/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "To cleanup, follow the below steps.\nTo remove telemetry configuration / port-forward process\nkubectl delete -f istio-telemetry.yaml  To remove the application virtual services / destination rules\nkubectl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml kubectl delete -f samples/bookinfo/networking/destination-rule-all.yaml  To remove the gateway / application\nkubectl delete -f samples/bookinfo/networking/bookinfo-gateway.yaml kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml  To remove Istio\nhelm delete --purge istio helm delete --purge istio-init  "
},
{
	"uri": "/batch/workflow-advanced/",
	"title": "Advanced Batch Workflow",
	"tags": [],
	"description": "",
	"content": " Advanced Batch Workflow Let\u0026rsquo;s take a look at a more complex workflow, involving passing artifacts between jobs, multiple dependencies, etc.\nSave the below manifest as teardrop.yaml using your favorite editor.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: teardrop- spec: entrypoint: teardrop templates: - name: create-chain container: image: alpine:latest command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;] args: [\u0026quot;echo '' \u0026gt;\u0026gt; /tmp/message\u0026quot;] outputs: artifacts: - name: chain path: /tmp/message - name: whalesay inputs: parameters: - name: message artifacts: - name: chain path: /tmp/message container: image: docker/whalesay command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;] args: [\u0026quot;echo Chain: ; cat /tmp/message* | sort | uniq | tee /tmp/message; cowsay This is Job {{inputs.parameters.message}}! ; echo {{inputs.parameters.message}} \u0026gt;\u0026gt; /tmp/message\u0026quot;] outputs: artifacts: - name: chain path: /tmp/message - name: whalesay-reduce inputs: parameters: - name: message artifacts: - name: chain-0 path: /tmp/message.0 - name: chain-1 path: /tmp/message.1 container: image: docker/whalesay command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;] args: [\u0026quot;echo Chain: ; cat /tmp/message* | sort | uniq | tee /tmp/message; cowsay This is Job {{inputs.parameters.message}}! ; echo {{inputs.parameters.message}} \u0026gt;\u0026gt; /tmp/message\u0026quot;] outputs: artifacts: - name: chain path: /tmp/message - name: teardrop dag: tasks: - name: create-chain template: create-chain - name: Alpha dependencies: [create-chain] template: whalesay arguments: parameters: [{name: message, value: Alpha}] artifacts: - name: chain from: \u0026quot;{{tasks.create-chain.outputs.artifacts.chain}}\u0026quot; - name: Bravo dependencies: [Alpha] template: whalesay arguments: parameters: [{name: message, value: Bravo}] artifacts: - name: chain from: \u0026quot;{{tasks.Alpha.outputs.artifacts.chain}}\u0026quot; - name: Charlie dependencies: [Alpha] template: whalesay arguments: parameters: [{name: message, value: Charlie}] artifacts: - name: chain from: \u0026quot;{{tasks.Alpha.outputs.artifacts.chain}}\u0026quot; - name: Delta dependencies: [Bravo] template: whalesay arguments: parameters: [{name: message, value: Delta}] artifacts: - name: chain from: \u0026quot;{{tasks.Bravo.outputs.artifacts.chain}}\u0026quot; - name: Echo dependencies: [Bravo, Charlie] template: whalesay-reduce arguments: parameters: [{name: message, value: Echo}] artifacts: - name: chain-0 from: \u0026quot;{{tasks.Bravo.outputs.artifacts.chain}}\u0026quot; - name: chain-1 from: \u0026quot;{{tasks.Charlie.outputs.artifacts.chain}}\u0026quot; - name: Foxtrot dependencies: [Charlie] template: whalesay arguments: parameters: [{name: message, value: Foxtrot}] artifacts: - name: chain from: \u0026quot;{{tasks.create-chain.outputs.artifacts.chain}}\u0026quot; - name: Golf dependencies: [Delta, Echo] template: whalesay-reduce arguments: parameters: [{name: message, value: Golf}] artifacts: - name: chain-0 from: \u0026quot;{{tasks.Delta.outputs.artifacts.chain}}\u0026quot; - name: chain-1 from: \u0026quot;{{tasks.Echo.outputs.artifacts.chain}}\u0026quot; - name: Hotel dependencies: [Echo, Foxtrot] template: whalesay-reduce arguments: parameters: [{name: message, value: Hotel}] artifacts: - name: chain-0 from: \u0026quot;{{tasks.Echo.outputs.artifacts.chain}}\u0026quot; - name: chain-1 from: \u0026quot;{{tasks.Foxtrot.outputs.artifacts.chain}}\u0026quot;  This workflow uses a Directed Acyclic Graph (DAG) to explicitly define job dependencies. Each job in the workflow calls a whalesay template and passes a parameter with a unique name. Some jobs call a whalesay-reduce template which accepts multiple artifacts and combines them into a single artifact.\nEach job in the workflow pulls the artifact(s) and lists them in the \u0026ldquo;Chain\u0026rdquo;, then calls whalesay for the current job. Each job will then have a list of the previous job dependency chain (list of all jobs that had to complete before current job could run).\nRun the workflow.\nargo submit --watch teardrop.yaml  Name: teardrop-jfg5w Namespace: default ServiceAccount: default Status: Succeeded Created: Sat Nov 17 16:01:42 -0500 (7 minutes ago) Started: Sat Nov 17 16:01:42 -0500 (7 minutes ago) Finished: Sat Nov 17 16:03:35 -0500 (5 minutes ago) Duration: 1 minute 53 seconds STEP PODNAME DURATION MESSAGE ✔ teardrop-jfg5w ├-✔ create-chain teardrop-jfg5w-3938249022 3s ├-✔ Alpha teardrop-jfg5w-3385521262 6s ├-✔ Bravo teardrop-jfg5w-1878939134 35s ├-✔ Charlie teardrop-jfg5w-3753534620 35s ├-✔ Foxtrot teardrop-jfg5w-2036090354 5s ├-✔ Delta teardrop-jfg5w-37094256 34s ├-✔ Echo teardrop-jfg5w-4165010455 31s ├-✔ Hotel teardrop-jfg5w-2342859904 4s └-✔ Golf teardrop-jfg5w-1687601882 30s  Continue to the Argo Dashboard to explore this model further.\n"
},
{
	"uri": "/introduction/basics/concepts_objects_details_2/",
	"title": "K8s Objects Detail (2/2)",
	"tags": [],
	"description": "",
	"content": " ReplicaSet  Ensures a defined number of pods are always running  Job  Ensures a pod properly runs to completion  Service  Maps a fixed IP address to a logical group of pods  Label  Key/Value pairs used for association and filtering  "
},
{
	"uri": "/intro_to_rbac/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Once you have completed this chapter, you can cleanup the files and resources you created by issuing the following commands:\nunset AWS_SECRET_ACCESS_KEY unset AWS_ACCESS_KEY_ID kubectl delete namespace rbac-test rm aws-auth.yaml rm rbacuser_creds.sh rm /tmp/create_output.json rm rbacuser-role.yaml  "
},
{
	"uri": "/servicemesh_with_appmesh/port_to_app_mesh/triggering_injector_first_time/",
	"title": "Bootstrap the Injector",
	"tags": [],
	"description": "",
	"content": "Right now, if we describe any of the pods running in the prod namespace, we\u0026rsquo;ll notice that they are running with just one container, the same one we initially deployed it with:\nkubectl get pods -nprod  yields:\nNAME READY STATUS RESTARTS AGE dj-5b445fbdf4-qf8sv 1/1 Running 0 3h jazz-v1-644856f4b4-mshnr 1/1 Running 0 3h metal-v1-84bffcc887-97qzw 1/1 Running 0 3h  and to take a closer look:\nkubectl describe pods/dj-5b445fbdf4-qf8sv -nprod  yields:\n... Containers: dj: Container ID: docker://76e6d5f7101dfce60158a63cf7af9fcb3c821c087db360e87c5e2fb8850b7aa9 Image: 970805265562.dkr.ecr.us-west-2.amazonaws.com/hello-world:latest Image ID: docker-pullable://970805265562.dkr.ecr.us-west-2.amazonaws.com/hello-world@sha256:581fe44cf2413a48f0cdf005b86b025501eaff6cafc7b26367860e07be060753 Port: 9080/TCP Host Port: 0/TCP State: Running ...  The injector controller we installed earlier watches for new pods to be created, and ensures any new pods that are created in the prod namespace are injected with the App Mesh sidecar. Since our dj pods were already running before the injector was created, we\u0026rsquo;ll force them to be recreated, this time with the sidecars auto-injected into them.\nIn production, there are more graceful ways to do this, but for the purpose of this tutorial, an easy way to have the deployment recreate the pods in an innocuous fashion is to patch into the deployment a simple date annotation.\nTo do that with our current deployment, first we get all the prod namespace pod names:\nkubectl get pods -nprod  The output will be the pod names:\nNAME READY STATUS RESTARTS AGE dj-5b445fbdf4-qf8sv 1/1 Running 0 3h jazz-v1-644856f4b4-mshnr 1/1 Running 0 3h metal-v1-84bffcc887-97qzw 1/1 Running 0 3h  Note that under the READY column, we see 1\u0026frasl;1, which indicates one container is running for each pod.\nNext, run the following commands to add a date label to each dj, jazz-v1, and metal-1 deployment, forcing the pods to be recreated:\nkubectl patch deployment dj -nprod -p \u0026quot;{\\\u0026quot;spec\\\u0026quot;:{\\\u0026quot;template\\\u0026quot;:{\\\u0026quot;metadata\\\u0026quot;:{\\\u0026quot;labels\\\u0026quot;:{\\\u0026quot;date\\\u0026quot;:\\\u0026quot;`date +'%s'`\\\u0026quot;}}}}}\u0026quot; kubectl patch deployment metal-v1 -nprod -p \u0026quot;{\\\u0026quot;spec\\\u0026quot;:{\\\u0026quot;template\\\u0026quot;:{\\\u0026quot;metadata\\\u0026quot;:{\\\u0026quot;labels\\\u0026quot;:{\\\u0026quot;date\\\u0026quot;:\\\u0026quot;`date +'%s'`\\\u0026quot;}}}}}\u0026quot; kubectl patch deployment jazz-v1 -nprod -p \u0026quot;{\\\u0026quot;spec\\\u0026quot;:{\\\u0026quot;template\\\u0026quot;:{\\\u0026quot;metadata\\\u0026quot;:{\\\u0026quot;labels\\\u0026quot;:{\\\u0026quot;date\\\u0026quot;:\\\u0026quot;`date +'%s'`\\\u0026quot;}}}}}\u0026quot;  Once again, get the pods:\nkubectl get pods -nprod  Now note how we see 2\u0026frasl;2 under READY, which indicates two container for each pod are running:\nNAME READY STATUS RESTARTS AGE dj-6cfb85cdd9-z5hsp 2/2 Running 0 10m jazz-v1-79d67b4fd6-hdrj9 2/2 Running 0 16s metal-v1-769b58d9dc-7q92q 2/2 Running 0 18s  If you don\u0026rsquo;t see the above exact output, and instead see “Terminating” or \u0026ldquo;Initializing\u0026rdquo; pods, wait about 10 seconds — (your redeployment is underway), and re-run the command.\n If we now describe the new dj pod to get more detail:\n... Containers: dj: Container ID: docker://bef63f2e45fb911f78230ef86c2a047a56c9acf554c2272bc094300c6394c7fb Image: 970805265562.dkr.ecr.us-west-2.amazonaws.com/hello-world:latest ... envoy: Container ID: docker://2bd0dc0707f80d436338fce399637dcbcf937eaf95fed90683eaaf5187fee43a Image: 111345817488.dkr.ecr.us-west-2.amazonaws.com/aws-appmesh-envoy:v1.8.0.2-beta ...  We\u0026rsquo;ll see that both the original container, and the auto-injected sidecar will both be running for any new pods created in the prod namespace.\n"
},
{
	"uri": "/servicemesh_with_appmesh/create_app_mesh_components/conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "Congratulations on deploying the App Mesh Components!\nNow that we have our App Mesh components created, we\u0026rsquo;ll introduce observability, analytics, and routing functionality into our DJ App by porting it to run on top of an App Mesh service mesh.\n"
},
{
	"uri": "/servicemesh_with_appmesh/create_the_k8s_app/test_the_app/",
	"title": "Test DJ App",
	"tags": [],
	"description": "",
	"content": "To test what we\u0026rsquo;ve just created, we\u0026rsquo;ll exec into the DJ pod, and curl out to the jazz-v1 and metal-v1 backends.\nFirst, we get the name of our DJ pod by listing all pods with the dj app selector:\nkubectl get pods -nprod -l app=dj  Output should be similar to:\nNAME READY STATUS RESTARTS AGE dj-5b445fbdf4-8xkwp 1/1 Running 0 32s  Next, we\u0026rsquo;ll exec into the DJ pod:\nkubectl exec -nprod -it \u0026lt;your-dj-pod-name\u0026gt; bash  Output should be similar to:\nroot@dj-5b445fbdf4-8xkwp:/usr/src/app#  Now that we have a root prompt into the DJ pod, we\u0026rsquo;ll issue a curl request to the jazz-v1 backend service:\ncurl jazz-v1.prod.svc.cluster.local:9080;echo  Output should be similar to:\n[\u0026quot;Astrud Gilberto\u0026quot;,\u0026quot;Miles Davis\u0026quot;]  Try it again, but issue the command to the metal-v1.prod.svc.cluster.local backend on port 9080:\ncurl metal-v1.prod.svc.cluster.local:9080;echo  You should get a list of heavy metal bands back:\n[\u0026quot;Megadeth\u0026quot;,\u0026quot;Judas Priest\u0026quot;]  When you\u0026rsquo;re done exploring this vast world of music, hit CTRL-D, or type exit to exit the container\u0026rsquo;s shell:\nroot@dj-779566bbf6-cqpxt:/usr/src/app# exit command terminated with exit code 1 $  "
},
{
	"uri": "/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Cleanup "
},
{
	"uri": "/helm_root/helm_nginx/",
	"title": "Deploy Nginx With Helm",
	"tags": [],
	"description": "",
	"content": " Deploy Nginx With Helm In this Chapter, we will dig deeper with Helm and demonstrate how to install the NGINX web server via the following steps:\n Update the Chart Repository   Search the Chart Repository   Add the Bitnami Repository   Install bitnami/nginx   Clean Up   "
},
{
	"uri": "/batch/dashboard/",
	"title": "Argo Dashboard",
	"tags": [],
	"description": "",
	"content": " Argo Dashboard Argo UI lists the workflows and visualizes each workflow (very handy for our last workflow).\nTo connect, use the same proxy connection setup in Deploy the Official Kubernetes Dashboard.\n  Show me the command   kubectl proxy --port=8080 --address='0.0.0.0' --disable-filter=true \u0026amp;  This will start the proxy, listen on port 8080, listen on all interfaces, and will disable the filtering of non-localhost requests.\nThis command will continue to run in the background of the current terminal\u0026rsquo;s session.\nWe are disabling request filtering, a security feature that guards against XSRF attacks. This isn\u0026rsquo;t recommended for a production environment, but is useful for our dev environment.\n   To access the Argo Dashboard:\n In your Cloud9 environment, click Preview / Preview Running Application Scroll to the end of the URL and append:  /api/v1/namespaces/argo/services/argo-ui/proxy/  You will see the teardrop workflow from Advanced Batch Workflow. Click on it to see a visualization of the workflow.\nThe workflow should relatively look like a teardrop, and provide a live status for each job. Click on Hotel to see a summary of the Hotel job.\nThis details basic information about the job, and includes a link to the Logs. The Hotel job logs list the job dependency chain and the current whalesay, and should look similar to:\nChain: Alpha Bravo Charlie Echo Foxtrot ____________________ \u0026lt; This is Job Hotel! \u0026gt; -------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/  Explore the other jobs in the workflow to see each job\u0026rsquo;s status and logs.\n"
},
{
	"uri": "/introduction/architecture/",
	"title": "Kubernetes Architecture",
	"tags": [],
	"description": "",
	"content": "In this section, we\u0026rsquo;ll cover the following topics:\n Architectural Overview   Control Plane   Data Plane   Kubernetes Cluster Setup   "
},
{
	"uri": "/servicemesh_with_appmesh/create_the_k8s_app/conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "Congratulations on deploying the initial DJ App architecture!\nBefore we create the App Mesh-enabled versions of DJ App, we\u0026rsquo;ll first deploy the App Mesh sidecar auto-injector, and the App Mesh CRDs into our cluster.\n"
},
{
	"uri": "/servicemesh_with_appmesh/port_to_app_mesh/testing_app_mesh_v1/",
	"title": "Testing the App Mesh",
	"tags": [],
	"description": "",
	"content": "We should now be able to call metal or jazz from dj, and be routed to either metal-v1 or jazz-v1.\nTo test if our ported DJ App is working as expected, we\u0026rsquo;ll first exec into the dj container. To do that, we get the name of our djpod by listing all pods with the dj selector:\nkubectl get pods -nprod -lapp=dj  Output should be similar to:\nNAME READY STATUS RESTARTS AGE dj-5b445fbdf4-8xkwp 1/1 Running 0 32s  Next, we\u0026rsquo;ll exec into the DJ pod returned from the last step:\nkubectl exec -nprod -it \u0026lt;your-dj-pod-name\u0026gt; -c dj bash  Output should be similar to:\nroot@dj-5b445fbdf4-8xkwp:/usr/src/app#  Now that we have a root prompt into the DJ pod, we\u0026rsquo;ll make a curl request to the virtual service jazz on port 9080, simulating what would happen if code running in the same pod made a request to the jazz backend:\ncurl jazz.prod.svc.cluster.local:9080;echo  Output should be similar to:\n[\u0026quot;Astrud Gilberto\u0026quot;,\u0026quot;Miles Davis\u0026quot;]  Try it again, but issue the command to the virtual metal service:\ncurl metal.prod.svc.cluster.local:9080;echo  You should get a list of heavy metal bands back:\n[\u0026quot;Megadeth\u0026quot;,\u0026quot;Judas Priest\u0026quot;]  When you\u0026rsquo;re done exploring this vast, service-mesh-enabled world of music, hit CTRL-D, or type exit to exit the container\u0026rsquo;s shell:\nroot@dj-779566bbf6-cqpxt:/usr/src/app# exit command terminated with exit code 1 $  Congrats! You\u0026rsquo;ve migrated the initial architecture to provide the same functionality, but now with App Mesh Virtual Services.\nLet\u0026rsquo;s see the true power of this new App Mesh service mesh-based architecture by adding a new version of the metal and jazz services, and taking a closer look at how we can route between the different versions, which is very useful when implementing canary testing.\n"
},
{
	"uri": "/helm_root/helm_micro/",
	"title": "Deploy Example Microservices Using Helm",
	"tags": [],
	"description": "",
	"content": " Deploy Example Microservices Using Helm In this chapter, we will demonstrate how to deploy microservices using a custom Helm Chart, instead of doing everything manually using kubectl.\nFor detailed information on working with chart templates, refer to the Helm docs\n"
},
{
	"uri": "/batch/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Cleanup Delete all workflows argo delete --all  Remove Artifact Repository Bucket aws s3 rb s3://batch-artifact-repository-${ACCOUNT_ID}/ --force  Undeploy Argo kubectl delete -n argo -f https://raw.githubusercontent.com/argoproj/argo/v2.2.1/manifests/install.yaml kubectl delete ns argo  Cleanup Kubernetes Job kubectl delete job/whalesay  "
},
{
	"uri": "/introduction/architecture/architecture_control_and_data_overview/",
	"title": "Architectural Overview",
	"tags": [],
	"description": "",
	"content": "graph TB internet((internet)) kubectl{kubectl} subgraph ControlPlane api(API Server) controller(Controller Manager) scheduler(Scheduler) etcd(etcd) end subgraph worker1 kubelet1(kubelet) kube-proxy1(kube-proxy) subgraph docker1 subgraph podA containerA[container] end subgraph podB containerB[container] end end end internet--kube-proxy1 kubectl--api controller--api scheduler--api api--kubelet1 api--etcd kubelet1--containerA kubelet1--containerB kube-proxy1--containerA kube-proxy1--containerB classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;  "
},
{
	"uri": "/introduction/eks/",
	"title": "Amazon EKS",
	"tags": [],
	"description": "",
	"content": "In this section, we\u0026rsquo;ll cover the following topics:\n EKS Cluster Creation Workflow   What happens when you create your EKS cluster   EKS Architecture for Control plane and Worker node communication   High Level   Amazon EKS!   "
},
{
	"uri": "/deploy/cleanup/",
	"title": "Cleanup the applications",
	"tags": [],
	"description": "",
	"content": "To delete the resources created by the applications, we should delete the application deployments:\nUndeploy the applications:\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml  "
},
{
	"uri": "/servicemesh_with_appmesh/port_to_app_mesh/canary_testing/",
	"title": "Canary Testing with a v2",
	"tags": [],
	"description": "",
	"content": "A canary release is a method of slowly exposing a new version of software. The theory behind it is that by serving the new version of the software initially to say, 5% of requests, if there is a problem, the problem only impacts a very small percentage of users before its discovered and rolled back.\nSo now back to our DJ App scenario\u0026hellip; the V2 of the metal and jazz services are out, and they now include the city each artist is from in the response. Let\u0026rsquo;s see how we can release v2 versions of metal and jazz services in a canary fashion using App Mesh.\nWhen we\u0026rsquo;re complete, requests to metal and jazz will be distributed in a weighted fashion to both the v1 and v2 versions.\nTo begin, we\u0026rsquo;ll rollout the v2 deployments, services, and Virtual Nodes with a single YAML file:\nkubectl apply -nprod -f 5_canary/jazz_v2.yaml  Output should be similar to:\ndeployment.apps/jazz-v2 created service/jazz-v2 created virtualnode.appmesh.k8s.aws/jazz-v2 created  Next, we\u0026rsquo;ll update the jazz Virtual Service by modifying the route to spread traffic 50\u0026frasl;50 across the two versions. If we take a look at it now, we\u0026rsquo;ll see the current route which points to jazz-v1 100%:\nkubectl describe virtualservice jazz -nprod  yields:\nName: jazz.prod.svc.cluster.local Namespace: prod Labels: \u0026lt;none\u0026gt; Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026quot;apiVersion\u0026quot;:\u0026quot;appmesh.k8s.aws/v1beta1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;VirtualService\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;jazz.prod.svc.cluster.local\u0026quot;,\u0026quot;namesp... API Version: appmesh.k8s.aws/v1beta1 Kind: VirtualService Metadata: Creation Timestamp: 2019-03-23T00:15:08Z Generation: 3 Resource Version: 2851527 Self Link: /apis/appmesh.k8s.aws/v1beta1/namespaces/prod/virtualservices/jazz.prod.svc.cluster.local UID: b76eed59-4d00-11e9-87e6-06dd752b96a6 Spec: Mesh Name: dj-app Routes: Http: Action: Weighted Targets: Virtual Node Name: jazz-v1 Weight: 100 Match: Prefix: / Name: jazz-route Virtual Router: Name: jazz-router Status: Conditions: Events: \u0026lt;none\u0026gt;  We apply the updated service definition:\nkubectl apply -nprod -f 5_canary/jazz_service_update.yaml  And when we describe the Virtual Service again, we see the updated route:\nkubectl describe virtualservice jazz -nprod  as 90\u0026frasl;10:\nName: jazz.prod.svc.cluster.local Namespace: prod Labels: \u0026lt;none\u0026gt; Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026quot;apiVersion\u0026quot;:\u0026quot;appmesh.k8s.aws/v1beta1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;VirtualService\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;jazz.prod.svc.cluster.local\u0026quot;,\u0026quot;namesp... API Version: appmesh.k8s.aws/v1beta1 Kind: VirtualService Metadata: Creation Timestamp: 2019-03-23T00:15:08Z Generation: 4 Resource Version: 2851774 Self Link: /apis/appmesh.k8s.aws/v1beta1/namespaces/prod/virtualservices/jazz.prod.svc.cluster.local UID: b76eed59-4d00-11e9-87e6-06dd752b96a6 Spec: Mesh Name: dj-app Routes: Http: Action: Weighted Targets: Virtual Node Name: jazz-v1 Weight: 90 Virtual Node Name: jazz-v2 Weight: 10 Match: Prefix: / Name: jazz-route Virtual Router: Name: jazz-router Status: Conditions: Events: \u0026lt;none\u0026gt;  We perform the same steps to deploy metal-v2. Rollout the v2 deployments, services, and Virtual Nodes with a single YAML file:\nkubectl apply -nprod -f 5_canary/metal_v2.yaml  Output should be similar to:\ndeployment.apps/metal-v2 created service/metal-v2 created virtualnode.appmesh.k8s.aws/metal-v2 created  Update the metal Virtual Service by modifying the route to spread traffic 50\u0026frasl;50 across the two versions:\nkubectl apply -nprod -f 5_canary/metal_service_update.yaml  And when we describe the Virtual Service again, we see the updated route:\nkubectl describe virtualservice metal -nprod  yields:\nName: metal.prod.svc.cluster.local Namespace: prod Labels: \u0026lt;none\u0026gt; Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026quot;apiVersion\u0026quot;:\u0026quot;appmesh.k8s.aws/v1beta1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;VirtualService\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;metal.prod.svc.cluster.local\u0026quot;,\u0026quot;names... API Version: appmesh.k8s.aws/v1beta1 Kind: VirtualService Metadata: Creation Timestamp: 2019-03-23T00:15:08Z Generation: 2 Resource Version: 2852282 Self Link: /apis/appmesh.k8s.aws/v1beta1/namespaces/prod/virtualservices/metal.prod.svc.cluster.local UID: b784e824-4d00-11e9-87e6-06dd752b96a6 Spec: Mesh Name: dj-app Routes: Http: Action: Weighted Targets: Virtual Node Name: metal-v1 Weight: 50 Virtual Node Name: metal-v2 Weight: 50 Match: Prefix: / Name: metal-route Virtual Router: Name: metal-router Status: Conditions: Events: \u0026lt;none\u0026gt;  Now that the v2\u0026rsquo;s are deployed, let\u0026rsquo;s test them out.\n"
},
{
	"uri": "/conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": " Conclusion "
},
{
	"uri": "/introduction/architecture/architecture_control/",
	"title": "Control Plane",
	"tags": [],
	"description": "",
	"content": "graph TB kubectl{kubectl} subgraph ControlPlane api(API Server) controller(Controller Manager) scheduler(Scheduler) etcd(etcd) end kubectl--api controller--api scheduler--api api--kubelet api--etcd classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;   One or More API Servers: Entry point for REST / kubectl\n etcd: Distributed key/value store\n Controller-manager: Always evaluating current vs desired state\n Scheduler: Schedules pods to worker nodes\n  Check out the official Kubernetes documentation for a more in-depth explanation of control plane components.\n"
},
{
	"uri": "/servicemesh_with_appmesh/port_to_app_mesh/testing_the_v2/",
	"title": "Testing DJ App v2",
	"tags": [],
	"description": "",
	"content": "To test if its working as expected, we\u0026rsquo;ll exec into the DJ pod. To do that, we get the name of our dj pod by listing all pods with the dj selector:\nkubectl get pods -nprod -l app=dj  Output should be similar to:\nNAME READY STATUS RESTARTS AGE dj-5b445fbdf4-8xkwp 1/1 Running 0 32s  Next, we\u0026rsquo;ll exec into the DJ pod, and make a curl request to the virtual service jazz, simulating what would happen if code running in the same pod made a request to the metal service by entering the following:\nkubectl exec -nprod -it \u0026lt;your-dj-pod-name\u0026gt; -c dj bash  Output should be similar to:\nroot@dj-5b445fbdf4-8xkwp:/usr/src/app#  Now that we have a root prompt into the DJ pod, we\u0026rsquo;ll issue our curl request to the jazz virtual service:\nwhile [ 1 ]; do curl http://metal.prod.svc.cluster.local:9080/;echo; done  Output should loop about 50\u0026frasl;50 between the v1 and v2 versions of the metal service, similar to:\n... [\u0026quot;Megadeth\u0026quot;,\u0026quot;Judas Priest\u0026quot;] [\u0026quot;Megadeth (Los Angeles, California)\u0026quot;,\u0026quot;Judas Priest (West Bromwich, England)\u0026quot;] [\u0026quot;Megadeth\u0026quot;,\u0026quot;Judas Priest\u0026quot;] [\u0026quot;Megadeth (Los Angeles, California)\u0026quot;,\u0026quot;Judas Priest (West Bromwich, England)\u0026quot;] ...  Hit CTRL-C to stop the looping.\nWe\u0026rsquo;ll next perform a similar test, but against the jazz service. Issue a curl request to the jazz virtual service from within the dj pod:\nwhile [ 1 ]; do curl http://jazz.prod.svc.cluster.local:9080/;echo; done  Output should loop about in a 90\u0026frasl;10 ratio between the v1 and v2 versions of the jazz service, similar to:\n... [\u0026quot;Astrud Gilberto\u0026quot;,\u0026quot;Miles Davis\u0026quot;] [\u0026quot;Astrud Gilberto\u0026quot;,\u0026quot;Miles Davis\u0026quot;] [\u0026quot;Astrud Gilberto\u0026quot;,\u0026quot;Miles Davis\u0026quot;] [\u0026quot;Astrud Gilberto (Bahia, Brazil)\u0026quot;,\u0026quot;Miles Davis (Alton, Illinois)\u0026quot;] [\u0026quot;Astrud Gilberto\u0026quot;,\u0026quot;Miles Davis\u0026quot;] ...  Hit CTRL-C to stop the looping, and type exit to exit the pod\u0026rsquo;s shell.\nCongrats on implementing the DJ App onto App Mesh!\n"
},
{
	"uri": "/helm_root/helm_nginx/updatecharts/",
	"title": "Update the Chart Repository",
	"tags": [],
	"description": "",
	"content": "Helm uses a packaging format called Charts. A Chart is a collection of files that describe k8s resources.\nCharts can be simple, describing something like a standalone web server (which is what we are going to create), but they can also be more complex, for example, a chart that represents a full web application stack included web servers, databases, proxies, etc.\nInstead of installing k8s resources manually via kubectl, we can use Helm to install pre-defined Charts faster, with less chance of typos or other operator errors.\nWhen you install Helm, you are provided with a default repository of Charts from the official Helm Chart Repository.\nThis is a very dynamic list that always changes due to updates and new additions. To keep Helm\u0026rsquo;s local list updated with all these changes, we need to occasionally run the repository update command.\nTo update Helm\u0026rsquo;s local list of Charts, run:\nhelm repo update  And you should see something similar to:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository Update Complete. ⎈ Happy Helming!⎈  Next, we\u0026rsquo;ll search for the NGINX web server Chart.\n"
},
{
	"uri": "/network-policies/tigera/tsce-sg-integration/",
	"title": "Integrating VPC Security Groups and Kubernetes Network Policy with TSCE",
	"tags": [],
	"description": "",
	"content": " The network security that Calico provides in EKS is great, however it is primarily focused on the EKS cluster itself. A common use-case for EKS, however, is to build a kubernetes cluster that can interact with other Amazon hosted resources, such as EC2 and RDS instances. The native protection for those resources is the VPC\u0026rsquo;s Security Group filtering.\nThe problem with this, however, is that, by default, VPC Security Groups can only be applied to EC2 instances. Therefore, if you wanted to allow some subset of your pods access to an RDS instance, for example, you would have to allow that access from all of your EKS worker nodes, thereby allowing ALL your EKS pods access to that RDS instance. That\u0026rsquo;s probably not what you want. Luckily, one of the capabilities that TSCE enables is the integration of the VPC Security Group mechanism and Kubernetes/Calico network policy.\nLet\u0026rsquo;s see how that works\u0026hellip;\nCreate an EC2 resource in your VPC We\u0026rsquo;re going to create a simple static webserver in your VPC, but not in your EKS cluster to act as a target to demonstrate how pods can become members of a VPC Security Group, and be referenced by VPC Security Group (VSG) policies.\nTo do this, we need a simple web server in the same VPC as your EKS cluster, but running as a separate EC2 instance, not in the EKS cluster itself. You can do this anyway you like, but if you want some hints, here are some steps you can take to accomplish this.\n Go into your EC2 console and create an EC2 instances  A t2-micro is more than sufficient An Amazon Linux AMI, SSD Volume is easy Tag it so you can easily find it later Set up a security group for the instance and allow inbound from any on port 80. Call it something like protect-sa-sg Create (or use an existing) SSH key Launch and then ssh into the instance (either directly or via the console) Install you favorite webseerver using the platform\u0026rsquo;s package tool. As an example in the case of the Amazon Linux, AMI, you might use yum to install httpd and start it: \u0026ldquo;\u0026rsquo; yum update -y yum install httpd -y service httpd start chkconfig httpd on \u0026ldquo;\u0026rsquo; Install a static file that will be used as the test target: \u0026ldquo;\u0026rsquo; cd /var/www/html echo \u0026ldquo;Welcome to Setec Astronomy\u0026rdquo; \u0026gt; index.html \u0026ldquo;\u0026rsquo;   Is the website reachable? Let\u0026rsquo;s launch a busybox pod in the cluster and do a curl to the IP address of the EC2 instance we just created. So, in the cloud9 shell, do the following:\nkubectl run -it test1 --image=busybox -- sh \\ # wget -O - http://\u0026lt;your_ec2_instance_private_IP_here\u0026gt;  You should see your text. Now exit from the busybox container, but note the instructions on how to re-attach to it, we\u0026rsquo;ll need that later.\n\\ # exit Session ended, resume using 'kubectl attach test1-cd46f75fd-fts4r -c test1 -i -t' command when the pod is running  Next, let\u0026rsquo;s repeat the same again, only this time with test2:\nkubectl run -it test2 --image=busybox -- sh \\ # wget -O - http://\u0026lt;your_ec2_instance_private_IP_here\u0026gt; \\ # exit Session ended, resume using 'kubectl attach test2-766c48655b-hr8zj -c test2 -i -t' command when the pod is running  Tighten up the Security Groups First of all, let\u0026rsquo;s create a new security group in the VPC, called allow-sa-sg.\nNext, change the protect-sa-sg security group to only allow inbound traffic for port 80 from the allow-sa-sg security group members.\nOnce that is done, get the Security Group identifiers (i.e. sg-xxxxxxxxxxx) for the allow-sa-sg security group. You can do this by listing the VPC\u0026rsquo;s security groups in the VPC console.\nNow annotate the test1 pod with the allow-sa-sg security group.\nkubectl annotate pod \u0026lt;test1 pod name\u0026gt; aws.tigera.io/security-groups='[\u0026quot;sg-xxxxxx\u0026quot;]'  Lastly, use the instructions provided for connecting back into the test1 and test2 pods and re-run the wget commands.\nYou should see that test1 can connect to the webserver, but test2 is denied.\nWe have now protected the VPC resource on a per-pod basis using VPC security groups.\n"
},
{
	"uri": "/introduction/architecture/architecture_worker/",
	"title": "Data Plane",
	"tags": [],
	"description": "",
	"content": "graph TB internet((internet)) subgraph worker1 kubelet1(kubelet) kube-proxy1(kube-proxy) subgraph docker1 subgraph podA containerA[container] end subgraph podB containerB[container] end end end internet--kube-proxy1 api--kubelet1 kubelet1--containerA kubelet1--containerB kube-proxy1--containerA kube-proxy1--containerB classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;   Made up of worker nodes\n kubelet: Acts as a conduit between the API server and the node\n kube-proxy: Manages IP translation and routing\n  Check out the official Kubernetes documentation for a more in-depth explanation of data plane components.\n"
},
{
	"uri": "/introduction/architecture/cluster_setup_options/",
	"title": "Kubernetes Cluster Setup",
	"tags": [],
	"description": "",
	"content": "In addition to the managed Amazon EKS solution, there are many tools available to help bootstrap and configure a self-managed Kubernetes cluster. They include:\n Minikube – Development and Learning Kops – Learning, Development, Production Kubeadm – Learning, Development, Production Docker for Mac - Learning, Development Kubernetes IN Docker - Learning, Development  Alongside these open source solutions, there are also many commercial options available.\nLet\u0026rsquo;s take a look at Amazon EKS!\n"
},
{
	"uri": "/introduction/eks/eks_customers/",
	"title": "EKS Cluster Creation Workflow",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/eks/eks_control_plane/",
	"title": "What happens when you create your EKS cluster",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/eks/eks_high_architecture/",
	"title": "EKS Architecture for Control plane and Worker node communication",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/eks/eks_high_level/",
	"title": "High Level",
	"tags": [],
	"description": "",
	"content": "Once your EKS cluster is ready, you get an API endpoint and you\u0026rsquo;d use Kubectl, community developed tool to interact with your cluster.\n"
},
{
	"uri": "/network-policies/tigera/tsce-cw-integration/",
	"title": "Integrating Detailed Kubernetes Networking Flow Logs in CloudWatch",
	"tags": [],
	"description": "",
	"content": " Now that we have policies installed, and traffic being generated in the cluster, we can look at the CloudWatch integration that TSCE provides.\nWe\u0026rsquo;re assuming that you have run through both the Calico section of this tutorial and the first part of the TSCE section. If you skipped the Calico section, please go back and run through that as well, as we are relying on the synthetic applications used in the Calico examples to generate flowlogs in CloudWatch.\nSelecting out cluster in CloudWatch You need to get the clusterID of the EKS cluster. To do that, run the following command:\n$ kubectl get clusterinformation default -o yaml --kubeconfig=\u0026lt;your kubeconfig\u0026gt; | grep GUID clusterGUID: 6af4b853f8fa484b9870a95ff5102e96  In the cloud9 shell, go to the AWS dashboard, and select the Cloudwatch service.\nViewing and graphing metrics Go to the metrics section, and select all of the metrics being reported by your clusterID. See the screenshot below\nJust as in the screenshot, you should now see CloudWatch graphing various statistics such as denied packets, unhealthy nodes, etc.\nViewing the flow logs Similarly, you can see the actual flow logs by switching from the Metrics to the Logs view, as shown below\n"
},
{
	"uri": "/introduction/eks/stay_tuned/",
	"title": "Amazon EKS!",
	"tags": [],
	"description": "",
	"content": "Stay tuned as we continue the journey with EKS in the next module!\nAlways ask questions! Feel free to ask them in person during this workshop, or any time on the official Kubernetes Slack channel accessible via http://slack.k8s.io/.\n"
},
{
	"uri": "/helm_root/helm_nginx/searchchart/",
	"title": "Search the Chart Repository",
	"tags": [],
	"description": "",
	"content": "Now that our repository Chart list has been updated, we can search for Charts.\nTo list all Charts:\nhelm search  That should output something similiar to:\nNAME CHART VERSION APP VERSION DESCRIPTION stable/acs-engine-autoscaler 2.2.0 2.1.1 Scales worker... stable/aerospike 0.1.7 v3.14.1.2 A Helm chart... ...  You can see from the output that it dumped the list of all Charts it knows about. In some cases that may be useful, but an even more useful search would involve a keyword argument. So next, we\u0026rsquo;ll search just for NGINX:\nhelm search nginx  That results in:\nNAME CHART VERSION APP VERSION DESCRIPTION stable/nginx-ingress 0.31.0 0.20.0 An nginx Ingress ... stable/nginx-ldapauth-proxy 0.1.2 1.13.5 nginx proxy ... stable/nginx-lego 0.3.1 Chart for... stable/gcloud-endpoints 0.1.2 1 DEPRECATED Develop... ...  This new list of Charts are specific to nginx, because we passed the nginx argument to the search command.\n"
},
{
	"uri": "/network-policies/tigera/init-policy/",
	"title": "Initializing Network Policy",
	"tags": [],
	"description": "",
	"content": "Now that we\u0026rsquo;ve seen that all the traffic is being allowed in the cluster, let\u0026rsquo;s start tightening the policies, and restrict some of the traffic. To do that, we will first create a default deny policy that will block all inbound traffic in our default namespace unless it is specifically allowed. To do that, we create a null policy that matches everything in the default namespace.\nThe YAML fragment that defines such a policy can be seen below\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny spec: podSelector: matchLabels: {}  Now create a file called default-deny.yaml with the above contents and install it in your cluster using kubectl.\n$ kubectl apply -f default-deny.yaml  "
},
{
	"uri": "/network-policies/tigera/backend-policy/",
	"title": "Policy Enabling the Backends",
	"tags": [],
	"description": "",
	"content": "Now, let\u0026rsquo;s create a policy that allows the ecsdemo-frontend microservice to communicate with the ecsdemo-nodejs microservice. That policy will look something like this:\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ecsdemo-nodejs spec: podSelector: matchLabels: app: ecsdemo-nodejs ingress: - ports: -port: 3000 from: - podSelector: matchLabels: app: ecsdemo-frontend  Let\u0026rsquo;s create that policy in a file, called ecsdemo-nodejs-policy.yaml and then load it into Kubernetes using kubectl.\n$ kubectl apply -f ecsdemo-nodejs-policy.yaml  Once you\u0026rsquo;ve done that, again, look at the flow logs and you will see that traffic between the frontend and the nodejs services is now being allowed, but the traffic from the frontend to the crystal microservice is still being blocked. Let\u0026rsquo;s fix that.\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ecsdemo-crystal spec: podSelector: matchLabels: app: ecsdemo-crystal ingress: - ports: -port: 3000 from: - podSelector: matchLabels: app: ecsdemo-frontend  Use the same file create and kubectl apply steps that we used above to apply this new policy, and now you will see that all the frontend to backend traffic is now being allowed, again.\n"
},
{
	"uri": "/helm_root/helm_nginx/addbitnamirepo/",
	"title": "Add the Bitnami Repository",
	"tags": [],
	"description": "",
	"content": "In the last slide, we saw that NGINX offers many different products via the default Helm Chart repository, but the NGINX standalone web server is not one of them.\nAfter a quick web search, we discover that there is a Chart for the NGINX standalone web server available via the Bitnami Chart repository.\nTo add the Bitnami Chart repo to our local list of searchable charts:\nhelm repo add bitnami https://charts.bitnami.com/bitnami  Once that completes, we can search all Bitnami Charts:\nhelm search bitnami  Which results in:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/bitnami-common 0.0.3 0.0.1 Chart with... bitnami/apache 2.1.2 2.4.37 Chart for Apache... bitnami/cassandra 0.1.0 3.11.3 Apache Cassandra... ...  Search once again for NGINX:\nhelm search nginx  Now we are seeing more NGINX options, across both repositories:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/nginx 1.1.2 1.14.1 Chart for the nginx server bitnami/nginx-ingress-controller 2.1.4 0.20.0 Chart for the nginx Ingress... stable/nginx-ingress 0.31.0 0.20.0 An nginx Ingress controller ...  Or even search the Bitnami repo, just for NGINX:\nhelm search bitnami/nginx  Which narrows it down to NGINX on Bitnami:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/nginx 1.1.2 1.14.1 Chart for the nginx server bitnami/nginx-ingress-controller 2.1.4 0.20.0 Chart for the nginx Ingress...  In both of those last two searches, we see\nbitnami/nginx  as a search result. That\u0026rsquo;s the one we\u0026rsquo;re looking for, so let\u0026rsquo;s use Helm to install it to the EKS cluster.\n"
},
{
	"uri": "/helm_root/helm_nginx/installnginx/",
	"title": "Install bitnami/nginx",
	"tags": [],
	"description": "",
	"content": " Installing the Bitnami standalone NGINX web server Chart involves us using the helm install command.\nWhen we install using Helm, we need to provide a deployment name, or a random one will be assigned to the deployment automatically.\nChallenge: How can you use Helm to deploy the bitnami/nginx chart?\nHINT: Use the helm utility to install the bitnami/nginx chart and specify the name mywebserver for the Kubernetes deployment. Consult the helm install documentation or run the helm install --help command to figure out the syntax\n  Expand here to see the solution   helm install --name mywebserver bitnami/nginx    Once you run this command, the output confirms the types of k8s objects that were created as a result:\nNAME: mywebserver LAST DEPLOYED: Tue Nov 13 19:55:25 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1beta1/Deployment NAME AGE mywebserver-nginx 0s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE mywebserver-nginx-85985c8466-tczst 0/1 ContainerCreating 0 0s ==\u0026gt; v1/Service NAME AGE mywebserver-nginx 0s  In the following kubectl command examples, it may take a minute or two for each of these objects\u0026rsquo; DESIRED and CURRENT values to match; if they don\u0026rsquo;t match on the first try, wait a few seconds, and run the command again to check the status.\n The first object shown in this output is a Deployment. A Deployment object manages rollouts (and rollbacks) of different versions of an application.\nYou can inspect this Deployment object in more detail by running the following command:\nkubectl describe deployment mywebserver-nginx  The next object shown created by the Chart is a Pod. A Pod is a group of one or more containers.\nTo verify the Pod object was successfully deployed, we can run the following command:\nkubectl get pods -l app=mywebserver-nginx  And you should see output similar to:\nNAME READY STATUS RESTARTS AGE mywebserver-nginx-85985c8466-tczst 1/1 Running 0 10s  The third object that this Chart creates for us is a Service The Service enables us to contact this NGINX web server from the Internet, via an Elastic Load Balancer (ELB).\nTo get the complete URL of this Service, run:\nkubectl get service mywebserver-nginx -o wide  That should output something similar to:\nNAME TYPE CLUSTER-IP EXTERNAL-IP mywebserver-nginx LoadBalancer 10.100.223.99 abc123.amazonaws.com  Copy the value for EXTERNAL-IP, open a new tab in your web browser, and paste it in. It may take a couple minutes for the ELB and its associated DNS name to become available; if you get an error, wait one minute, and hit reload.\n When the Service does come online, you should see a welcome message similar to:\nCongrats! You\u0026rsquo;ve now successfully deployed the NGINX standalone web server to your EKS cluster!\n"
},
{
	"uri": "/helm_root/helm_nginx/cleaningup/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "To remove all the objects that the Helm Chart created, we can use Helm delete.\nBefore we delete it though, we can verify what we have running via the Helm list command:\nhelm list  You should see output similar to below, which show that mywebserver is installed:\nNAME REVISION UPDATED STATUS CHART APP VERSION mywebserver 1 Tue Nov 13 19:55:25 2018 DEPLOYED nginx-1.1.2 1.14.1  It was a lot of fun; we had some great times sending HTTP back and forth, but now its time to delete this deployment. To delete:\nhelm delete --purge mywebserver  And you should be met with the output:\nrelease \u0026quot;mywebserver\u0026quot; deleted  kubectl will also demonstrate that our pods and service are no longer available:\nkubectl get pods -l app=mywebserver-nginx kubectl get service mywebserver-nginx -o wide  As would trying to access the service via the web browser via a page reload.\nWith that, cleanup is complete.\n"
},
{
	"uri": "/tabs-example/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Tabs with regular text:  Tab 1 Tab 2  echo \"This is tab 1.\"  println \"This is tab 2.\"   $(function(){$(\"#tab\").tabs();}); Tabs with code blocks:  Tab 1 Tab 2  echo \u0026#34;This is tab 1.\u0026#34;   println \u0026#34;This is tab 2.\u0026#34;    $(function(){$(\"#tab-with-code\").tabs();}); Tabs showing installation process:  eksctl terraform cloudformation  To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl-yourusername --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n  We start by initializing the Terraform state:\nterraform init  We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n  To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text)  Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS}  Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text  When your cluster status is ACTIVE you can proceed.\n  $(function(){$(\"#tab-installation\").tabs();}); Second set of tabs showing installation process:  eksctl terraform cloudformation  To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl-yourusername --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n  We start by initializing the Terraform state:\nterraform init  We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n  To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text)  Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS}  Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text  When your cluster status is ACTIVE you can proceed.\n  $(function(){$(\"#more-tab-installation\").tabs();}); "
},
{
	"uri": "/weave_flux/app.files/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Hello World  Hello World! Server\u0026nbsp;address: server_address\nServer\u0026nbsp;name: server_hostname\nDate: server_date\nURI: server_url\n "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/authors/",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": " Thanks to our wonderful contributors  for making Open Source a better place! .ghContributors{ display:flex; flex-flow: wrap; align-content: flex-start } .ghContributors  div{ width: 50% ; display: inline-flex; margin-bottom: 5px; } .ghContributors  div label{ padding-left: 4px ; } .ghContributors  div span{ font-size: x-small; padding-left: 4px ; }   @brentley 79 commits   @geremyCohen 30 commits   @oak2278 22 commits   @arun-gupta 12 commits   @dalbhanj 10 commits   @rnzsgh 9 commits   @jpeddicord 6 commits   @buzzsurfr 6 commits   @nikipat 4 commits   @gmarchand 3 commits   @mpdominguez 3 commits   @tabern 3 commits   @alexpulver 2 commits   @alexei-led 2 commits   @DocValerian 2 commits   @rasensio 2 commits   @jennylover 2 commits   @waynekhan 2 commits   @johnstanfield 2 commits   @jonjozwiak 2 commits   @rudpot 2 commits   @ocxo 1 commits   @andreivmaksimov 1 commits   @sharmaanshul21 1 commits   @bhean 1 commits   @ChanceLee0111 1 commits   @cannoc 1 commits   @ckamps 1 commits   @liljenstolpe 1 commits   @rebelthor 1 commits   @gibbster 1 commits   @deki 1 commits   @enghwa 1 commits   @nrdlngr 1 commits   @feanil 1 commits   @GElkayam 1 commits   @geordan-hatech 1 commits   @giusedroid 1 commits   @itaysk 1 commits   @jsunmapr 1 commits   @techsolx 1 commits   @jonDowdle 1 commits   @jmferrer 1 commits   @jupp0r 1 commits   @kimsaabyepedersen 1 commits   @kbiton 1 commits   @Lixxia 1 commits   @LiranBri 1 commits   @mreferre 1 commits   @mikesigs 1 commits   @nihooge 1 commits   @wolruf 1 commits   @runningman84 1 commits   @ranrotx 1 commits   @ryanwilsonperkin 1 commits   @shaiss 1 commits   @sajee 1 commits   @lavignes 1 commits   @onyno 1 commits   @obbaeiei 1 commits   @toricls 1 commits   @trevorrobertsjr 1 commits   @tsahiduek 1 commits   @Turochamp 1 commits   @vjsikha 1 commits   @oik741 1 commits   @algestam 1 commits   @auziel 1 commits   @barelnir 1 commits   @clavery-chef 1 commits   @gearora 1 commits   @jaybarnes 1 commits   @snowjunkie 1 commits   @thepoosh 1 commits   @omeraplak 1 commits   @TemuulenS 1 commits   @ptux 1 commits   "
},
{
	"uri": "/tabs-example/tabs/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tabs-example/tabs/eks/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": "To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text)  Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS}  Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text  When your cluster status is ACTIVE you can proceed.\n"
},
{
	"uri": "/tabs-example/tabs/launcheks/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": "We start by initializing the Terraform state:\nterraform init  We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n "
},
{
	"uri": "/example_cf_templates/",
	"title": "Example of using CloudFormation Templates",
	"tags": [],
	"description": "",
	"content": " Click below to add a CloudFormation Stack    Use these templates:       Template 1 example  Launch    Download     Template 2 example  Launch    Download     Template 3 example  Launch    Download      "
},
{
	"uri": "/intro_to_rbac/",
	"title": "Intro_to_rbacs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/",
	"title": "Introductions",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/prerequisites/eu-west-1/",
	"title": "Ireland",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://eu-west-1.console.aws.amazon.com/cloud9/home?region=eu-west-1\n"
},
{
	"uri": "/prerequisites/self_paced/eu-west-1/",
	"title": "Ireland",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://eu-west-1.console.aws.amazon.com/cloud9/home?region=eu-west-1\n"
},
{
	"uri": "/more_resources/",
	"title": "More Resources",
	"tags": [],
	"description": "",
	"content": " Discover more AWS resources for building and running your application on AWS:\nMore Workshops  Amazon ECS Workshop - Learn how to use Stelligent Mu to deploy a microservice architecture that runs in AWS Fargate Amazon Lightsail Workshop - If you are getting started with the cloud and looking for a way to run an extremely low cost environment Lightsail is perfect. Learn how to deploy to Amazon Lightsail with this workshop.  Tools for AWS Fargate and Amazon ECS  Containers on AWS - Learn common best-practices for running containers on AWS fargate - Command line tool for interacting with AWS Fargate. With just a single command you can build, push, and launch your container in Fargate, orchestrated by ECS. Terraform - Use Terraform to deploy your docker containers in Fargate Wonqa is a tool for spinning up disposable QA environments in AWS Fargate, with SSL enabled by Let\u0026rsquo;s Encrypt. More details about Wonqa on the Wonder Engineering blog coldbrew - Fantastic tool that provisions ECS infrastructure, builds and deploys your container, and connects your services to an application load balancer automatically. Has a great developer experience for day to day use mu - Automates everything relating to ECS devops and CI/CD. This framework lets you write a simple metadata file and it constructs all the infrastructure you need so that you can deploy to ECS by simply pushing to your Git repo.  Courses  Microservices with Docker, Flask, and React - Learn how to build, test, and deploy microservices powered by Docker, Flask, React Amazon ECS!  "
},
{
	"uri": "/prerequisites/self_paced/us-east-2/",
	"title": "Ohio",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://us-east-2.console.aws.amazon.com/cloud9/home?region=us-east-2\n"
},
{
	"uri": "/prerequisites/us-east-2/",
	"title": "Ohio",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://us-east-2.console.aws.amazon.com/cloud9/home?region=us-east-2\n"
},
{
	"uri": "/prerequisites/self_paced/us-west-2/",
	"title": "Oregon",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://us-west-2.console.aws.amazon.com/cloud9/home?region=us-west-2\n"
},
{
	"uri": "/prerequisites/us-west-2/",
	"title": "Oregon",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://us-west-2.console.aws.amazon.com/cloud9/home?region=us-west-2\n"
},
{
	"uri": "/prerequisites/ap-southeast-1/",
	"title": "Singapore",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://ap-southeast-1.console.aws.amazon.com/cloud9/home?region=ap-southeast-1\n"
},
{
	"uri": "/prerequisites/self_paced/ap-southeast-1/",
	"title": "Singapore",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment: https://ap-southeast-1.console.aws.amazon.com/cloud9/home?region=ap-southeast-1\n"
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]